project,pull_number,pull_type,id,text,classification,indicator
iceberg,1772,comment,728256911,"+1
Thanks @aokolnychyi!",non_debt,-
fineract,709,comment,583203509,"ekhacoop@ welcome to the Apache Fineract community! It looks like you are trying to make your first contribution to this Git repository? However, this Pull Request is ""empty"" - there are no ""Files Changed"" (above).  I'm therefore closing this.  We look forward to receiving more contributions from you in the future!",non_debt,-
carbondata,2691,comment,418741401,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/3/",non_debt,-
beam,1974,review,100887469,"What is the relationship between ""dataflow counters"" and ""metric updates""?  This comment refers to counters, while the code below refers to metric update protos.  Should this comment instead mention translating accumulators for metric updates so that this class should be renamed accordingly?
Also, the naming of the methods below is confusing: two arguments are passed, e.g. set_boolean(accumulator, metric_update_proto).  Just reading this signature suggests that the accumulator will somehow have a boolean set from metric_update_proto, which is the opposite of what is intended.  Can you instead rename these methods translate_*, e.g. for translate_boolean(accumulator, metric_update_proto)?  This would make it clear that we are translating accumulator into metric_update_proto.  If you do this, can you rename set_scalar and set_mean in this file as well?",code_debt,low_quality_code
geode,112,review,55901760,"Like setup, can probably do without the comments",non_debt,-
spark,8760,review,45791721,"can you change these params to `maxFailedTasks` and `maxBlacklistedExecutors`, and change the comment to:
This strategy adds an executor to the blacklist for _all_ tasks when the executor has too many task failures.  An executor is placed in the blacklist when there are more than [[maxFailedTasks]] failed tasks.  Furthermore, all executors in one node are put into the blacklist if there are more than [[maxBlacklistedExecutors]] blacklisted executors on one node.  The benefit of this strategy is that different taskSets can learn experience from other taskSet to avoid allocating tasks on problematic executors.",non_debt,-
carbondata,2826,comment,432122362,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/951/",non_debt,-
spark,3649,comment,66556800,"  [Test build #24335 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/24335/consoleFull) for   PR 3649 at commit [`26aa58f`](https://github.com/apache/spark/commit/26aa58f951df85c279da844775dbee9d7a0953ff).
- This patch **passes all tests**.
- This patch merges cleanly.
- This patch adds the following public classes _(experimental)_:
  - `implicit class StringToSymbol(val sc: StringContext) extends AnyVal`",non_debt,-
pulsar,5491,comment,552050267,run cpp tests,non_debt,-
spark,20020,comment,353035963," * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds the following public classes _(experimental)_:
  * `trait DataWritingCommand extends Command `
  * `case class DataWritingCommandExec(cmd: DataWritingCommand, children: Seq[SparkPlan])`",non_debt,-
incubator-mxnet,11047,comment,438531615,@ankkhedia Is this good to go?,non_debt,-
nifi-minifi-cpp,926,review,508700255,"Okay, I get you. 
Wonder if we can find a solution in between. For eg. UUIDString is not a typedef, but publicly inherits SmallString, constructors are private and ID (where we create those) is a friend class?",non_debt,-
flink,8446,review,285160885,"I think you are right that in order to break up the cyclic dependency we would need to decouple the `SchedulingResultPartition` from the `SchedulingExecutionVertex` (via the `ExecutionVertexID` for example).
I'm wondering how bad this cyclic dependency and the need for mutable state is, though. From a user's perspective, the existing interfaces are a bit easier and more convenient to use. On the down side, it makes the implementation a bit harder and harder to test in isolation. However, do we want to test the `Scheduling*` implementations in isolation? Moreover, you always have this problem in graph structures which have bidirectional edges or loops.",build_debt,under-declared_dependencies
hudi,1469,comment,634970887,"# [Codecov](https://codecov.io/gh/apache/hudi/pull/1469?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/hudi/pull/1469?src=pr&el=continue).",non_debt,-
incubator-mxnet,15270,summary,0,"In file included from ../src/kvstore/kvstore.cc:28:
../src/kvstore/./kvstore_local.h:281:23: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
    auto validator = [this](const int key, const NDArray& nd, bool ignore_sparse) -> bool {
                      ^
../src/kvstore/./kvstore_local.h:326:23: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
    auto validator = [this](const int key, const RSPVal& val_rowid, bool ignore_sparse) -> bool {
                      ^
In file included from ../src/c_api/c_api_profile.cc:35:
../src/c_api/../profiler/./profiler.h:1160:8: warning: 'mxnet::profiler::ProfileOperator::start' hides overloaded virtual function [-Woverloaded-virtual]
  void start(mxnet::Context::DeviceType dev_type, uint32_t dev_id) {
       ^
../src/c_api/../profiler/./profiler.h:870:8: note: hidden overloaded virtual function 'mxnet::profiler::ProfileEvent::start' declared here: different number of parameters (0 vs 2)
  void start() override {
       ^
../src/c_api/../profiler/./profiler.h:1212:8: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
      [this](OprExecStat *stat) {}, name_.c_str(), dev_type_, dev_id_,
       ^
See also #14940",non_debt,-
spark,23266,review,240395676,"@dongjoon-hyun, the problem that you pointed out was that the schema shouldn't require a reader. 
That's what I'm saying here, too: the table should have a schema and it shouldn't need to implement the read path to validate a write using the table schema.",non_debt,-
flink,2363,comment,240791329,"Thanks for your contribution @zentol. I've gone over the code and made some inline comments. My main concern/question is actually the representation of metric's type and hierarchy information. I think that encoding it in a string and then re-parsing it on the receiver side to reconstruct the information is rather fragile and error-prone especially wrt maintainability. Maybe you can give me some background why you decided to do it so.
Apart from that, I think the code contains many tests, which I really like :-)",design_debt,non-optimal_design
spark,29795,review,493190061,nit: use local variable if possible,code_debt,low_quality_code
druid,3958,summary,0,"GroupBy v2 doesn't cache on the broker, so it isn't actually testing
what the test was supposed to be testing. Also, the test failed due
to mismatched expectations.",non_debt,-
trafficcontrol,5681,summary,0,Use the Python Postinstall implementation by default,non_debt,-
tinkerpop,144,summary,0,TINKERPOP3-957: Improve speed of addV(),code_debt,slow_algorithm
beam,2143,comment,283739463,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8052/<h2>Failed Tests: <span class='status-failure'>1</span></h2><h3><a name='beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark' /><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8052/org.apache.beam$beam-runners-spark/testReport'>beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark</a>: <span class='status-failure'>1</span></h3><ul><li><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8052/org.apache.beam$beam-runners-spark/testReport/org.apache.beam.runners.spark.translation.streaming/ResumeFromCheckpointStreamingTest/testWithResume/'><strong>org.apache.beam.runners.spark.translation.streaming.ResumeFromCheckpointStreamingTest.testWithResume</strong></a></li></ul>
--none--",non_debt,-
flink,14084,review,554882968,"Just not supported yet, the type TIME_STAMP_WITH_LOCAL_TIME_ZONE is rarely used. But I think we should both support them in this version.",requirement_debt,requirement_partially_implemented
kafka,1549,summary,0,"@ijuma i checked the cases where this test has failed and it seems to always be on the verification of the left join. I've ran this test plenty of times and i can't get it to fail. However in the interest of having stable builds, i've removed just the part of the test that is failing (which happens to be the last verification).
Thanks,
Damian",non_debt,-
spark,31905,comment,811106021,"Regarding your questions:
Can you provide a common streaming use case where a non-event-base processing of metrics is useful? The `Observation` proposes a ""get me the metrics"" API that hides listeners and multi-threading. In streaming context, there are no finite metrics.
Can you elaborate on this, please? Do you mean an async `get`? Can you provide some pseudo-code on how to use the metrics async.
It is not thread-safe in a scenario where the action on the observed dataset is called in a different thread than where the metrics are retrieved. I'd say such a multi-thread driver process is not the general use case for batch processing. Even if, the proposed `Observation` API aims at hiding multi-threading complexity so that users can use `Dataset.observe` in single-threaded scenarios. Users that are experience with multi-threading have no problem in using the existing listeners approach. They are not the target user of this extension.
Agreed.",requirement_debt,non-functional_requirements_not_fully_satisfied
incubator-brooklyn,763,review,35646804,.. except of course that when suspending a machine `JcloudsLocation` does not call the pre- and post-release methods of any attached `JcloudsLocationCustomizers`. I think this is fair given that the machine may be resumed.,non_debt,-
accumulo,332,comment,368365644,"Agreed.
You may have gotten that impression because I'm against bundling. But, I'm also against using Hadoop's bundled libs for same reason I'm against bundling our own. I'm in favor of intentional and thoughtful dependency convergence, as a downstream activity. I'd actually prefer we not ship any binary tarball packaging... but since we do, we might as well do it in a way that works well for most users. In any case, I agree with you, this can be improved once we get the basics in. :smile_cat:",design_debt,non-optimal_design
spark,20938,comment,383849805,retest this please,non_debt,-
incubator-brooklyn,174,review,17779614,What is this for? Is `docs/_config_local.yml` used anywhere?,non_debt,-
druid,8287,review,314501737,Same about the log level.,non_debt,-
trafficcontrol,2952,review,227523969,Fixed,non_debt,-
incubator-dolphinscheduler,4250,comment,746145200,"# [Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/4250?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/4250?src=pr&el=continue).",non_debt,-
superset,650,review,67918604,"There's definitely a more panda-esque way to do all this, but it's not shocking as is.",non_debt,-
drill,1862,review,333560129,Fixed,non_debt,-
cloudstack,1705,summary,0,Made the changes to improve logging.,code_debt,low_quality_code
rocketmq,376,comment,406472654,issue is  https://github.com/apache/rocketmq/issues/375,non_debt,-
flink,4932,summary,0,"This PR allows reporters to define their default delimiter. Previously we always defaulted to `.`.
Reporters may now implement the `DelimiterProvider` interface with which they may define a delimiter. This delimiter is used if no delimiter was explicitly configured for the reporter.
* Add DelimiterProvider interface
* modify Prometheus&JmxReporter to define the default delimiter
* modify delimiter setup in the MetricRegistry
* add a test to the MetricRegistryTest to verify functionality
This change added tests and can be verified as follows:
Run MetricRegistryTest#testDelimiterOverride.",non_debt,-
spark,15266,comment,254403378,Any other comment? @cloud-fan Thanks!,non_debt,-
spark,18567,comment,313762412,"BTW, do we need a new test case for `correctly set the active session`?",test_debt,low_coverage
incubator-pagespeed-ngx,639,comment,37984712,"This doesn't pass the system tests on my machine, hanging very early (with nginx-1.4.4):
With curl:
Requesting with `PageSpeed=off` on the querystring seems to works OK.
Requesting another url, `curl --raw -vv http://localhost:8050/mod_pagespeed_example/`
shows something else which looks concerning:
@jeffkaufman Can you reproduce this (perhaps with nginx-1.4.4)?",non_debt,-
flink,12917,review,462827759,Done.,non_debt,-
incubator-pinot,5769,review,467200428,"I think both ways have some pros and cons. I personally think that using a single endpoint is easier for users?
* The main purpose of online AD endpoint is to provide a convenient way to run AD tasks so the endpoint should be as simple as possible. Using a single endpoint is more user-friendly. Using a separate CRUD endpoint does allow more flexibility but it will ask users to firstly register their data before using this service.
* Secondly, online service will not allow much too large size of data so in my sense, sending data in the request every time is not a bottleneck?
I think we could provide both ways. This is phase 1 for this feature. In phase 2, we probably could support another two endpoints to support what you suggested. 
Regarding the cleanup, I think currently in phase 1, it is just a one-call request so I mentioned this as stateless because users will not have a separate endpoint to retrieve anomalies afterwards and hence we could clean up them. In phase 2, another two endpoints will be provided and for those endpoints, we do not need to do the cleanup.
Thanks for your suggestions!",design_debt,non-optimal_design
phoenix,244,summary,0,‚Ä¶ow for updated data row,non_debt,-
dubbo,2217,comment,412890407,"hi, i am using the dependency is
`
`
,which had been solved the bug?
Look forward to your favourable reply.",non_debt,-
drill,640,summary,0,Merge pull request #2 from apache/master,non_debt,-
pulsar,9443,comment,774180857,/pulsarbot run-failure-checks,non_debt,-
tvm,4280,review,349316135,"example ? Instead of exmaple ?
Message should say build with example external runtime and not relay vm profiler support .",non_debt,-
spark,1338,review,15431198,"@JoshRosen @MLnick the batch size here only affects transient data when writing and the re-serialization shouldn't be done if the data is already in pickle format (batch serialized or not). I'm uploading a patch to that effect. Since the batch size has no effect on the data persisted in files, I'm not exposing it to users. See you have any further comments and if you have a better suggestion for the default size 10, let me know.
@mateiz what you suggested is exposing the batch size for reading. I'll work on that next.",non_debt,-
nifi,2516,summary,0,NIFI-4516 FetchSolr Processor,non_debt,-
airflow,15174,review,606788324,"Agree with @ephraimbuddy , you are missing `.filter(DagRun.dag_id == dag_id)` that Ephraim suggested",non_debt,-
spark,28911,comment,661130435,Thank you for the review. I'll try to address them tomorrow!,non_debt,-
hadoop,2698,review,581200607,done,non_debt,-
hbase,2459,comment,700412617,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
storm,1662,comment,243796241,Fixed.  Re-ran tests.  Thanks.,non_debt,-
spark,31052,review,553663753,17165658-31052 review-553663753,non_debt,-
thrift,1309,review,127775849,Why was this removed?,non_debt,-
beam,13137,review,522334301,Maybe you also need to add null checks in process... up to you.,code_debt,low_quality_code
druid,586,review,13470857,revert this plesae,non_debt,-
ignite,5484,review,235874251,Equivalent `for` loop takes fewer lines.,non_debt,-
flink,14138,comment,730796662,cc @wuchong,non_debt,-
superset,9378,summary,0,"@graceguo-supercat noticed `black==19.10b0` and `black==19.3b0` is treating function trailing commas differently. This [upgrades black to `19.10b0`](https://github.com/psf/black/compare/19.3b0...19.10b0) to bring in the fix (https://github.com/psf/black/pull/763).
Mypy is also updated because it's a dependency of black.
N/A
CI Successful build.
N/A
@graceguo-supercat @john-bodley",non_debt,-
spark,19995,comment,352105632,cc/ @vanzin @ueshin @jiangxb1987 @erikerlandson @liyinan926,non_debt,-
skywalking,6220,review,559188687,fixed.,non_debt,-
activemq-artemis,1000,summary,0,36057260-1000 summary-0,non_debt,-
storm,3264,summary,0,STORM-3632 reduce SimpleSaslServerCallbackHandler logging,non_debt,-
spark,446,comment,43287373,Merged build started.,non_debt,-
spark,5867,comment,99559042,"  [Test build #31992 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/31992/consoleFull) for   PR 5867 at commit [`490d778`](https://github.com/apache/spark/commit/490d778b16934984acc577d19787268958ce5fd2).
- This patch **fails Spark unit tests**.
- This patch **does not merge cleanly**.
- This patch adds no public classes.",non_debt,-
trafficserver,6486,review,388565991,"Hahaha, the bike shedder arrives!
I'm not sure. Is it worth the set up for a single file?
In any case, I don't like the "".a"" approach. If you want a change, I would create a header file, make the `Overridable_Map` not static, and grab the file via ""Makefile.inc"" reaching over to ""../shared/txn-vars.cc"".
What would the directory structure be?
-shared
--src
---txn_vars.cc
--include
---txn_vars.h
or
-shared
--txn_vars.cc
--include
---txn_vars.h",non_debt,-
spark,17293,comment,311412286,"ping @HyukjinKwon, @gatorsmile",non_debt,-
arrow,6103,summary,0,ARROW-7473: [C++][Gandiva] Improve error message for locate function,code_debt,low_quality_code
groovy,480,comment,275655612,"I think no documentation and no test. I extracted the example from the code you changed actually to show you what your change will affect. In my opinion the get/set path needs to be removed, but that is another topic. The trouble is that get/set in the MOP is in general very badly documented. So it might or it might not be, that your change is a breaking one. Well no, it is a breaking change in terms of semantics, but if real world examples will be badly affected by this? no idea. But I have another example:
    public foo
}
a.x = 1
assert a.x == 1
a.foo = 2
assert a.@foo == 2
assert a.foo == null
assert a == [x:1]
the last three asserts will all fail.",test_debt,lack_of_tests
zookeeper,161,comment,278843293,+1.,non_debt,-
beam,14404,comment,813101273,Run Java PreCommit,non_debt,-
spark,19870,summary,0,[SPARK-22665][SQL] Avoid repartitioning with empty list of expressions,non_debt,-
airflow,8002,comment,605732413,"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contribution Guide (https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)
Here are some useful points:
- Pay attention to the quality of your code (flake8, pylint and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/master/STATIC_CODE_CHECKS.rst#prerequisites-for-pre-commit-hooks) will help you with that.
- In case of a new feature add useful documentation (in docstrings or in `docs/` directory). Adding a new operator? Check this short [guide](https://github.com/apache/airflow/blob/master/docs/howto/custom-operator.rst) Consider adding an example DAG that shows how users should use it.
- Consider using [Breeze environment](https://github.com/apache/airflow/blob/master/BREEZE.rst) for testing locally, it‚Äôs a heavy docker but it ships with a working Airflow and a lot of integrations.
- Be patient and persistent. It might take some time to get a review or get the final approval from Committers.
- Be sure to read the [Airflow Coding style]( https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#coding-style-and-best-practices).
Apache Airflow is a community-driven project and together we are making it better üöÄ.
Mailing List: dev@airflow.apache.org
Slack: https://apache-airflow-slack.herokuapp.com/",non_debt,-
spark,3663,comment,66551116,"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/24334/
Test FAILed.",non_debt,-
nifi,2778,review,194239526,Why not do the conversion while inserting?,non_debt,-
spark,23445,review,247331027,the `* 1000` should be inside the bracket.,non_debt,-
hadoop,720,comment,481838904,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
accumulo,256,review,116251656,How is it done on the client side?,non_debt,-
spark,19893,comment,350759207,"@jiangxb1987 feel free to take a look at it. More eyes, more possibilities.",non_debt,-
bookkeeper,3,comment,171381380,"@vik5jagan yup. but this pull request seems to try to merge branch 4.0 to master. is that intended? There is a link describing how to contribute to BookKeeper : https://cwiki.apache.org/confluence/display/BOOKKEEPER/Contributing+to+BookKeeper .
Let me know if you have any questions.",non_debt,-
spark,1819,comment,52405112,"  [QA tests have finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18681/consoleFull) for   PR 1819 at commit [`41ebc5f`](https://github.com/apache/spark/commit/41ebc5f912093fdf7b21808ce19da1bae514435e).
- This patch **fails** unit tests.
- This patch merges cleanly.
- This patch adds the following public classes _(experimental)_:
  - `abstract class Serializer`
  - `abstract class SerializerInstance`
  - `abstract class SerializationStream`
  - `abstract class DeserializationStream`
  - `class ShuffleBlockManager(blockManager: BlockManager,`
  - `case class OutputFaker(output: Seq[Attribute], child: SparkPlan) extends SparkPlan`
  - `implicit class LogicalPlanHacks(s: SchemaRDD)`
  - `implicit class PhysicalPlanHacks(originalPlan: SparkPlan)`
  - `class FakeParquetSerDe extends SerDe`",non_debt,-
incubator-pinot,6239,review,518275105,"True, time unit won't be set as well.
The new code doesn't have the requirement of setting start/end before time unit.",non_debt,-
beam,13905,comment,774255477,"# [Codecov](https://codecov.io/gh/apache/beam/pull/13905?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/beam/pull/13905?src=pr&el=continue).",non_debt,-
zookeeper,419,review,151766248,"I agree that it should be configurable, but I don't think using a Builder pattern precludes this.  I'm about to push changes that adds support for using system properties, that's a good idea.",non_debt,-
beam,7391,comment,451005714,R:@kennknowles,non_debt,-
flink,6129,review,193707442,"@zentol the result need be sorted, otherwise the test would report error, because the result is not ordered, print this :",non_debt,-
incubator-heron,320,summary,0,updated copyright in heron/cli,non_debt,-
helix,1390,comment,698625813,"1- Rebased and ran the tests again and updated the test results.
2- Done.",non_debt,-
pulsar,3019,review,243864237,"yes, I think earlier code was not formatted with formatter that caused this reformatting change.",code_debt,low_quality_code
tvm,654,summary,0,Compat for opencl mode between cpu mode and gpu mode,non_debt,-
nifi,2260,review,149995922,Hoping to remove this try/catch if [NIFI-4590](https://issues.apache.org/jira/browse/NIFI-4590) is implemented first.,non_debt,-
spark,16865,comment,278796046,"I understand the motivation here, could you show the benefit of this change for a real use case?",non_debt,-
tvm,6699,review,506719578,Better to comment why `-3` needs a special process.,non_debt,-
shardingsphere,6462,summary,0,change the url in index,non_debt,-
trafficcontrol,3979,summary,0,Fix broken unit tests and run go fmt,non_debt,-
flink,9980,comment,545849540,"If the PR isn't ready to be reviewed, why is it open?",non_debt,-
geode-native,152,review,151284670,Sounds good to me.,non_debt,-
ignite,8104,review,463530051,"I think this can be no-op, since `RollbackOnly` is always `true`.",non_debt,-
netbeans,130,comment,338476937,"I've rebased the patch on the current master. If there are no objects, I'd like to merge to master tonight or tomorrow.",non_debt,-
trafficserver,1594,review,106927789,"It does not. My initial implementation required this, then I forgot to change this back",non_debt,-
spark,29026,comment,655306439," * This patch **fails to generate documentation**.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
storm,2865,summary,0,STORM-3247 remove BLOBSTORE_SUPERUSER,non_debt,-
beam,5384,review,189163677,"Let's drop this comment for two reasons:
1. First part of the comment is describing the same thing as the code in English, it is not adding additional context.
2. Second part is dataflow runner specific, and the example does not even run on dataflow runner yet.",documentation_debt,low_quality_documentation
calcite,2214,summary,0,"[CALCITE-4106] Consider ""listCoerced"" in TypeCoercionImpl#inOperationCoercion (Jiatao Tao)",non_debt,-
flink,11554,comment,605457410,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit 7f2e408128e5d7929dd4301362a20d8d90c69597 (Sat Mar 28 14:50:34 UTC 2020)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* ‚ùì 1. The [description] looks good.
* ‚ùì 2. There is [consensus] that the contribution should go into to Flink.
* ‚ùì 3. Needs [attention] from.
* ‚ùì 4. The change fits into the overall [architecture].
* ‚ùì 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier",non_debt,-
pulsar,9083,comment,752413384,@eolivelli left my comments.,non_debt,-
spark,17468,comment,290152298," * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
fineract,1564,summary,0,"This PR contains the following updates:
---
-   **ux:** Disabled Execute button while request is in progress ([#&#8203;6776](https://togithub.com/swagger-api/swagger-ui/issues/6776)) ([2bf39e0](https://togithub.com/swagger-api/swagger-ui/commit/2bf39e0ad526336198490122c7978221487a1e35))
-   **sample-gen:** first oneOf or anyOf should be combined with schema ([#&#8203;6775](https://togithub.com/swagger-api/swagger-ui/issues/6775)) ([0f541a1](https://togithub.com/swagger-api/swagger-ui/commit/0f541a1ab055ce104da9758ddbf1a77dcd839c70))
-   **style:** response data flows off the screen ([#&#8203;6764](https://togithub.com/swagger-api/swagger-ui/issues/6764)) ([85a3ec9](https://togithub.com/swagger-api/swagger-ui/commit/85a3ec983e5822f87715d63720e511f8be65f6a9))
-   **examples:** Request Body examples should respect media-type ([#&#8203;6739](https://togithub.com/swagger-api/swagger-ui/issues/6739))  ([68e9b1b](https://togithub.com/swagger-api/swagger-ui/commit/68e9b1b43995a68d40813146aab174228e7a1257))
---
:date: **Schedule**: At any time (no schedule defined).
:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.
:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.
:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.
---
---
This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#github/apache/fineract).",non_debt,-
spark,10411,summary,0,"This documents the implementation of ALS in `spark.ml` with example code in scala, java and python.",non_debt,-
hudi,2500,comment,798849454,Pushed small refactors. Will merge once CI passes,non_debt,-
camel,2206,review,165614505,"Hmm maybe such a type converter is not so idea when you require those stuff to be on the exchange property. Sometimes a type converter is not the best thing to add, they should really ideally just be for basic type conversions. Consider removing this if possible.",non_debt,-
parquet-mr,466,summary,0,20675636-466 summary-0,non_debt,-
zeppelin,2706,comment,353205540,@zjffdu can u confirm it's a flaky test?,test_debt,flaky_test
netbeans,276,summary,0,"Explicit max and min heap size settings removed.
This file (changed in this PR) is the default `.conf` file your application will get when you create an application on top of NetBeans Platform.
See [NETBEANS-149](https://issues.apache.org/jira/browse/NETBEANS-149) for details.",non_debt,-
geode,230,summary,0,GEODE-1744: Probable bugs from == use fixed,non_debt,-
flink,12143,comment,628995582,rebase latest master,non_debt,-
hadoop,1357,review,318728169,"I would prefer `OZONE_SH_OPTS`.  However, this is an existing variable.  I kept its name to avoid breaking anyone's scripts.",non_debt,-
incubator-doris,1610,review,315559959,"OK, Done",non_debt,-
druid,3939,review,102355893,Use log message formatting `%d`,code_debt,low_quality_code
spark,11179,review,52870230,"Use `BDV.vertcat(BDV(Array(gradientLogSigmaSum / totalCnt.toDouble)), BDV(Array(gradientInterceptSum/totalCnt.toDouble)), gradientBetaSum/totalCnt.toDouble)` to combine three parts directly.",non_debt,-
spark,3009,comment,63914127,"  [Test build #23698 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/23698/consoleFull) for   PR 3009 at commit [`6f17f3f`](https://github.com/apache/spark/commit/6f17f3f61102f5685d20cf42f79a049a5bbaad06).
- This patch **fails Spark unit tests**.
- This patch merges cleanly.
- This patch adds no public classes.",non_debt,-
spark,12627,comment,218659906,"Jenkins, retest this please",non_debt,-
pulsar,322,comment,294382800,@msb-at-yahoo - Addressed all your comments,non_debt,-
dubbo,1705,summary,0,[Dubbo-1659] Optimize_hessian_desr_performance,non_debt,-
reef,955,comment,212143854,"LGTM, will test and merge",non_debt,-
airflow,3969,comment,425430405,"# [Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/3969?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/3969?src=pr&el=continue).",non_debt,-
fineract,1584,review,563163346,"@BLasan a thought: instead of using `busybox` and `nc`, why not just re-use `image: mysql:5.7` with `mysqladmin` ?",non_debt,-
shardingsphere,4566,summary,0,Rename TableSegmentsAvailable to TableAvailable,non_debt,-
trafficserver,1482,comment,281828523,FreeBSD build *successful*! See https://ci.trafficserver.apache.org/job/freebsd-github/1602/ for details.,non_debt,-
trafodion,115,summary,0,"Adding .rat-excludes, readme for rat and remove unneeded files lacking copyrights.Also hive/TEST009 fix.",code_debt,dead_code
arrow,5506,comment,535392580,@kou there are a couple of missing artifacts:,non_debt,-
daffodil,334,review,395809427,"Ok, that makes sense!",non_debt,-
pulsar,6903,comment,632959266,/pulsarbot run-failure-checks,non_debt,-
ozone,1007,comment,637932167,Thanks @xiaoyuyao for review. I will merge it in.,non_debt,-
spark,19525,review,149534129,Or we can merge this test case with existing read/write test.,non_debt,-
kafka,2429,comment,275224539,"About fine grained configuration:
Yes, users should not need to think about tasks. It should be on operator basis. Thus, we would need to make standby tasks ""smarter"" and only populate some stores (as requested by the user) in the background. Would be a bigger change, but not impossible.
Yes, that is exactly what I had in mind.",non_debt,-
camel,4687,summary,0,CAMEL-15896: Upgrade google-cloud-pubsub to 1.109.0,non_debt,-
pulsar,318,comment,290524380,"@rdhabalia @saandrews Added 1 commit on top of this PR. Take a look at https://github.com/merlimat/pulsar/commit/08de7c7c47fdb242074b08ddbb81a3fae8b2f72b 
Few things: 
 1. Use proper ref-count sematic for EntryImpl. EntryImpl owns a ref of the buffer (so, while we have an `EntryImpl`, the buffer is alive.
 2. Removed recycling on PositionImpl, this was long due since it's impossible to achieve on the current premises. Use ledgerId/entryId in entryImpl so that is more compact",non_debt,-
incubator-pinot,2890,review,195580463,so we will never delete a segment in realtime for 5 days?,non_debt,-
beam,13927,comment,775452670,"It's part of Dataflow VR test. 
I usually change the validatesRunnerV2 target to include just one test case and if you have write access to apache-beam-testing, you can invoke the test locally by ./gradlew :runners:google-cloud-dataflow-java:validatesRunnerV2 directly.",non_debt,-
pulsar,1788,summary,0,adding windowing function CLI option descriptions,non_debt,-
spark,20068,review,158616591,I think we can just use `spark`.,non_debt,-
trafodion,644,review,73953757,Doesn't look like columnReference is used after this. Is this a memory leak?,design_debt,non-optimal_design
flink,5108,review,154299536,"On second thought, passing in the runtime context wouldn‚Äôt work. To let user custom partitioners  be stateful, we‚Äôll essentially need to make FlinkKafkaPartitioner a CheckpointedFunction, and let the FlinkKafkaProducerBase invoke the checkpoint methods.
We should be able to avoid breaking the user API by having empty base implementations for the checkpoint methods on the FlinkKafkaPartitioner.",non_debt,-
openwhisk,4077,comment,433268766,"This isn't actually a defect - it's true that 11 vs 10 is not perfect, but the reason for ceil/floor vs round is that for small number (say 2 invokers):
The point for small N up to the reciprocal of the fraction, both sets overlap.",non_debt,-
madlib,439,comment,530095343,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/madlib-pr-build/1057/",non_debt,-
kafka,8491,review,408885908,Do we want Scala 2.12 or 2.13 here?,non_debt,-
spark,26108,review,334704586,"I'm a little confused about why this needs to be a config. Isn't this just metadata written to the state store? Why would a user ever need to set this?
I have to go over the rest of the code with more care, since I'm really not familiar with it. But it looks like an insane amount of code just to add one single field to some state object...",design_debt,non-optimal_design
tvm,3156,comment,491091992,"Yes, I will be adding ADTs today, along with tests. Thanks for the reviews",non_debt,-
incubator-mxnet,8015,review,141497578,"yes, let's worry about this later.",design_debt,non-optimal_design
druid,8983,comment,633212891,"This pull request has been marked as stale due to 60 days of inactivity. It will be closed in 4 weeks if no further activity occurs. If you think that's incorrect or this pull request should instead be reviewed, please simply write any comment. Even if closed, you can still revive the PR at any time or discuss it on the dev@druid.apache.org list. Thank you for your contributions.",non_debt,-
nutch,534,comment,642132183,"Yes, that's ok. We'll put a notice about a breaking change to the release notes, so that users having there own indexer plugin know they have to adapt it.
We could try to only extend the IndexWriter interface and provide default do-nothing implementations for newly added methods as most index writers do not write data to the filesystem.",non_debt,-
spark,17079,summary,0,[SPARK-19748][SQL]refresh function has a wrong order to do cache invalidate and regenerate the inmemory var for InMemoryFileIndex with FileStatusCache,non_debt,-
arrow,1635,review,171726050,"Per above, it may be worth writing a ""large memory"" test with the `large_memory` pytest mark (which we can run locally, but not in Travis CI) where we have a field that overflows the 2G in a BinaryArray so we can test the rechunking / splitting of the null bitmap. I guess you'll have to pass a mask to get some nulls to make sure the logic is correct",non_debt,-
trafficserver,7449,comment,767353602,[approve ci Clang-Analyzer],non_debt,-
arrow,9799,comment,806259818,https://issues.apache.org/jira/browse/ARROW-11259,non_debt,-
tajo,113,comment,53052194,"I've updated the patch. I've improved TajoTestingCluster to take system properties as follows:
If `CODEGEN` is a session variable, it will be applied to QueryContext instance used in all unit tests. So, in order to test the code generation feature, you should give `-DCODEGEN=true` when you execute `mvn install`. It can be used for other session variables too.
For test for real queries, you need to set a session variable CODEGEN  as follows:",non_debt,-
spark,21748,comment,406706176,"Anyone know what's happening with this:
@shaneknapp",non_debt,-
kafka,2937,summary,0,Also add tests and a few clean-ups.,code_debt,low_quality_code
spark,8038,review,36684718,"If we change `inputIter`'s initial value to `null`, we need to also change it to a null check.",non_debt,-
calcite,1981,comment,638588987,A friendly ping~,non_debt,-
tvm,7260,review,556711333,70746484-7260 review-556711333,non_debt,-
incubator-heron,3032,comment,437498318,"Overall is ok for me. 
**IT** coverage will be useful so i will be addressing through #3106",test_debt,low_coverage
beam,1212,review,85438597,"It looks like only a couple tests classes are using this-- should they all?
Note that the transforms and tests we have in the core SDK are likely to get copy-pasted many times over their life, for future SDK transforms and by users. Your implementation here establishes the recipe for how to implement and test display data.",non_debt,-
beam,2383,comment,291235194,Coverage increased (+0.003%) to 70.148% when pulling **bc981235946ce768f0ab469922ded96e5a76d3c0 on mariapython:ppp_final** into **af8f586b60853056a20b08d88f7dca72eac657bc on apache:master**.,non_debt,-
pulsar,2981,comment,438904081,"right now, it's not supporting wild-card char such as `*` but 
- consumer can subscribe to any subscription if consumer's principal/role is authorized at namespace-level (`consume`) or per-topic (`consume`).
- however, if per-subscription auth is configured for a specific subscription-name then consumer's `principal/role` has to be authorized role as per `per-subscription` to connect to that specific subscription. so, in `pre-fix sub` usecase, user can grant access to set of roles for a specific subscription and only those roles can access the subscription. so, we can avoid sub-name conflict.",non_debt,-
airflow,6348,review,336572034,Changing it because this way the tense is also in sync with the rest of the headings,non_debt,-
spark,30325,comment,725252148,"Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/35513/",non_debt,-
storm,128,comment,45091287,"I finally made it through all of the code.  It looks good for the most part.  Just a few minor comments.  I also am wiling to maintain/support this code. I have a umber of customers who I know would be very interested in using this, so I would be on the hook for supporting it anyways :)",code_debt,low_quality_code
zeppelin,3497,comment,548180234,"Thanks for the contribution, @amakaur  Could you add unit test  ?",test_debt,lack_of_tests
flink,8607,review,290189678,Done,non_debt,-
carbondata,432,summary,0,"Carbon-core module should not depend on spark, this PR removes this dependency",build_debt,under-declared_dependencies
calcite,1013,comment,466271735,@XuQianJin-Stars Could you please solve the conflicts and squash your commits into a single commit?,non_debt,-
spark,21476,review,192476723,"This is so specific to the way YARN runs things that I don't think it would be useful anywhere else. If at some point it becomes useful, the code can be moved.
I think the tests I added are better than just unit testing this function, since that way the code is actually being run through YARN and bash.",code_debt,low_quality_code
lucene-solr,49,comment,231126560,"I think after addressing the comments I just added, it's probably good to go.  I still don't love the name, especially since it extends BoolFunction it ought to now end with Function.",non_debt,-
superset,12626,summary,0,docs: Updates to Superset Site for 1.0,code_debt,low_quality_code
storm,1608,review,74004298,Can we file JIRA for this? This is a low priority to have any acls user may want to provide.,non_debt,-
airflow,2642,comment,332794100,"# [Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/2642?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/2642?src=pr&el=continue).",non_debt,-
tvm,2082,comment,437256876,"cc @Huyuwei @merrymercy one thing I find is that it is a bit tricky to do topi dispatching in the schedule, do you think if it is possible to fold the depthwise schedule and compute directly into the topi function of conv2d?",non_debt,-
incubator-mxnet,12494,comment,419833188,Just discovered that it has been fixed with #12479 already.,non_debt,-
zeppelin,1613,comment,262817245,@zjffdu @Leemoonsoo I have handled the cases in https://github.com/apache/zeppelin/pull/1678,non_debt,-
incubator-doris,3694,comment,634495845,"Hi, @morningman I have some question about olap_scan_node, could you plz help me figure it out?
1. what's the relationship between scanner and tablet, one tablet can be scanned by many scanner?they are n:1 or 1:1?
2. what's the relationship between TPaloScanRange and ColumnValueRange, is ColumnValueRange to deal with predicate case and TPaloScanRange can indicate that the range a scanner need to scan from StorageEngine's prefix index?
3. What's the usage of function `extend_scan_key` and the config variable `doris_scanner_row_num`?
I would appreciate your reply, thanks~",non_debt,-
airflow,2319,summary,0,[AIRFLOW-1236] SlackPostOperator using Slack Incoming WebHook,non_debt,-
spark,3246,review,22343430,"There are _12_ new overloads of `createStream` on top of the existing 4. This seems like big overkill. There should be one version in Java/Scala that takes all arguments, one each that takes minimal arguments, and any others needed to retain binary compatibility. The rest seem superfluous.",design_debt,non-optimal_design
spark,1276,summary,0,"rdd.id() was returning an Attribute Error in some cases because self._id is not getting set.  So instead of returning the _id attribute, return the value of id() from the jrdd.  Fixes bug SPARK-2334.
Test with: sc.parallelize([1,2,3]).map(lambda x: x+1).id()",non_debt,-
flink,10999,review,374455972,20587599-10999 review-374455972,non_debt,-
spark,21301,comment,388513984,@kiszk . Could you run for all?,non_debt,-
beam,11020,review,387292177,"It's an arbitrary name we give to the jQuery v3.4.1 we imported. It's like a namesapce. Note the magic happens in `window.jquery341 = jQuery.noConflict(true);`.
The problem here is that:
1. A frontend can connect to the kernel at any time: code executed by kernel in the past does not have any effect to new frontends.
2. Multiple frontends can connect to the same kernel: each frontend has its own state (browser: HTML and JS), the rendered HTML+JS cannot assume the existence of any global variable, function definition or libraries.
This ensures no matter how many jQuery gets imported at any time, the interactive notebook always checks and uses the single jQuery configured by interactive modules with Datatable plugin initialized.
And the `function($)` signature ensures that any customized script executed will use `$` as the singleton instance  `window.jquery341`. This ensures that code reading `$` as jQuery will always work.
The advantage of doing this isolation is:
1. The JS imported by interactive modules to any frontend does not alter their existing states. Everything in the notebook still works as it was no matter what libraries and global vars have been used.
2. HTML with JS rendered by interactive modules will have determined behavior because it always uses the same libraries.
3. Whether/when a frontend is connected to the kernel doesn't matter now. The visualization HTML contains everything it needs to setup and/or execute scripts.
4. Arbitrary DOM changes doesn't matter now. Even if the user screws the notebook's HTML, the data visualization broadcast from kernels will always be rendered correctly.",non_debt,-
storm,2597,comment,376861395,Requested changes incorporated. Thanks for the review.,non_debt,-
zeppelin,1484,comment,251847119,Merging it into branch-0.6,non_debt,-
kafka,8846,review,474843042,Should we update the comment to indicate this is the initial backoff time now?,non_debt,-
druid,10464,comment,721179142,This PR has been approved for about 4 days with no further feedback so I am going to go ahead and merge. Thank you for this patch @a2l007 ... I know that my team is going to really enjoy this new API response dedicated to time outs.,non_debt,-
jena,151,comment,252431461,"Should we take a step back and reconsider whether updating the current API in HttpOp is the right thing to do.
Maybe we ought to
- introduce a different style more fluid
- reconsider a design where the caller is responsible for setting up more of the `HttpClient` and `HttpOp` provides operations for nothing special (no auth) + ops that use a redefined `HttpClient`. This is to reduce method bloat.
On (1): something like (quick sketch)
where there are implicit builder objects from, `.get(url)`, `.post(url)`, `.put(url)`, `'delete(url)`.",design_debt,non-optimal_design
pulsar,602,summary,0,Fix #601: Use long when calling curl_easy_getinfo in tests,non_debt,-
flink,6159,review,195443441,"@Nonnegative int keyGroupId, for consistency",code_debt,low_quality_code
trafficcontrol,4671,review,418741562,"I believe the ORT replacement will be installed via RPM -- the 5, 10, 20, however many binaries/things there are going to be, should all be included in the same RPM. At least, that's what I understand out of the blueprint review so far.",non_debt,-
storm,1714,review,80442067,"1. Yes CAST is required because the type of looked up value is Object. While we're saying based on only type on runtime, we might need to consider the type on schema.
2. Implicit cast is not possible because we can't determine the value type on compile time, especially we use ANY to represent its type.",non_debt,-
spark,2802,comment,61728290,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/22897/
Test PASSed.",non_debt,-
arrow,7737,review,453912928,I think adding it to Array for consistency sounds good,non_debt,-
incubator-pinot,6395,comment,752282798,@snleee Please take a look and see if the interface change is okay for the LinkedIn internal tasks,non_debt,-
druid,5299,review,168384306,Removed,non_debt,-
kafka,6935,review,293618884,"Could we consolidate actions in common for version 4 and 5?
Like",non_debt,-
trafficserver,2300,comment,318132384,"It seems possible to collapse at least some of this class hierarchy, but I was told that there's future plans for making subclasses of the PrinterIface class. There just seems like a lot of class hierarchy (up to 4) for such a small amount of code, and I really don't think we should make provisioning now, for something that *might* be done later. If it becomes an issue later, it's easy to refactor (don't do premature ""abstractions"" :-).",code_debt,low_quality_code
flink,6906,review,228072160,done,non_debt,-
apisix,2036,review,487762501,why call `res_fun` twice?,documentation_debt,low_quality_documentation
airflow,1966,summary,0,"Now allows to store the credentials in the keyring
of the OS. Retains backwards compatibility.
Testing Done:
- Tested with a PR
@jlowin",non_debt,-
trafficserver,2172,review,123165279,"This is checking length of encoded data, right? It looks like some Huffman code has 3 byte boundaries. Is this should be 4? ( e.g. `|11111111|11111111|11111110|0010         fffffe2  [28]`)",non_debt,-
kafka,3325,review,128649545,"Instead of using this separate class and do the `instanceof` check on each call (which maybe expensive), maybe we could just have a `WrappedBatchingStateRestoreCallback` which only takes the non-batching `StateRestoreCallback` in constructor and then in `restoreAll` always do the for-loop, and in places that we need it (seems we only have two callers) we can do sth. like
Just once.",code_debt,low_quality_code
kafka,5049,comment,391065272,@ijuma Thanks! LGTM. Merged to trunk.,non_debt,-
airflow,4826,comment,468996164,"# [Codecov](https://codecov.io/gh/apache/airflow/pull/4826?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/4826?src=pr&el=continue).",non_debt,-
incubator-mxnet,11213,review,194908752,ONNX models are context agnostic. For running an ONNX model you will import ONNX model to ur framework model format i.e sym and params  for MXNet. Now you use mxnet api to set your context and either retrain this model(i.e ignore the params file ) or use it for inference.,non_debt,-
spark,1505,comment,49694965,QA tests have started for PR 1505. This patch merges cleanly. <br>View progress: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16941/consoleFull,non_debt,-
flink,11862,review,415500318,"both planners have checked whether a sink is an `OverwritableTableSink`. 
I will add some tests about override mode",non_debt,-
spark,8093,review,36782853,"Hmm... you'll also need to change `taskEndReasonFromJson`, otherwise the history server won't see the new information. Also, when adding new properties to serialized classes, we generally use `Option[Foo]` (see calls to `Utils.jsonOption` in this class).",code_debt,low_quality_code
carbondata,2836,comment,431731434,so we should not change this value without better knowledge about it.,code_debt,low_quality_code
superset,10728,review,479692093,"nit: commented out code bothers me. It's in a storybook, though, so I won't strongly object.",code_debt,low_quality_code
spark,20894,comment,375960919,"Yea, I got the point. So, the case is when some files have different header.  I was thinking of a case when all files have the same header but the user provided a wrong order of schema (comparing to those all headers).",non_debt,-
flink,10173,comment,553341813,"Meta data
Hash:ed7277689d9a82134d631fe144f0add5ace6380f Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/136299450 TriggerType:PUSH TriggerID:ed7277689d9a82134d631fe144f0add5ace6380f
Hash:25b3f3db03ffe6f5dbe243e4c1a53479093610bf Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/136363611 TriggerType:PUSH TriggerID:25b3f3db03ffe6f5dbe243e4c1a53479093610bf
-->
* ed7277689d9a82134d631fe144f0add5ace6380f : SUCCESS [Build](https://travis-ci.com/flink-ci/flink/builds/136299450)
* 25b3f3db03ffe6f5dbe243e4c1a53479093610bf : SUCCESS [Build](https://travis-ci.com/flink-ci/flink/builds/136363611)
  The @flinkbot bot supports the following commands:
 - `@flinkbot run travis` re-run the last Travis build",non_debt,-
dubbo,4202,comment,497193608,"# [Codecov](https://codecov.io/gh/apache/dubbo/pull/4202?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/dubbo/pull/4202?src=pr&el=continue).",non_debt,-
spark,663,comment,42368011,Merged build finished.,non_debt,-
trafficserver,4302,review,222111613,"right.  
The Catch unit tests test only the v4 calculation. Your changes are in the plugin which will be hard to test from the unit-test we have. I guess you would need to create AuTest test which you will have to be created from scratch for this plugin (it will be great but it is up to you).",non_debt,-
groovy,23,review,32766889,Are the CompileStatic annotations here and in the following test necessary? BugsSTCTest extends StaticTypeCheckingTestCase.,non_debt,-
drill,1892,review,363140288,"You probably *really* don't want to do this. Converting the response to a string causes the client to download the **entire** response to memory, then convert it to a Java string. Doing so completely negates the disk-based buffering that seems to be implemented below.",code_debt,low_quality_code
couchdb,597,review,125812352,"You can avoid `case` double-fold here with guards in condition, something like",non_debt,-
spark,15717,comment,262736803,@gatorsmile Sure I'm fine with that. Thanks!,non_debt,-
trafodion,203,comment,161557545,Test Failed.  https://jenkins.esgyn.com/job/Check-PR-master/162/,non_debt,-
spark,5031,comment,91972864,"  [Test build #30100 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/30100/consoleFull) for   PR 5031 at commit [`3e9ac16`](https://github.com/apache/spark/commit/3e9ac16a8e2e0bdd11f850428115bdc891874b1e).
- This patch **fails Scala style tests**.
- This patch merges cleanly.
- This patch adds no public classes.
- This patch does not change any dependencies.",non_debt,-
kafka,8615,comment,624857586,"Thanks for the review. Since we are adding a unit test, move the argument checking in checkArgs()
Edit: Since the existing tests are assuming that alter user entity type is valid with bootstrap server, revoke the change to the original place and test with the command entry point main()",non_debt,-
beam,6676,review,233138480,"Looks like doc was outdated, corrected it, batchSize controls elements used in batch INSERT SQL, i.e INSERT INTO t(a, b, c) VALUES(x1, y1, z1), ..., (xN, yN, yN); Batching can speed up loading data greatly, but not all SQL database may support it, so controlling batch size gives some lever for various SQL vendors.",documentation_debt,outdated_documentation
carbondata,3116,comment,458964957,@xubo245 I thought that there is no need to add test cases for boolean type properties validation.Previous test cases is enough to this validation.,non_debt,-
tvm,6481,summary,0,Add new target style to apply clang-format style,non_debt,-
spark,4363,review,24212525,nit: indent two spaces,code_debt,low_quality_code
spark,18555,comment,317694973," * This patch **fails to build**.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
incubator-pinot,3356,review,226710943,s/if only cares about/if you need only/,non_debt,-
spark,23181,review,237952522,nit: `clone()`,code_debt,low_quality_code
flink,7759,review,258458285,"consider converting the following conditions to methods, this would assign a name to the condition, and possibly would make it a bit easier to test.",code_debt,low_quality_code
pulsar,4148,summary,0,"Update testng ver
Update testcontainers ver
Remove arquillian references
Remove jmockit references",non_debt,-
spark,17001,review,105565378,How about?,non_debt,-
carbondata,1828,comment,359700190,"SDV Build Success , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3038/",non_debt,-
spark,23883,review,261809314,do you mean we support `DESCRIBE QUERY TABLE t`?,non_debt,-
incubator-doris,3336,summary,0,"when add a parition with storage_cooldown_time property like this:
alter table tablexxx ADD PARTITION p20200421 VALUES LESS THAN(""1588262400"") (""storage_medium"" = ""SSD"", ""storage_cooldown_time"" = ""2020-05-01 00:00:00"");
and show partitions from tablexxx;
the CooldownTime is wrong: 2610-02-17 10:16:40, and what is more, the storage migration is based on the wrong timestamp.
The reason is that the result of DateLiteral.getLongValue is not timestamp.",non_debt,-
spark,12241,comment,207105529,"To fix MiMa, you need to add a line to the `mimaProjects` definition to temporarily exclude the new project: https://github.com/dbtsai/spark/blob/b9870d6c62d75cd59698bd72751bc4f7854cd2e3/project/SparkBuild.scala#L254
Also, add a TODO and followup task for post-2.0-release to remove the exclude once 2.0 has been published and there is a previous artifact for MiMa to compare against.",requirement_debt,requirement_partially_implemented
incubator-brooklyn,315,comment,62530359,"Apparent cause of failure
[incubator-brooklyn-pull-requests] $ /home/jenkins/tools/java/latest1.6/bin/java -Xmx26 -Xms256m -XX:MaxPermSize=512m -cp /home/jenkins/jenkins-slave/maven3-agent.jar:/home/jenkins/tools/maven/apache-maven-3.0.4/boot/plexus-classworlds-2.4.jar org.jvnet.hudson.maven3.agent.Maven3Main /home/jenkins/tools/maven/apache-maven-3.0.4 /x1/jenkins/jenkins-slave/slave.jar /home/jenkins/jenkins-slave/maven3-interceptor.jar /home/jenkins/jenkins-slave/maven3-interceptor-commons.jar 37905
Error occurred during initialization of VM
Incompatible minimum and maximum heap sizes specified
ERROR: Failed to launch Maven. Exit code = 1
Invalid configuration of Jenkins job",non_debt,-
druid,1535,summary,0,Update alphanumeric sort docs + more tests / examples,test_debt,low_coverage
storm,1016,review,49783732,will remove,non_debt,-
spark,1434,review,15685887,"very cool.  however, as part of the simplification effort, i remove this altogether.  i'm only creating 1 dstream",non_debt,-
qpid-dispatch,995,summary,0,DISPATCH-1926: Added code to selectively allow deletion of http:yes l‚Ä¶,non_debt,-
spark,13565,review,66319324,"Pretty sure that such a long duration isn't really necessary, but I don't think it hurts to make it longer just in case.",code_debt,low_quality_code
ambari,1002,summary,0,"Restart JN with services up and running is not a good idea and can result in potential data loss.
Since we do not have server side ability to Stop All except ZK, here is new steps needed:
Stop All Services
Reconfigure
Install NN
Install ZKFC
Start ZK
Start JN
....
  21517 passing (34s)
  48 pending",non_debt,-
spark,976,comment,45188866,Merged build finished. All automated tests passed.,non_debt,-
hawq,1357,comment,388497218,Merged to Master,non_debt,-
samza,117,review,111880574,Shouldn't you wait until all processors are running (have called onStart) before setting ApplicationStatus to running?,non_debt,-
incubator-doris,3587,review,425210765,The second parameter is better to be a complete sentence.,code_debt,low_quality_code
carbondata,2254,comment,386807053,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/4737/",non_debt,-
spark,9118,comment,148409085,"LGTM too, the tests seem to have passed as well.",non_debt,-
spark,6015,comment,100359571,Merged build finished. Test PASSed.,non_debt,-
spark,26477,comment,569146827," * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
spark,29089,comment,658349453,LGTM. I just had one question in the tests.,non_debt,-
beam,12490,review,469488264,Ack.,non_debt,-
flink,8889,summary,0,"Now, for all hadoop related dependencies, we just use the single jar of flink-shaded-hadoop-2-uber, instead of depending on individual hadoop jars. It's much more convenient for users to use now.
To summarize all flink-connector-hive's dependencies, which are all of provided scope:
hive-metastore
hive-exec
flink-shaded-hadoop-2-uber
- adds a few more dependencies for HiveCatalog in flink-connector-hive to connect to a remote hive metastore service
We need to add end-to-end test in order to cover such work. cc @lirui-apache @zjuwangg",non_debt,-
incubator-weex,1368,summary,0,"add extMsg when reproteror for containerinfo
use case :",non_debt,-
incubator-mxnet,12740,summary,0,Fix regression in MKLDNN caused by PR 12019,non_debt,-
spark,21498,comment,405398355,I'd like to close this for now. Wait for necessary change on statistics.,non_debt,-
ambari,2868,summary,0,"[AMBARI-25194] Increase the Agent cert validity to 3 years
Manually tested by updating the ca.config file.",non_debt,-
tajo,781,comment,142859287,"+1 LGTM
I've found that this patch ran as expected on testing cluster.",non_debt,-
carbondata,1920,comment,373254604,"Build Success with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/3125/",non_debt,-
airflow,12459,comment,729924740,The PR should be OK to be merged with just subset of tests as it does not modify Core of Airflow. The committers might merge it or can add a label 'full tests needed' and re-run it to run all tests if they see it is needed!,non_debt,-
spark,11108,review,56434392,"- Okay, I see why we don't want to have `println` lines in the example now. If it is just one line of code, I prefer putting it inside the example.
- This won't work on a distributed server. We need to collect them first.",non_debt,-
beam,1412,comment,272051002,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6522/
--none--",non_debt,-
spark,20194,review,161000312,"Impala is the only reference I can find https://www.cloudera.com/documentation/enterprise/5-10-x/topics/impala_show.html
To be consistent with the other SHOW function, we can make LIKE optional?",code_debt,low_quality_code
spark,1218,review,15828037,"@mridulm yes I'm aware of that. But before, SparkContext was using reflection to instantiate two different classes in the yarn package, and then connect them manually. I removed one of those (see that there's still reflection code to load `YarnClusterScheduler`) because it seemed unnecessary.",code_debt,dead_code
flink,4364,review,128512830,"In very rare cases, it might. I want to change the `Execution` a bit on the `master` to make this unnecessary.
However, that is too much surgery in a critical part for a bugfix release, so I decided to be conservative in the runtime code and rather pay this price in the tests.",code_debt,complex_code
spark,14553,comment,250657559,"- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.",non_debt,-
beam,13723,comment,762779925,"@dandy10 I tried to use beam from this PR but I ended up with the same error still.
I simply cloned the repo, installed dependencies from build-requirements.txt and ran setup.py which completed successfully. Did I miss something or do I have to build anything else as well?",non_debt,-
spark,29712,comment,691362271," * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
druid,3956,comment,281423634,"Well, it's definitely sketchy to be calling aggregate and get concurrently. HyperLogLogCollector isn't thread safe. It's possible that you'll get bizarre values from time to time, like if the offset of an HLLC is in the process of being incremented in one thread while it's¬†being read in another thread. So I think this PR has value.",requirement_debt,non-functional_requirements_not_fully_satisfied
trafficserver,6379,review,373984797,i think this is already defined in #6317 so it should not be here again ?,code_debt,low_quality_code
arrow,4476,review,291815907,"The changes to update to TS@3.5.1 were relatively minor changes to the type mapping interfaces, and are all in this commit: https://github.com/trxcllnt/arrow/commit/492d1f111bb78b75ca79790beb4654119af93d51#diff-3cc15e0c860bac3718152b2c143638ad
I don't disagree on preferring this to be split out and handled separately, but practically speaking I was working under a number of constraints that made it more difficult. First, TypeScript released v3.3+ with breaking changes to type resolution halfway through the process of working on these features (leading to [this issue]( https://github.com/apache/arrow/issues/4452)).
Second, I was using the new Builders in app code as they were being developed. The apps needed TS 3.5, but wouldn't compile without these fixes, so I had to update Arrow. But the Builders branch was sufficiently different and would have been difficult to track and constantly rebase with that change (and wasn't even done yet), it was easier to do it here.",non_debt,-
spark,7626,review,35370290,Yea this is right now used by a lot of test code. Somewhat annoying to update those.,design_debt,non-optimal_design
beam,2417,comment,294992173,Run Dataflow ValidatesRunner,non_debt,-
incubator-pinot,6004,review,496927468,I tried and don't see a better way to organize the code. We need to have slightly different logic for each primitive type,design_debt,non-optimal_design
kafka,5451,review,209621584,I'm not sure I follow completely.  Do you mean the sections in common? As we can't test the entire topology description as it has changed from the optimization.,non_debt,-
spark,3082,comment,61603087,Network-related parts LGTM,non_debt,-
spark,11283,review,54821968,"Since `GROUPING SETS((year, month), (year), ())`  is valid SQL",non_debt,-
tvm,1695,review,219359481,"@tqchen So we actually pass fcreate(json, libmod, device_type_id[0], device_type_id[1], *device_type_id_others), right?",non_debt,-
nifi,4303,review,432322027,"This looks a little strange to my be retrospect, but the documentation says ‚ÄúTo reduce the amount of time admins spend on authorization management, policies are inherited from parent resource to child resource‚Äù, so I think in practice the case where you cannot access the process group but you can access a resource within should not happen.",documentation_debt,low_quality_documentation
spark,5806,comment,97712608, Merged build triggered.,non_debt,-
kafka,4135,comment,339793030,"@mjsax, would appreciate your feedback on this. Thanks.",non_debt,-
spark,32053,comment,814607664,"Kubernetes integration test status failure
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/41565/",non_debt,-
trafficserver,766,comment,229453154,FreeBSD build _successful_! See https://ci.trafficserver.apache.org/job/Github-FreeBSD/385/ for details.,non_debt,-
ignite,4446,summary,0,IGNITE-5103 - Server drops client node from cluster when no metrics u‚Ä¶,non_debt,-
flink,13316,review,491921585,"Then, I would suggest using `shuffle` as a prefix and omit the `netty` part. I tried to align the REST API response with the metrics naming here.",non_debt,-
brooklyn-server,722,comment,307025231,retest this please,non_debt,-
trafodion,916,comment,273313051,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1540/,non_debt,-
carbondata,3134,comment,471892216,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2677/",non_debt,-
incubator-mxnet,13201,summary,0,[MXNET-1187] Added Java SSD Inference Tutorial for website,non_debt,-
spark,26918,comment,593960934," * This patch **fails Spark unit tests**.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
beam,8429,comment,508841026,"This pull request has been closed due to lack of activity. If you think that is incorrect, or the pull request requires review, you can revive the PR at any time.",non_debt,-
kafka,7255,review,318235376,I've changed the `TestUtils.createServer()` to include the `threadNamePrefix` when creating a KafkaServer in tests.,non_debt,-
spark,19607,comment,347107409,retest this please,non_debt,-
zeppelin,2079,comment,283460224,merging if no more comment,non_debt,-
hive,2138,summary,0,"df80c15 - copy-pasting sources from `iceberg-hive-metastore` 0.11.x, this commit **should not need a review**
**Please review:** fff81a7 - Changes needed compared to Iceberg repo to make tests pass:
- Extra conf setting in TestHiveMetastore#initConf: https://github.com/apache/hive/commit/fff81a77632995b25d930b85f50a3bf49a46bc7f#diff-454838f58a7e8537062a492232ebeaff66cb75051a4d2591ce54bbd1a1779fa5R223
- Database location URI behaves differently in upstream Hive: https://github.com/apache/hive/commit/fff81a77632995b25d930b85f50a3bf49a46bc7f#diff-70672eb7a795f778b743e1965d02f66a0d4f918c551dadcefd9b4864e9a93522R555
and https://github.com/apache/hive/commit/fff81a77632995b25d930b85f50a3bf49a46bc7f#diff-5173630509f49d2e68c1f0b06ea0d47166ff2e7dd0154bf24c763662546d90e6R405
Added the same checkstyle files as for iceberg-handler.
Moved `AssertHelpers` to iceberg-catalog test sources.
Deleted `TestHiveMetastore` from iceberg-handler (since it's already in iceberg-catalog) but ported over the changes from the previous commit.",non_debt,-
carbondata,1714,summary,0,"Add version for CarbonData
Are there any other good idea for CarbonData version?
        - Whether new unit test cases have been added or why no new tests are required?
        - How it is tested? Please attach test report.
        - Is it a performance related change? Please attach the performance test report.
        - Any additional information to help reviewers in testing this change.",non_debt,-
hadoop,2169,comment,663908887,"Thanks, @aajisaka. I merged this.",non_debt,-
incubator-heron,806,review,64695958,"done, see #809",non_debt,-
shardingsphere-elasticjob,1608,summary,0,For checkstyle & fix typo,documentation_debt,low_quality_documentation
nifi-minifi-cpp,807,review,452334340,feel free to mark this and all of the above as resolved,non_debt,-
tajo,618,comment,122178566,"Hi @jihoonson 
Thanks for your review of great depth. 
As you commented, above case will be resolved automatically in TAJO-1346. I'll commit it to the master branch soon. :-)",non_debt,-
activemq-artemis,2272,review,213342991,Above threshold?,non_debt,-
incubator-mxnet,19725,summary,0,Update CD Jenkins config for include/mkldnn/oneapi/dnnl,non_debt,-
spark,19143,comment,327427666,"You haven't actually added tests, and that's not all this PR does. At the least, this doesn't match the intent you describe, and should be closed. I'd back up and describe the test you want in the JIRA.",test_debt,lack_of_tests
trafficserver,2578,comment,335346485,[approve ci],non_debt,-
trafficserver,4894,comment,459190361,"To the first point, I ended up splitting on protocol and then client/session.  So there is a ProxySesion.  Then there is a Http1Session and Http2Session that inherit from that.  Then there is Http1ClientSession  and Http1ServerSession, etc.  It turns out there was more commonality within the protocol, so you got more sharing by splitting on the protocol first.
To the second point.  I originally had a Client and Server versions of the protocol specific Transacation classes, but eventually there was really no difference between the two, so a trimmed that class hierarchy back.",non_debt,-
arrow,7994,review,478679324,"Oh I see, it gets caught and rethrown below",non_debt,-
spark,31198,comment,762392793,"Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/38790/",non_debt,-
airflow,11685,comment,716582827,It's rebased.,non_debt,-
trafficserver,866,comment,240594556,"I'd say it is not sufficient.
As a workaround for most cases, it works, and personally I'm OK with it as is. However, I think the behavior of `TSStringPercentEncode` shouldn't be changed because it's just a workaround for logging issue.
So, if the API keeps current behavior, then I'm fine with landing this change.",code_debt,low_quality_code
couchdb,726,summary,0,Provide a more accurate size check for max_document_size limit,non_debt,-
samza,566,review,198292707,"ok, got it.",non_debt,-
nifi-minifi-cpp,4,comment,234306309,"@apiri 
It sounds good. Thanks.",non_debt,-
avro,133,review,80818577,"this might break the usage of GetName in exisitng usage.
Not sure if we would want to make the function an internal use only.",non_debt,-
pulsar,9096,review,563300245,"@codelipenghui The `LedgerInfo` class is generated by protobuf, which is already an immutable class, we don't need to do extra things to make it immutable.",non_debt,-
incubator-heron,176,review,56283550,"instead of string formatter - can we use string builder so that we don't have to count the number of formatting arguments - often it takes a couple of iterations to get this right?  String Builder now will be good especially with the CLI options, thoughts?",code_debt,low_quality_code
spark,1114,comment,48992157,"Jenkins, test this please",non_debt,-
beam,13439,review,534645786,Done!,non_debt,-
spark,19222,review,147573040,"Good catch, you are right.",non_debt,-
spark,8531,review,48378751,I'm thinking of taking over this PR and am considering dropping this logic since it adds complexity and might not be a huge performance issue in practice. Let me know if you disagree.,code_debt,complex_code
spark,9353,review,43474236,This is not necessary. See discussion at https://issues.apache.org/jira/browse/SPARK-11337. You can include all imports in a single block or move unused imports to a separate group. We only need to keep imports ordered in each group. Use empty lines to separate import groups.,code_debt,complex_code
druid,4313,comment,306859112,"This PR is the only left in 0.10.1, so what we decide to do? @b-slim @gianm",non_debt,-
carbondata,3424,comment,547247341,"Build Failed with Spark 2.2.1, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.2/716/",non_debt,-
nifi,3558,comment,507417670,"+1, LGTM
Thanks for the contribution, @szaboferee!  Merged to master.",non_debt,-
carbondata,3876,comment,668424168,"Build Failed  with Spark 2.4.5, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.5/1852/",non_debt,-
spark,13270,review,64482317,Added a `__PLACEHOLDER__` suffix to the dummy table location path to fix case one. And made sure that we handle Hive compatible tables properly.,non_debt,-
spark,6323,comment,104419014,"  [Test build #33266 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/33266/consoleFull) for   PR 6323 at commit [`bd4a5dd`](https://github.com/apache/spark/commit/bd4a5dd52e0081ed08d934e9330790b86236a6b1).
- This patch **passes all tests**.
- This patch merges cleanly.
- This patch adds no public classes.",non_debt,-
beam,3790,comment,325988457,R: @kennknowles,non_debt,-
netbeans,320,summary,0,"This utility method determines whether a Tree can be used in places
where a Primary is required (for instance, as the receiver expression of
a method invocation).",non_debt,-
ambari,1108,summary,0,"‚Ä¶ils to start
RetriableException was added to the exceptions when we retry an hdfs operation.
Manually tested on my local cluster.",non_debt,-
nifi-minifi-cpp,924,comment,732347261,I'm pretty sure the answer is yes: we need to include all transitive dependencies' licenses and notices. Otherwise it would be trivial to circumvent these requirements by creating a wrapper project that depends on the software and exposes all of its components.,build_debt,under-declared_dependencies
superset,13210,review,582436382,"`scalar` in linear algebra, relative to `vector`. refer to SQLAlachemy and Pandas variable naming. 
`isArray` more straightforward, I changed this variable name.",code_debt,low_quality_code
bigtop,555,comment,573495530,"In https://ci.bigtop.apache.org/view/Test/job/Build-Deploy-Smoke-Test-Pull-Request-All-Distros/25/:
It seems the permission issues when it tried to delete the directory which was created by 'root'.",non_debt,-
flink,6087,review,192733056,I think maybe this is not a fatal error.,non_debt,-
accumulo,329,summary,0,2524488-329 summary-0,non_debt,-
spark,17608,comment,294076299,"@HyukjinKwon 
Thank you again, and now windows machines can compile apache master.
No my java version is 1.8.",non_debt,-
hive,1780,comment,761269576,Updated PR to improve readability (https://github.com/apache/hive/pull/1780/commits/37e707705c6cc7c025bbe7fb1b67682062dc84c7) and at the same time address HIVE-24646.,code_debt,low_quality_code
activemq-artemis,2716,comment,577397473,"@michaelandrepearce, fair enough.",non_debt,-
couchdb,1972,review,267940882,:-) Np. Thanks for taking the time to look over the PR,non_debt,-
kafka,1995,review,82672441,Amended.,non_debt,-
ozone,894,summary,0,"FileStatus is a Hadoop specific class. The return type of getFileStatus OM call should be Hadoop independent and a simple POJO can be used.
OzoneFileSystem can create the appropriate FileStatus implementation based on the information in this simple POJO.
https://issues.apache.org/jira/browse/HDDS-3501
With full CI build on my fork.",non_debt,-
trafficcontrol,2548,comment,405349698,Can one of the admins verify this patch?,non_debt,-
beam,4161,comment,346207393,R: @tgroh Please review. Thanks,non_debt,-
rocketmq,1536,review,336341159,"When the sync process was notified more than 5 times, it will lead to early return before sync completely, so it seems that it should not depend on the number of notified times.",non_debt,-
flink,13477,summary,0,[FLINK-19406][table-planner-blink] Casting row time to timestamp lose‚Ä¶,non_debt,-
flink,8176,comment,484774952,@KurtYoung the failed test is fixed,non_debt,-
cloudstack,2503,comment,385585396,"I doubt this Jenkins error has anything to do with this PR:
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running streamer.ByteBufferTest
Tests run: 400, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.194 sec - in streamer.ByteBufferTest
Running streamer.BaseElementTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.231 sec - in streamer.BaseElementTest
Running common.ClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.575 sec - in common.ClientTest
Running rdpclient.MockServerTest
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.055 sec <<< FAILURE! - in rdpclient.MockServerTest
testIsMockServerCanUpgradeConnectionToSsl(rdpclient.MockServerTest)  Time elapsed: 1.043 sec  <<< ERROR!
javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)
	at sun.security.ssl.Handshaker.activate(Handshaker.java:529)
	at sun.security.ssl.SSLSocketImpl.kickstartHandshake(SSLSocketImpl.java:1492)
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1361)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397)
	at rdpclient.MockServerTest.testIsMockServerCanUpgradeConnectionToSsl(MockServerTest.java:166)
Error in mock server: Received fatal alert: handshake_failure
javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:154)
	at sun.security.ssl.SSLSocketImpl.recvAlert(SSLSocketImpl.java:2038)
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1135)
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1385)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397)
	at streamer.debug.MockServer.run(MockServer.java:122)
	at java.lang.Thread.run(Thread.java:748)
Results :
Tests in error: 
  MockServerTest.testIsMockServerCanUpgradeConnectionToSsl:166 ¬ª SSLHandshake No...",non_debt,-
zookeeper,361,comment,459616531,retest this please,non_debt,-
spark,13192,review,64440872,are we assuming the comment holder will always take an entire line?,non_debt,-
druid,7789,review,288693182,Null is for backward compatibility (for cases when a specific component is not needed),non_debt,-
druid,9203,summary,0,Backport of #9195 to 0.17.0.,non_debt,-
spark,27560,review,379244083,"I think all DS v2 classes are still evolving. Shall we use `@Evolving` consistently? e.g. https://github.com/apache/spark/pull/27560/files#diff-ab2a72871faeb3ad8bae1d7f951899deR29
cc @brkyvz @rdblue",non_debt,-
flink,2425,review,86549853,"Can we add these options directly via `ConfogOptions` similar to that: https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java
Maybe start a new class, `SecurityOptions`.",non_debt,-
carbondata,2932,comment,440594063,"Build Success with Spark 2.3.1, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/9737/",non_debt,-
flink,14774,comment,768719211,@WeiZhong94 Thanks a lot for the view. Updated the PR.,non_debt,-
flink,12542,summary,0,"This pull request fixed the bug that UDAF with Object Array return type (e.g. Row[]) will generate wrong result.
The problem is we reuse 'reuseArray' as the return value of ObjectArrayConverter.toBinaryArray(). It leads to 'prevAggValue' and 'newAggValue' in GroupAggFunction.processElement() contains exactly the same BinaryArray, so 'equaliser.equalsWithoutHeader(prevAggValue, newAggValue)' is always true.
return a copy of reuseArray in ObjectArrayConverter.toBinaryArray()
This change is a trivial rework / code cleanup without any test coverage.",non_debt,-
tinkerpop,1299,comment,651702036,"With some added changes, this seems to be working now. Running more tests today and then expect to immediately merge.",non_debt,-
ozone,1872,summary,0,"Run configurations startup order
https://issues.apache.org/jira/browse/HDDS-4785",non_debt,-
cloudstack,3241,comment,509613238,Packaging result: ‚úîcentos6 ‚úîcentos7 ‚úîdebian. JID-106,non_debt,-
beam,2945,summary,0,"quickly and easily:
   `[BEAM-<Jira issue #>] Description of pull request`
       number, if there is one.
       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.pdf).
---
Sometimes during the cythonization of beam it finds other files in either target/tox which cause the compilation to fail. We should only pick beam files here.
R: @robertwb PTAL",non_debt,-
kafka,7463,review,333754419,Do we really need to re-initialize committed offsets and task time during Resume?,non_debt,-
spark,15571,comment,255263463,"@JoshRosen I could imagine us introducing this same issue in the future with more Params in sharedParams.scala (like `HasAggregationDepth`).  As long as it's fine for us add MimaExcludes (and are sure Java users won't experience issues), then this seems fine.",non_debt,-
hbase,2448,comment,698750938,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
kafka,8541,review,414900861,"This accounted for most of the test failures, and it's already fixed on trunk.",non_debt,-
attic-stratos,210,summary,0,Adding the command for update network partition,non_debt,-
spark,9280,review,43116722,Removed....,non_debt,-
openwhisk,4921,review,448128928,"Then how do you think about activations requiring transactional supports?
For example, if an action updates data in DB, it can cause multiple updates with at least once semantic.
Do you keep transactional support in your mind as well?",non_debt,-
spark,3651,review,21697554,"OK, in the name of keeping it simple I might not touch this this time. Since this occurs 2 places only, it doesn't save much.",code_debt,low_quality_code
flink,6173,review,196042694,"We have unmodifiable map alternatives in the JDK. If there is not a good reason why we need guava here, I would suggest to solve this without this import. Even in that case, I would use import and not fully qualified classname.",code_debt,low_quality_code
beam,8280,comment,490157112,This LGTM. LMK if I should merge.,non_debt,-
tvm,1406,review,201922072,tvm.abs !!,non_debt,-
systemds,529,comment,316237640,@mboehm7 Can this be merged?,non_debt,-
lucene-solr,2459,review,589262984,"It's a bug, thanks for pointing it out!",non_debt,-
couchdb,1064,summary,0,"     with your text. If a section needs no action - remove it.
     Also remember, that CouchDB uses the Review-Then-Commit (RTC) model
     of code collaboration. Positive feedback is represented +1 from committers
     and negative is a -1. The -1 also means veto, and needs to be addressed
     to proceed. Once there are no objections, the PR can be merged by a
     CouchDB committer.
     See: http://couchdb.apache.org/bylaws.html#decisions for more info. -->
     what problem it solves or how it makes things better. -->
Add couch_stats tracking back to couch_log using stats defined in
https://github.com/apache/couchdb/blob/master/src/couch_log/priv/stats_descriptions.cfg
     Does it provides any behaviour that the end users
     could notice? -->
     repositories please put links to those issues or pull requests here.  -->
issue #832",non_debt,-
netbeans,1477,comment,529485461,Thank you!,non_debt,-
beam,11437,comment,629549669,Run Java_Examples_Dataflow PreCommit,non_debt,-
flink,13957,summary,0,[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema,non_debt,-
trafodion,1064,comment,294060779,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1728/,non_debt,-
activemq-artemis,2012,comment,380818801,"As noted my biggest concern is that message refs need to be as light as humanly possible as they‚Äôre all in memory and affects greatly the scaling.
I would personally prefer the refactor if needed, than take this hit. 
Especially as this is only needed by someone wanting to use this in a plugin. Which means everyone else has to suffer",design_debt,non-optimal_design
spark,25576,comment,524639817," * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
beam,871,comment,241927745,R: @dhalperi,non_debt,-
ignite,7250,summary,0,31006158-7250 summary-0,non_debt,-
kafka,2787,comment,291326176,"Actually, I take back my comment on validation that we don't use the same field name, seems that is separately being tracked in KAFKA-4855.",non_debt,-
camel,2023,comment,335785218,Thanks for the PR it has been merged. Do you mind closing this?,non_debt,-
geode,2238,summary,0,"Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.",non_debt,-
camel,2520,summary,0,study,non_debt,-
iceberg,784,review,377400690,"This needs to return the `Configuration` that was passed in when creating the `HadoopInputFile`, if there was one. The file system's `Configuration` may not contain the same configuration properties as the one that was passed in, and this `Configuration` is needed in some paths.",non_debt,-
spark,14277,review,71648516,ok. `CodegenFallback` does it.,non_debt,-
kafka,1908,comment,250894831,"LGTM. Merging to trunk and 0.10.1. Thanks @edoardocomar and @mimaison for the hard work (and patience) on this patch! I have some minor cleanups/improvements on the client and in testing, which I will submit in a follow-up PR.",code_debt,low_quality_code
superset,8464,comment,547571294,"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8464?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8464?src=pr&el=continue).",non_debt,-
kafka,8683,comment,649518932,@d8tltanc Looks like we haven't add the new configs to Connect: https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3103/,non_debt,-
spark,5252,comment,87749174,"Jenkins, retest this please.",non_debt,-
spark,7065,comment,116488507,Merged build finished. Test PASSed.,non_debt,-
tajo,146,comment,55915147,Thanks for the quick review. I fixed groupby too. Please check it again :),non_debt,-
spark,23062,review,234404871,Spacing here. Is this simpler with .forall?,code_debt,low_quality_code
daffodil,16,review,162372600,"Also related to the grammar, Section 12.3.3 says that implicit lengths are not allowed for packed decimals. Probably binaryNumberKnownLengthInBits needs an SDE based on the binary rep if lengthKind is implicit.",non_debt,-
incubator-brooklyn,321,summary,0,Upgrade to jclouds to 1.8.1,non_debt,-
spark,23459,comment,451639170,Merged to master.,non_debt,-
hbase,1508,comment,613408101,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
incubator-mxnet,17762,comment,610516730,"@mxnet-bot run ci [unix-gpu, windows-gpu]",non_debt,-
incubator-mxnet,7390,summary,0,"The file takes in data in dictionary format, for both predictions and ground truth. 
Ranking problems are very common in several areas of ML, and specifically computing metrics at a certain position is common in many applications. Usually for recommender systems, we don't want to compute the overall precision, but only, say precision@10, because only 10 items might be shown to the user. It makes sense to optimize your model that gets the top entries right, sacrificing overall precision. Similar arguments hold for recall, coverage etc. Having such a standard metric available in MXNet will let more people in the ML community make use of it, and increase it's adoptation.",non_debt,-
storm,177,comment,54259123,"Referring the ReferenceError - it was caused by a bug in randomsentence.js that happens only on case of a ""fail"" for one of the tuples. I fixed the bug (fix committed), still not sure why did you get ""fail"" in the split sentences example.",non_debt,-
carbondata,2712,comment,420515611,"Build Failed with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/413/",non_debt,-
spark,11468,comment,204450309,"@vectorijk Do you have time to update this PR? If not, I can help.",non_debt,-
carbondata,4078,comment,787036665,"@VenuReddy2103
 I checked all places, but 14 places still keep the same with previous.
1) in 9 places, it uses only one event, fireEvent is ok. 
2) in 4 places, preEvent and postEvent are in the different code blocks.
3) in 1 place, there is a large code block between preEvent and postEvent.",code_debt,low_quality_code
spark,23798,comment,467005376,Thanks @HeartSaVioR for the comments.,non_debt,-
spark,24851,comment,515094415," * This patch **fails to generate documentation**.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
incubator-doris,5475,review,595647486,Change to sink's class name,non_debt,-
flink,10358,review,354734758,"Java characters (e.g. char and Character type) have internal UTF16 encoding, meaning that they will always be encoded as 16-bit numbers. But there are some symbols called surrogate pairs, which may require a pair of 16-bit characters to be encoded.
So from the wire format point of view, when you do `""üåâ"".length()` you will get 2 (as there are two underlying 16-bit characters to encode this symbol), but with `""üåâ"".codePointCount(0,""üåâ"".length())` you will get 1 (as there only one single symbol). But the string serializer is not operating on symbols and UTF code points, it is operating on the underlying 16-bit numbers, so it must handle it properly out of the box.",non_debt,-
geode,2350,review,211410999,üëç,non_debt,-
bookkeeper,183,comment,317592597,@kishorekasi @eolivelli what is the plan for this? Are we including this in 4.5.0?,non_debt,-
kafka,4508,comment,363278373,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-test-coverage/359/",non_debt,-
pulsar,5032,review,317796702,"This will require to update this file for each release which is not ideal. We could get the list of versions from `versions.json` though the maven install might take a while to complete.
Another option would be to just build the latest version and make sure we add the version tag, so that we just build 1 version (since the older docs will not change).",design_debt,non-optimal_design
flink,3409,review,107005694,"I see, good point!",non_debt,-
kafka,7781,review,376628980,Done.,non_debt,-
superset,8517,comment,550431368,"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8517?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8517?src=pr&el=continue).",non_debt,-
flink,4199,comment,337176876,I opened a PR for Calcite https://github.com/apache/calcite/pull/549. I will rebase this branch.,non_debt,-
spark,11620,review,56418491,indentation,code_debt,low_quality_code
beam,1637,summary,0,[BEAM-1153] GcsUtil: use non-batch API for single file size requests.,non_debt,-
incubator-heron,1820,review,113760959,Is this the implementation class? If so can we use `class` instead of type to be more explicit?,code_debt,low_quality_code
activemq-artemis,2558,comment,472853473,"I see you added a test, but you still haven't addressed my question about synchronized on .class.
Why is that needed?",code_debt,complex_code
airflow,7038,review,368378012,33884891-7038 review-368378012,non_debt,-
spark,17172,review,105237069,"we can also add a new case for `backToBackFilterLong`, as we handle boolean type now.",non_debt,-
pulsar,2531,comment,419126414,@sijie I did. Are you asking if it's possible to do so?,non_debt,-
hive,585,summary,0,"Fix: [#HIVE-21555](https://issues.apache.org/jira/browse/HIVE-21555).
The cached thread pool is proper when the task is light-weight and the number of sub-threads are not too many. Otherwise, fixed thread pool should be used to avoid OutOfMemoryError.",non_debt,-
incubator-pagespeed-ngx,391,summary,0,keepalive-tests: add more keepalive tests,non_debt,-
shardingsphere,7353,review,486951060,Remove.,non_debt,-
tinkerpop,712,review,140818483,"Even though `Bindings` is based on a `ThreadLocal<T>` instance, I think the implementation is not thread-safe.
For example: Multiple tasks executed serially on the same thread, modifying the same bindings dictionary.
Besides not being thread-safe, it doesn't support defining a binding on 1 thread and adding the step on another, sample:",requirement_debt,non-functional_requirements_not_fully_satisfied
arrow,5562,comment,571541474,"Yes (in an updated form of course), see https://github.com/apache/arrow/pull/5562#issuecomment-553782658
@kszucs what do you mean exactly with the task integration tests?",non_debt,-
spark,908,summary,0,Suggest workarounds for partitionBy in Spark 1.0.0 due to SPARK-1931,non_debt,-
carbondata,2589,summary,0,[CARBONDATA-2825][CARBONDATA-2828] CarbonStore and InternalCarbonStore API,non_debt,-
spark,10278,comment,164213638,"- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.",non_debt,-
flink,10399,review,354658627,`java.time.Instant.class` -> `Instant.class`,non_debt,-
hadoop,2056,summary,0,"Contributed by: Mehakmeet Singh
Tested by: mvn -T 1C -Dparallel-tests=abfs clean verify
Region: East US, West US",non_debt,-
trafodion,1249,comment,333286141,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2092/,non_debt,-
airflow,6327,review,335973293,"Yes.
True - that would also not break backwards compatibility.",non_debt,-
spark,28856,comment,646492969," * This patch **fails to build**.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
ignite,2683,summary,0,31006158-2683 summary-0,non_debt,-
arrow,124,comment,242861570,@wesm Here is a rebased PR (since part of this was used in ARROW-264).,non_debt,-
spark,30973,review,551081871,The @HyukjinKwon idea looks better and I think it's easier for users to use this new feature without any configuration.,non_debt,-
incubator-mxnet,9035,review,156499301,doesn't look like a good idea to use python. Why not build.bat?,non_debt,-
incubator-brooklyn,227,comment,58339657,"[incubator-brooklyn-pull-requests #74](https://builds.apache.org/job/incubator-brooklyn-pull-requests/74/) SUCCESS
This pull request looks good",non_debt,-
thrift,1468,review,161387735,Are these hard or soft dependencies?  There are newer versions of these.,non_debt,-
spark,27226,comment,574962770," * This patch **fails Scala style tests**.
 * This patch merges cleanly.
 * This patch adds no public classes.",non_debt,-
incubator-dolphinscheduler,3729,summary,0,"[Feature-3633][server,common] Let dependent nodes wait for a while before the pre-dependency starts",non_debt,-
arrow,7465,comment,645152229,https://issues.apache.org/jira/browse/PARQUET-1877,non_debt,-
spark,14078,comment,231427333,good to go :+1:,non_debt,-
shardingsphere,6757,summary,0,49876476-6757 summary-0,non_debt,-
spark,4906,comment,84196807,"Your query may be caused by my confusion above.  To answer your query, the prediction F(x) should be the raw prediction, not the discrete -1/+1 value.",non_debt,-
hawq,1321,summary,0,"Rewrite the tuple construct and consume work flow to improve the read and write performance in pluggable storage framework. To be specific, it uses virtual tuple in tuple table slot instead of heap tuple between storage and executor to avoid tuple data copy during query processing.
@huor @jiny2",code_debt,slow_algorithm
spark,23260,comment,455328233,"@squito Yes, exactly. Nice summarization.
IMHO there're some cases we still want to get custom log URL while the status of application is ""in progress"", because according to the logic on determining whether the application is finished, app could be shown as ""inprogress"" when the app terminated unexpectedly.
https://github.com/apache/spark/blob/0b3abef1950f486001160ec578e4f628c199eeb4/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala#L98-L100
In this case, showing origin log URLs may not work but showing custom log URLs would work if the external log service gathers executor logs continuously rather than gathering logs when app is finished - I expect the behavior of external log service as former not latter. Even if app is still running, app UI will still provide origin log URLs so we can get it from there instead of SHS.
What I agreed to make change on only SHS was my prev. patch was making log urls being static again which is fragile if there's a change on external log service. Now we just require end users to modify the configuration and restart SHS (unless there's a cache mechanism to avoid re-reading events when restarting SHS.) IMHO, showing custom log URLs even for apps which are shown as ""inprogress"" still makes sense to me. 
Btw, I didn't add variables for other resource managers as they don't provide log URLs for now. (If possible we may be better to address it as well via separate PRs.)",non_debt,-
kylin,1534,comment,753489521,"# [Codecov](https://codecov.io/gh/apache/kylin/pull/1534?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/kylin/pull/1534?src=pr&el=continue).",non_debt,-
jena,395,comment,380796885,"Used `luke` to look at the Lucene index created, and everything looked OK.",non_debt,-
attic-apex-malhar,618,review,116693824,Corrected other occurrences. Next PR update should reflect the changes.,non_debt,-
spark,13047,comment,218411251,Can one of the admins verify this patch?,non_debt,-
arrow,6863,review,404923143,?,non_debt,-
spark,20047,comment,353410377,Looks good but you have to fix conflicts now.,non_debt,-
gobblin,339,review,41068350,We probably don't want to do that because in the future the number of contains will change at runtime and `containerMap.size()` will no longer be a good indication.,non_debt,-
phoenix,677,review,366069653,same as above.,non_debt,-
spark,1337,comment,48673768,QA tests have started for PR 1337. This patch merges cleanly. <br>View progress: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16529/consoleFull,non_debt,-
zeppelin,827,review,61180039,not sure I follow - why don't we `return r.code()` directly?,non_debt,-
lucene-solr,1749,summary,0,LUCENE-9462: Fields without positions should still return MatchIterator.,non_debt,-
kafka,5519,comment,414969484,"@ijuma If I understand your comment correctly, you are talking about calling `RequestBuilder.build()` only once to address this issue without modifying the Request class itself. Here are my thoughts:
- Controller has separate request send threads talking to brokers and each of them contains a separate network client. Although we pass in the same request builder to each of the network clients, we cannot ensure that `RequestBuilder.build()` is called only once because we currently don't maintain states across these network clients. However, we can explicitly cache the request object in the RequestBuilder to avoid re-instantiation (this is what this PR did).
- Even if we can ensure `RequestBuilder.build()` is called only once and we use the same request object in the network clients, in the current implementation we will still create multiple Struct objects each time we serialize the request when network clients are trying to send out the request. To avoid that, this PR caches the Struct object and re-uses it when `toStruct()` is called.
Although this is a general optimization for all types of request, this PR only changes `UpdateMetadataRequest` instead of `AbstractRequest` because `UpdateMetadataRequest` is the only use case where we send out requests with the same payload for multiple times.",non_debt,-
tvm,2969,review,272690624,"I think for docs to be generated, you need to capture them as `#[doc=$doc]` like [this](https://github.com/rust-ndarray/ndarray/blob/master/src/impl_ops.rs#L51) for example.",non_debt,-
arrow,6836,review,404507509,Agreed.,non_debt,-
drill,1953,review,379551973,"Thanks, replaced as you proposed, but also left mentioning that we have metadata about segments, files, row groups, partitions since it wasn't described in this doc yet.",documentation_debt,low_quality_documentation
orc,570,review,528911518,Could you add `@deprecated` to give a proper warning?,code_debt,low_quality_code
spark,25006,comment,532946412,"nope, that should be fine enough.",non_debt,-
spark,12969,review,62413154,"Are you trying to handle the case where the map is empty? why not handle it directly then?
Just needs one new line of code, like: `require(distinctMap.nonEmpty)`",non_debt,-
spark,2841,comment,59612787,Can one of the admins verify this patch?,non_debt,-
zeppelin,332,summary,0,"We use web-socket api for creating and cloning notes from our servers. To know noteId of created or cloned note, we need some response message. I propose sending NOTE message to client on creating/cloning note.
There could be other ways to reach it. For example, we can send newId and note for creation an id, name, newId for cloning. If newId is not exists, we generate it on server.
Another way will be to create separate api for this, to not mix this with front-end api. It could be REST or smth.",non_debt,-
bookkeeper,2503,comment,737383375,@eolivelli Looks like some tests will need refactoring.,non_debt,-
calcite,2286,review,534475582,"What about using `rules(RelTrait in, JdbcConvention out, RelBuilderFactory relBuilderFactory)` here, but with null `in` value and adding if statement in that other overloaded method?
It allows slightly avoid duplication of code",code_debt,duplicated_code
hadoop,574,comment,471147658,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
tvm,6536,review,493576190,"Given this is repeated several times, do you think it's worth disabling for the whole file?",non_debt,-
spark,25782,comment,531643617,LGTM if tests pass.,non_debt,-
incubator-heron,1629,review,97181411,done via https://github.com/twitter/heron/pull/1629/commits/4fe8a8f834e984ba6cc1e3eff71e5932186cbaee!,non_debt,-
spark,7643,review,36024377,"I tried
The return result is `1`. I think you put `enddate` and `startdate` in the wrong order.
The format is `datediff(string enddate, string startdate)` (https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF).",non_debt,-
airflow,1831,review,87747258,Thx @jackar,non_debt,-
trafficcontrol,4447,review,384053483,woot! making perl better :D,non_debt,-
trafficcontrol,4718,review,430736160,done in bae7ca548739e0d9fe501afba8b2c2559b7843f9,non_debt,-
spark,1151,comment,47764564, Merged build triggered.,non_debt,-
beam,6623,comment,428383967,Will merge after tests pass. Thank you for the quick review.,non_debt,-
ozone,1489,review,513169793,Do we need LongAdder still?,non_debt,-
druid,466,comment,41075002,Thanks guys!,non_debt,-
kafka,5860,review,230097399,agreed,non_debt,-
openwhisk,3449,comment,373702523,"What a mess...
On Mar 16, 2018 5:12 AM, ""Carlos Santana"" <notifications@github.com> wrote:",code_debt,low_quality_code
spark,22754,review,227392626,"So strange, l have handled it, but we can not see the change here",non_debt,-
spark,15971,review,89209660,"This is pretty hacky. It makes assumptions about how things are formatted in the event log.
The previous assert should be enough (ignoring my previous comment about changing this test).",code_debt,complex_code
spark,23559,review,249274532,I think we can make it inlined.,non_debt,-
trafficserver,6620,comment,614369514,@masaori335 Do we need this in 8.1.x as well ?,non_debt,-
beam,1981,review,100672648,Done.,non_debt,-
airflow,9149,summary,0,[AIRFLOW-7014] Add Apache Kylin operator,non_debt,-
pulsar,2464,comment,417559636,run cpp tests,non_debt,-
beam,539,comment,228871382,"My fault. I should open a JIRA issue before I start on this. I thought it is a quick change.
For the WordCount part, I like to move it to a top level class, and document why Spark runners need its own copy. (+@dhalperi for Read.from doesn't work for HDFS)
For the Tfidf part, I think I am close to run it with Spark runner in beam-examples module. #533 
I am having a spark dependency problem in the test (JavaSparkContext class not found):
https://builds.apache.org/job/beam_PreCommit_MavenVerify/1908/console
Could you help me to take a look? And, you can make a call on whether you want to include the Tfidf in this PR or left to me to handle in #533.",documentation_debt,outdated_documentation
skywalking,701,comment,353835231,@wu-sheng no tks!,non_debt,-
spark,2677,summary,0,[SPARK-3816][SQL] Add table properties from storage handler to output jobConf,non_debt,-
shardingsphere,2541,description,0,Fixes #2142.,non_debt,-
hbase,1368,comment,605358334,"This matches what I see locally, https://builds.apache.org/job/HBase%20Nightly/job/HBASE-24049-packaging-integration-hadoop-2.10.0/3//artifact/output-integration/hadoop-2/hadoop_cluster_command.err/*view*/",non_debt,-
spark,15090,review,79015135,"@viirya Yeah, as @hvanhovell suggested, I'll use the resolve function of `LogicalPlan`.",non_debt,-
spark,32030,review,609934635,"Having a default implementation will lead to people who add new expressions don't implement `withNewChildrenInternal` and we again be back to the same situation having many slow `withNewChildren` implementations, so I prefer to make have it like this to enforce `withNewChildrenInternal` implementation. Actually, even now, there are two expressions added to the master and I need to update this PR to implement the `withNewChildrenInternal` for them. The `legacyWithNewChildren` is here for a transition period, we have some expressions that are a bit hard to write `withNewChildrenInternal` for and probably need some refactoring. The goal is to remove `legacyWithNewChildren` altogether at some point.",non_debt,-
spark,7841,comment,155985411,@rxin sure I'll put together a PR for the python API tonight,non_debt,-
beam,72,comment,200614533,"Note on your aside: you will not need to rebase. The commits d510b4e, 8bce693, and 9f8dd18 will simply be added to GitHub and the master pointer updated. Then it will automatically look like the PR you intend.",non_debt,-
geode-native,49,review,105226885,True. gfcpp.properties was changed to geode.properties with PR #13.,non_debt,-
carbondata,2890,comment,437867858,"Build Success with Spark 2.3.1, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/9627/",non_debt,-
incubator-weex,42,description,0,* Modify WXStorage unit test,non_debt,-
samza,1251,review,366655309,Removed.,non_debt,-
beam,1986,comment,279238091,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7329/
--none--",non_debt,-
incubator-mxnet,12724,description,0,"mxnet-mkldnn on mac requires that openmp be enabled (else performance is degraded). There are currently instructions specifying that the Makefile needs to be modified to address this; however, that is not a good user experience. 
the issue is that mac by default does not include openmp by default and therefore a guard was added to prevent users from trying to build with this. the developer should instead not build with openmp enabled on mac and the Makefile should not attempt to catch this (the user may falsely believe they are using openmp when they are not).
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here",non_debt,-
tvm,2493,review,253587562,"thanks for sharing the info!
I would like to use the default value 1 or something else, because if we force user to supply the shape, it's difficult for users who are not ml practitioner, since the user has to be familiar with the graph and figure out the related ops and shapes. We'd like to make the graphdef works even without specifying shape, then users could just use the model from tf community although they don't know the related graph structure.  
If user is willing to supply the related shape, then specified shape will be used instead of -1/1.",non_debt,-
nifi-minifi-cpp,1040,review,605509404,"minor, but `org::apache::nifi::minifi::` is not needed
in a few other places, too, `org::apache::nifi::minifi::utils::StringUtils::toBool(...)` could be shortened to `utils::StringUtils::toBool(...)`",code_debt,dead_code
spark,24029,description,0,"This moves parsing `CREATE TABLE ... USING` statements into catalyst. Catalyst produces logical plans with the parsed information and those plans are converted to v1 `DataSource` plans in `DataSourceAnalysis`.
This prepares for adding v2 create plans that should receive the information parsed from SQL without being translated to v1 plans first.
This also makes it possible to parse in catalyst instead of breaking the parser across the abstract `AstBuilder` in catalyst and `SparkSqlParser` in core.
For more information, see the [mailing list thread](https://lists.apache.org/thread.html/54f4e1929ceb9a2b0cac7cb058000feb8de5d6c667b2e0950804c613@%3Cdev.spark.apache.org%3E).
This uses existing tests to catch regressions. This introduces no behavior changes.",non_debt,-
flink,592,comment,92341849,"Looks good.
I would expect a change in the documentation when you add support for a new feature ;)",documentation_debt,outdated_documentation
airflow,3876,review,216786431,`:type`,non_debt,-
ignite,4705,description,0,31006158-4705 description-0,non_debt,-
superset,10643,comment,676600127,...I wonder how that happened,non_debt,-
spark,2685,comment,61052515,"Okay I think the issue is pretty tough. Unfortunately hive is directly using the shaded objenesis classes. However, Spark needs Kryo 2.21 which depends on the original objenesis classes.
Here is the hive code that uses it:
https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L186
So we can't just remove kryo that hive uses. This is pretty ugly. One solution might be to update chill in Spark so that Spark is using the same Kryo version as Hive.",design_debt,non-optimal_design
beam,13654,description,0,"Since enabling the strict dependencies on all Java modules is a large and critical change to be merged all at once. Therefore, task [[BEAM-10961](https://issues.apache.org/jira/browse/BEAM-10961)] is divided into sub-tasks and strict dependencies are enabled to each module separately which is comparatively easy to review and merge.
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.",non_debt,-
cxf,561,comment,498837564,"I'm wondering if it would make sense to insert a layer between AbstractFeature and the actual features like:  
and then each of the subclasses would look something like:
and a lot of the duplicate code in each of the subclasses goes away. 
Thoughts?",code_debt,duplicated_code
spark,16715,comment,277767226,"@yanboliang, just a friendly reminder please don't forget to review the PR when you have time. Thanks!",non_debt,-
trafficserver,6388,description,0,"Fix #6386
Ran below command",non_debt,-
spark,19374,review,144682434,"FWIW, I _believe_ that when we submit a job with the dispatcher `deployMode` is actually set to `client`, so this logic may not be invoked as expected.",non_debt,-
spark,23823,comment,475437609,retest this please,non_debt,-
hive,2111,review,607667423,"Since it's a new interface method, can you add some javadoc please?",documentation_debt,outdated_documentation
tvm,726,summary,0,"when there is no intrin func, using body for initialization. For issu‚Ä¶",non_debt,-
flink,14084,comment,781454473,"@leonardBang Sorry for the late response, I kind of lost sight of this PR :( I can't seem to find the test comment you mentioned, where would I find that?",non_debt,-
skywalking,5293,comment,673764279,Really detected one besides the mock. You need some fix on the doc this time. :),documentation_debt,low_quality_documentation
lucene-solr,1397,comment,612055874,"OK indexing throughput is noisy, but net/net I think there's no perf impact.  This is `wikimediumall` from `luceneutil` on 128 core box, JDK 11:",non_debt,-
druid,6230,description,0,"This patch includes the following bug fixes:
- TopNColumnSelectorStrategyFactory: Cast dimension values to the output type
  during dimExtractionScanAndAggregate instead of updateDimExtractionResults.
  This fixes a bug where, for example, grouping on doubles-cast-to-longs would
  fail to merge two doubles that should have been combined into the same long value.
- TopNQueryEngine: Use DimExtractionTopNAlgorithm when treating string columns
  as numeric dimensions. This fixes a similar bug: grouping on string-cast-to-long
  would fail to merge two strings that should have been combined.
- GroupByQuery: Cast numeric types to the expected output type before comparing them
  in compareDimsForLimitPushDown. This fixes #6123.
- GroupByQueryQueryToolChest: Convert Jackson-deserialized dimension values into
  the proper output type. This fixes an inconsistency between results that came
  from cache vs. not-cache: for example, Jackson sometimes deserializes integers
  as Integers and sometimes as Longs.
And the following code-cleanup changes, related to the fixes above:
- DimensionHandlerUtils: Introduce convertObjectToType, compareObjectsAsType,
  and converterFromTypeToType to make it easier to handle casting operations.
- TopN in general: Rename various ""dimName"" variables to ""dimValue"" where they
  actually represent dimension values. The old names were confusing.
* Remove unused imports.",build_debt,over-declared_dependencies
gobblin,541,review,50347098,I see makes sense,non_debt,-
attic-stratos,192,review,23697450,I think its better to use org.wso2.carbon.utils.CarbonUtils.getCarbonHome() here.,code_debt,low_quality_code
superset,10255,summary,0,feat(chart-data-api): make pivoted columns flattenable,non_debt,-
spark,15148,comment,259376702,"I tend to agree that the terminology used here is a little confusing, and doesn't seem to match up with the ""general"" terminology (I use that term loosely however).
In my dealings with LSH, I too have tended to come across the version that @sethah mentions (and @karlhigley's package, and others such as https://github.com/marufaytekin/lsh-spark, implement). that is, each input vector is hashed into `L` ""tables"" of hash signatures of ""length"" or ""dimension"" `d`. Each hash signature is created by concatenating the result of applying `d` ""hash functions"".
I agree what's effectively implemented here is `L = outputDim` and `d=1`. What I find a bit troubling is that it is done ""implicitly"", as part of the `hashDistance` function. Without knowing that is what is happening, it is not clear to a new user - coming from other common LSH implementations - that `outputDim` is not the ""number of hash functions"" or ""length of the hash signatures"" but actually the ""number of hash tables"".
In terms of `transform` - I disagree somewhat that the main use case is ""dimensionality reduction"". Perhaps there are common examples of using the hash signatures as a lower-dim representation as a feature in some model (e.g. in a similar way to say a PCA transform), but I haven't seen that. In my view, the real use case is the approximate nearest neighbour search.
I'll give a concrete example for the `transform` output. Let's say I want to export recommendation model factor vectors (from ALS), or Word2Vec vectors, etc, to a real-time scoring system. I have many items, so I'd like to use LSH to make my scoring feasible. I do this by effectively doing a real-time version of OR-amplification. I store the hash tables (`L` tables of `d` hash signatures) with my vectors. When doing ""similar items"" for a given item, I retrieve the hash sigs of the query item, and use these to filter down the candidate item set for my scoring. This is in fact something I'm working on in a demo project currently. So if we will support the OR/AND combo, then it will be very important to output the full `L x d` set of hash sigs in `transform`.
My recommendation is: 
1. future proof the API by returning `Array[Vector]` in `transform` (as mentioned above by others);
2. we need to update the docs / user guide to make it really clear what the implementation is doing;
3. I think we need to make it clear that the implied `d` value here is `1` - we can mention that AND amplification will be implemented later and perhaps even link to a JIRA.
4. rename `outputDim` to something like `numHashTables`.
5. when we add AND-amp, we can add the parameter `hashSignatureLength` or `numHashFunctions`.
6. make as much private as possible to avoid being stuck with any implementation detail in future releases (e.g. I also don't see why `randUnitVectors` or `randCoefficients` needs to be public).
One issue I have is that currently we would output a `1 x L` set of hash values. But it actually should be `L x 1` i.e. a set of signatures of length `1`. I guess we can leave it as is, but document what the output actually is.
I believe we should support OR/AND in future. If so, then to me many things need to change - `hashFunction`, `hashDistance` etc will need to be refactored. Most of the implementation is private/protected so I think it will be ok. Let's just ensure we're not left with an API that we can't change in future. Setting `L` and `d=1` must then yield the same result as current impl to avoid a behavior change (I guess this will be ok since current default for `L` is `1`, and we can make the default for `d` when added also `1`).
Finally, my understanding was results from some performance testing would be posted. I don't believe we've seen this yet.",design_debt,non-optimal_design
kafka,4049,comment,337747023,@hachikuji  Is this the expected result?,non_debt,-
phoenix,52,review,26907122,"FYI, no need for commit after DDL statements.",non_debt,-
spark,15806,review,86991593,"If I correctly interpret [this comment](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L181-L183), an exception will be thrown when `recoverFromCheckpointLocation` == `false` and `fs.exists(checkpointPath)` == `true` are satisfied.
Is [the comment](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L181-L183) wrong if this change is valid? (Note: since I made mistake, I have just update this comment)",non_debt,-
spark,26303,comment,548469185,retest this please,non_debt,-
spark,19140,description,0,"I observed this while running a oozie job trying to connect to hbase via spark.
It look like the creds are not being passed in thehttps://github.com/apache/spark/blob/branch-2.2/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HadoopFSCredentialProvider.scala#L53 for 2.2 release.
More Info as to why it fails on secure grid:
Oozie client gets the necessary tokens the application needs before launching. It passes those tokens along to the oozie launcher job (MR job) which will then actually call the Spark client to launch the spark app and pass the tokens along.
The oozie launcher job cannot get anymore tokens because all it has is tokens ( you can't get tokens with tokens, you need tgt or keytab).
The error here is because the launcher job runs the Spark Client to submit the spark job but the spark client doesn't see that it already has the hdfs tokens so it tries to get more, which ends with the exception.
There was a change with SPARK-19021 to generalize the hdfs credentials provider that changed it so we don't pass the existing credentials into the call to get tokens so it doesn't realize it already has the necessary tokens.
https://issues.apache.org/jira/browse/SPARK-21890
Modified to pass creds to get delegation tokens",non_debt,-
arrow,8343,comment,704824694,"Yes, they are.",non_debt,-
zookeeper,224,comment,353191810,"@hanm thanks for this! I pulled the code into a specialized branch on which I hope to land all of our jenkins related tooling:
https://github.com/apache/zookeeper/tree/jenkins-tools
I also changed the script around a bit to use out testReport from the jobs to identify failed tests. If folks have suggestions they can now use this to submit PRs in a similar way as the rest of our codebase
ps. I've also updated the jenkins job to reflect: 
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-Find-Flaky-Tests/",non_debt,-
trafficserver,2772,description,0,356066-2772 description-0,non_debt,-
samza,843,review,243703884,Here's one organization that comes to mind:   ,non_debt,-
geode,3228,summary,0,GEODE-6365: Add server group support for JDBC List Mapping and Destroy Mapping Commands,non_debt,-
madlib,142,comment,310788886,"DT output seems correct, it seems RT randomness was the cause. LGTM +1",non_debt,-
geode,2907,summary,0,GEODE-6067: add list data-source gfsh command,non_debt,-
beam,11162,review,395216164,"If its unbounded, a runner will never be able to execute it without expansion which requires the runner to at least inspect whether its capable of doing it.",non_debt,-
nifi,4767,review,561227339,Perhaps renaming this variable to something like `DEFAULT_STORE_TYPE` would clarify the reason for declaring it here as opposed to just using the enum value where necessary.,code_debt,low_quality_code
beam,4266,summary,0,[BEAM-3326] Add a BundleProcessor to SdkHarnessClient,non_debt,-
pulsar,43,review,82681951,"actually, at this point we haven't persisted the message so, we don't have message-id so, we are not logging message-id.",non_debt,-
bookkeeper,1907,comment,456220683,rebuild java11,non_debt,-
flink,8322,review,292929426,`tolerableCpFailureNumber <= UNLIMITED_TOLERABLE_FAILURE_NUMBER` always evaluates to `true` and can be removed.,non_debt,-
flink,966,comment,126706999,Ah yes. I'll update them in a while. There's actually some problem with the unit test I've written too. Travis fails sporadically.,test_debt,flaky_test
trafficserver,3840,comment,397724591,Do you know how do we include the yaml library inside lib/ts? @randall @bryancall,non_debt,-
skywalking,4987,review,450269014,Config should only be changed through the initialization stage. Nothing more.,non_debt,-
kafka,2501,comment,277423574,ping @junrao,non_debt,-
camel-quarkus,225,description,0,193065376-225 description-0,non_debt,-
arrow,7030,review,416409859,Done. Now it is changed to iterate ArrowRecordBatch rather than VectorSchemaRoot.,non_debt,-
carbondata,2555,comment,408309734,retest sdv please,non_debt,-
spark,14537,comment,265352177,The schema inferring is replaced with metastore schema completely in #14690. I think we can close this now? cc @cloud-fan @liancheng,non_debt,-
spark,15231,description,0,"`write.df`/`read.df` API require path which is not actually always necessary in Spark. Currently, it only affects the datasources implementing `CreatableRelationProvider`. Currently, Spark currently does not have internal data sources implementing this but it'd affect other external datasources.
In addition we'd be able to use this way in Spark's JDBC datasource after https://github.com/apache/spark/pull/12601 is merged.
**Before**
- `read.df`
- `write.df`
**After**
- `read.df`
- `write.df`
Unit tests in `test_sparkSQL.R`",non_debt,-
tvm,5151,summary,0,[Doc] TVM release process,non_debt,-
ignite,6392,summary,0,IGNITE-11654: [ML] Memory leak in KNNClassificationModel,design_debt,non-optimal_design
avro,158,comment,426640874,@johnsgill3 @walshb -- any progress here?,non_debt,-
brooklyn-server,915,description,0,47246081-915 description-0,non_debt,-
zeppelin,3186,description,0,"Maven build fails due to invalid file name character, this PR fix it by renaming the note file name.
[Bug Fix ]
* https://issues.apache.org/jira/browse/ZEPPELIN-3788
* CI pass",non_debt,-
spark,15146,summary,0,[SPARK-17590][SQL] Analyze CTE definitions at once and allow CTE subquery to define CTE,non_debt,-
incubator-mxnet,19772,comment,786476224,"As part of this PR, the docker push target has been moved to a public ECR repository.",non_debt,-
hbase,2077,review,458520038,"Thanks @wchevreuil for reviewing.
[ReplicationSinkManager.chooseSinks](https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSinkManager.java#L151) will be called to re-fetch the sinks when HBaseInterClusterReplicationEndpoint.replicate [catches ConnectException or UnknownHostException](https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java#L563), It's same as before.
And I've added a config ""hbase.replication.fetch.servers.usezk"" to optionally still use ZK impl.",non_debt,-
tvm,953,review,171908616,"Because we already have exception handling, this can be done from front end",non_debt,-
carbondata,2030,comment,370892543,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/4106/",non_debt,-
spark,15851,comment,260038511,Thanks for testing and fixing this. A couple of minor comments.,non_debt,-
flink,507,summary,0,Add the class member variables with 'this',non_debt,-
spark,22056,description,0,"When a `SparkSession` is stopped, `SQLConf.get` should use the fallback conf to avoid weird issues like
a new test suite",non_debt,-
hbase,1482,review,416140357,What does this function do? Favor SSD? Needs comment.,documentation_debt,low_quality_documentation
airflow,5420,summary,0,[AIRFLOW-4797] Fix zombie detection,non_debt,-
trafficserver,2525,comment,329823741,Re-opening PR after some manipulations around local git branches,non_debt,-
flink,1588,comment,180753150,"I didn't test this myself, but this diff could be sufficient for testing your change:",non_debt,-
spark,326,comment,39626508,Merged,non_debt,-
rocketmq,1393,description,0,"fixed ""error=Algorithm HmacSHA1 not available"" when use ACL on windows.",non_debt,-
iceberg,1145,review,465458831,"Not the same thread, but:
Before invoking them, the caller will take a same lock.",non_debt,-
spark,21405,review,191372561,Added test for it.,non_debt,-
iceberg,416,review,317822579,"This actually needs to pass the current metadata location to `CustomService`. Otherwise, `CustomService` can't guarantee that the location update is an atomic swap, and the atomic swap is required for consistency. Otherwise, two processes could each start a change, validate that metadata hasn't changed and both call this update at the same time. The second one would win because the update doesn't know to reject it because it isn't based on the other process's changes.",non_debt,-
spark,24807,comment,500879466,"I think the SparkSession.close() behavior is on purpose, and that's a coherent behavior (i.e. just don't shut anything down until you're done, and then everything shuts down). What's not consistent with that is maintaining some state in the session that can't be cleared. 
I think the ways forward are probably:
- A new lifecycle method like `clear()`? more user burden but at least provides _some_ means of doing cleanup without changing `close()`
- Figure out how to automatically dispose of those resources or not hold them
- Just change the behavior of session's `close()` to not shut down the context. Behavior change, yes, but perhaps less surprising than anything.
Eh, do people like @cloud-fan or @gatorsmile or @HyukjinKwon or @dongjoon-hyun have thoughts on this? I feel like reference counting is going to end in tears here eventually, but, it's not crazy",design_debt,non-optimal_design
spark,24041,comment,471500788,"Yea, very nice PR description.",non_debt,-
trafficcontrol,3380,review,264532031,done,non_debt,-
skywalking,1256,comment,391530381,I had fix this. maybe you can try,non_debt,-
incubator-brooklyn,606,comment,94850687,"[incubator-brooklyn-pull-requests #1062](https://builds.apache.org/job/incubator-brooklyn-pull-requests/1062/) SUCCESS
This pull request looks good",non_debt,-
kafka,3219,summary,0,"KAFKA-5374. AdminClient gets ""server returned information about unkno‚Ä¶",non_debt,-
spark,15397,comment,252392495,"How is this going to work with assign?  It seems like it's just avoiding the problem, not fixing it.",non_debt,-
iceberg,2379,review,609298267,"I think we will need to discuss how to pack files and whether we need to look at the min/max stats on the sort key, for example. Does not have to be done now, though.",non_debt,-
beam,4071,description,0,"Also add (optional) access to the contexts on serializing and
deserializing pipelines.
---",non_debt,-
kafka,6596,review,276504405,Similar cleanup https://github.com/apache/kafka/pull/6598,non_debt,-
spark,29982,comment,709978338,"Seems the bracketedCommentLevel could not handle this case:
WIP",non_debt,-
iceberg,1748,review,521566746,"I got this working when I tested 0.10.0-rc4. In Hive, I had to set `iceberg.mr.catalog=hive` so that the correct catalog was used. Unfortunately, that broke the Hadoop table.
I think we might want to add support for Hadoop tables in our catalogs using an identifier that contains a path to fix this.",non_debt,-
spark,288,review,11231217,"There is a default one for each actor, yes -- from its actorSystem.",non_debt,-
spark,19208,review,149225714,"Although we're making this public, let's not make all of its APIs public.  Can you please make the constructor private and make this class final?",non_debt,-
parquet-mr,808,comment,672774756,"@shangxinli,
If we will agree on the extending of the schema with metadata is a good idea and as you said the serialization/deserialization is also required we need to change the format first. The schema objects in parquet-mr are only exist in the parquet-mr runtime. To have them serialized we need to convert this object structure to the thrift object structure defined in the format. If we don't have the new metatdata fields in the format we cannot serialize/deserialize them. So it is a much bigger topic. Also, I'd like to see this feature separated from the encryption as it would be general approach for storing metadata in the schema. Meanwhile, I am not convinced that we need to have such extension.
About the namespace prefix etc. I don't agree this is not user friendly. That's why I've suggested to implement a helper API so the user doesn't need to deal with the conf keys (and values) directly. 
@ggershinsky,
I don't agree we cannot have a meeting about this topic in terms of transparency. What we have to do is to document here about what we have discussed and what are the conclusions. Meanwhile, I am not sure if a meeting would help but I am happy to participate if anyone thinks otherwise.
Also, if we think we are getting stuck with this issue I would suggest involving other members of the community. Maybe draw their attention on the dev list about this PR or bring up the topic on the next parquet sync.",design_debt,non-optimal_design
tvm,7404,comment,776327280,Thanks @kevinthesun,non_debt,-
couchdb,2426,description,0,"Design doc writes could fail on the target when replicating if with non-admin
credentials. Typically the replicator will skip over them and bump the
`doc_write_failures` counter. However, that relies on the POST request
returning a `200 OK` response. If the authentication scheme is implemented such
that the whole request fails if some docs don't have enough permission to be
written, then the replication job ends up crashing with an ugly exception and
gets stuck retrying forever. In order to accomodate that scanario write _design
docs in their separate requests just like we write attachments.
Fixes: #2415",code_debt,low_quality_code
beam,11554,comment,628786151,"Before merging, please share a link to the archived copy of the old website.",non_debt,-
beam,4910,review,175947778,At this pointm it may be worth wrapping the getXOrThrow calls to actually return a legible error message since the generated code does not print the input ids.,code_debt,low_quality_code
lucene-solr,2245,review,564552680,"YW @iverase.
Right, that case should write a whole new segment, using the latest Codec, holding the just indexed document with vector field.",non_debt,-
spark,23982,review,263647807,nit: `left_anti`,code_debt,low_quality_code
superset,5800,comment,454525415,@mistercrunch almost there! Can I help?,non_debt,-
spark,22288,comment,425489570,retest this please,non_debt,-
spark,19432,summary,0,[SPARK-22203][SQL]Add job description for file listing Spark jobs,non_debt,-
beam,6554,comment,426507664,+1,non_debt,-
hadoop,1263,comment,523038215,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
beam,6093,comment,409713790,R: @bsidhom @ryan-williams @tweise,non_debt,-
spark,6003,comment,100132904,Merged build finished. Test FAILed.,non_debt,-
spark,22820,comment,432872093,"@erikerlandson @skonto @liyinan926 for review
Do you see additional places where we would want to leverage the newer version for optimality?",non_debt,-
beam,8026,summary,0,[BEAM-6726] explicitly specify signing key,non_debt,-
carbondata,1476,comment,343572149,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/988/",non_debt,-
lucene-solr,2229,review,562524364,"Would it be acceptable to reformat the code when such a demand appears, or after I'm finished with these improvements? These ""some people"" might never need to look into Hunspell code.",code_debt,low_quality_code
spark,14339,description,0,"It seems this is a regression assuming from https://issues.apache.org/jira/browse/SPARK-16698.
Field name having dots throws an exception. For example the codes below:
throws an exception as below:
This problem was introduced in https://github.com/apache/spark/commit/17eec0a71ba8713c559d641e3f43a1be726b037c#diff-27c76f96a7b2733ecfd6f46a1716e153R121
When extracting the data columns, it does not count that it can contain dots in field names. Actually, it seems the fields name are not expected as quoted. So, It does not have to consider whether this is wrapped with quotes because the actual schema (inferred or user-given schema) would not have the quotes for fields. 
For example, this throws an exception. (**Loading JSON from RDD is fine**)
as below:
Unit tests in `FileSourceStrategySuite`.",non_debt,-
ignite,8207,review,485543038,Should we consider making this fields `public` too?,non_debt,-
calcite,1714,comment,570713589,"LGTM thanks. 
It could be better if you can have a rebase locally and solve conflicts so people can easy merge your PR.",non_debt,-
beam,1889,comment,276743422,"R: @chamikaramj 
Re-opening https://github.com/apache/beam/pull/1848 as a new PR.
Added a unit test and removed the change in `wordcount.py`. Adding `WordCountOptions` work when `wordcount` is run only by itself but fails when running with `tox`. All tests after the wordcount test start requiring the `output` flag as defined in the wordcount options. This happens with or without the current change to the `pipeline_options`. We need to fix that later.",non_debt,-
activemq-artemis,2106,comment,391761041,@jbertram although I can see an issue by inspecting the code. not just for AMQP  but noConsumers is only increased.. if you open a consumer... close it.. and open it again... maxConsumers is ignored.,non_debt,-
arrow,2808,summary,0,ARROW-3582: [CI] fix incantation for C++/Java detection tool,non_debt,-
spark,6759,review,32197414,"Yes, we can use that dataset to verify.",non_debt,-
druid,6349,review,224141572,"if these are host and port entries, can they use `HostAndPort` objects?",non_debt,-
flink,12412,review,433847667,Shouldn't this be `public` and not `package-private`? (The same for the other methods),non_debt,-
airflow,2869,review,156870749,"Ah, I didn't realize that, let me update it. Thanks!",non_debt,-
flink,9748,review,328404835,"Add comment for this in this commit otherwise, other people would be confused.",documentation_debt,low_quality_documentation
nifi-minifi-cpp,62,review,104827822,"sure. we can remove that. For my last test, i found test is failed and have trouble to find the log for the test that just ran.",non_debt,-
zeppelin,208,review,58003498,"Makes perfect sense, thank you for explanation. 
I believe that is the case where changed one is distributed under the same licence, as per [#4 in licence doc](http://www.apache.org/legal/src-headers.html#3party).",non_debt,-
nifi,200,comment,180874341,"DAVID SMITH  on dev@nifi.apache.org replies:
Hi Guys
st.=20
www.avast.com  |
As you may remember I have developed some processors that publish/subscribe=
 to AMQP brokers, but I was having problems writing Junit tests for these p=
rocessors.=C2=A0I was interested to see that you have been working on NiFi =
Pull Request 865.=C2=A0I have looked at your code for these processors, we =
are both using different property descriptors to allow messages to be publi=
shed and pulled. I also noticed that you are using RabbitMQ libraries to co=
nnect to the broker, whereas I connect to the AMQP broker using the QPID JM=
S libraries. I can still see a use for my processors and I would still be i=
nterested getting my processors uploaded to run alongside yours in a future=
 release of NiFi.I have tidied up my code and pushed it back to github:
https://github.com/helicopterman22/nifi_amqp_processors.git
I would appreciate your feedback=C2=A0Dave
=20
=20
 Github user asfgit closed the pull request at:
=C2=A0 =C2=A0 https://github.com/apache/nifi/pull/200",non_debt,-
spark,23291,review,247307662,"Why some benchmarks use large num rows, others use small num rows?",non_debt,-
camel,2151,comment,352353675,"Yes thanks @adriancole 
And thanks for the heads-up on that meeting. I dont know my calendar at that time in 2018.",non_debt,-
superset,8867,review,360617020,"It's not super clear here what this does or why you'd want to use this config element. I had to read a bit of code to understand it.
Then add a proper example with type annotation",documentation_debt,low_quality_documentation
lucene-solr,1847,summary,0,LUCENE-9514: Include TermVectorsWriter in DWPT accounting,non_debt,-
tvm,4144,summary,0,Fix typo,documentation_debt,low_quality_documentation
reef,261,review,34074950,done both changes,non_debt,-
beam,2536,comment,294071905,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/9519/<h2>Build result: FAILURE</span></h2>[...truncated 2.31 MB...]	at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.maven.plugin.MojoExecutionException: Command execution failed.	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:302)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)	... 31 moreCaused by: org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)	at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:166)	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:764)	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:711)	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:289)	... 33 more2017-04-14T02:39:05.915 [ERROR] 2017-04-14T02:39:05.915 [ERROR] Re-run Maven using the -X switch to enable full debug logging.2017-04-14T02:39:05.915 [ERROR] 2017-04-14T02:39:05.915 [ERROR] For more information about the errors and possible solutions, please read the following articles:2017-04-14T02:39:05.915 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2017-04-14T02:39:05.915 [ERROR] 2017-04-14T02:39:05.915 [ERROR] After correcting the problems, you can resume the build with the command2017-04-14T02:39:05.915 [ERROR]   mvn <goals> -rf :beam-sdks-pythonchannel stoppedSetting status of 82ba164b6f0ca69abbc707163232fa5b5791dc9a to FAILURE with url https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/9519/ and message: 'Build finished. 'Using context: Jenkins: Maven clean install
--none--",non_debt,-
bigtop,81,summary,0,Bigtop 2312,non_debt,-
spark,23677,review,298342839,"I couldn't figure out what this comment meant, till I realized you were copying it from `addPendingTask`, though I coudln't figure out what it meant there either, till I looked in history and realized it was long obsolete -- added [here](https://github.com/apache/spark/commit/5e91495f5c718c837b5a5af2268f6faad00d357f), then removed [here](https://github.com/apache/spark/commit/3535b91ddc9fd05b613a121e09263b0f378bd5fa) (but the comment was left).  Can you delete both comments?  (unless you can see a way it still has some relevance ...)",non_debt,-
arrow,8177,review,488076230,You're `using` them but still using the full qualified names below :-),non_debt,-
kafka,4880,review,182225768,"Not possible if we want to share code. We call `decodeTasks` that does not know if it's decoding prevTasks or standbyTasks -- alternative, we can do two methods. Let me know what you think",non_debt,-
kafka,3131,comment,312119156,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5798/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
hadoop,1881,review,388858948,nit: this line is long enough its time to split,code_debt,low_quality_code
flink,12758,comment,649547883,Thanks for your contribution @houmaozheng . Looks good to me.,non_debt,-
cloudstack,2721,review,201307689,"    - TESTS=""smoke/test_accounts
             smoke/test_affinity_groups
             smoke/test_affinity_groups_projects
             smoke/test_deploy_vgpu_enabled_vm
             smoke/test_deploy_vm_iso
             smoke/test_deploy_vm_root_resize
             smoke/test_deploy_vm_with_userdata
             smoke/test_deploy_vms_with_varied_deploymentplanners
             **smoke/test_diagnostics**
             smoke/test_disk_offerings
             smoke/test_dynamicroles
             smoke/test_global_settings
             smoke/test_guest_vlan_range""",non_debt,-
beam,329,description,0,"quickly and easily:
  `[BEAM-<Jira issue #>] Description of pull request`
     Travis-CI on your fork and ensure the whole test matrix passes).
     number, if there is one.
     [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---",non_debt,-
flink,5367,review,165039357,I think this method is not really necessary. We already have logic for accessing composite types (see `visitFieldAccess()`). Maybe we just have to make the methods there a bit more generic.,code_debt,complex_code
netbeans,449,description,0,Added mockito library.,non_debt,-
hudi,1149,comment,669295110,"@nsivabalan What is the reason to change the name from `UserDefinedBulkInsertPartitioner` to `BulkInsertPartitioner`. I see that you added a new method to the interface, which is fine, but want to understand the reason for the name change",non_debt,-
flink,8449,comment,492630818,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
* ‚ùì 1. The [description] looks good.
* ‚ùì 2. There is [consensus] that the contribution should go into to Flink.
* ‚ùì 3. Needs [attention] from.
* ‚ùì 4. The change fits into the overall [architecture].
* ‚ùì 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier",non_debt,-
trafodion,309,comment,180436567,"jenkins, add user",non_debt,-
kafka,4855,comment,383759105,rebased,non_debt,-
druid,10070,review,445155089,minor nit:mark return type as nullable,code_debt,low_quality_code
cloudstack,3600,comment,580152807,"than, can you approve please @andrijapanicsb",non_debt,-
beam,7174,review,238478822,"if the intention is to use it only for batch, you would need to add {..}",non_debt,-
trafodion,294,comment,178076645,Looks good to me. Thanks for fixing all those typos.,documentation_debt,low_quality_documentation
ozone,2000,review,600354804,Removed it,non_debt,-
calcite,552,review,275722923,"Agree, this is one of the key changes in this PR.",non_debt,-
spark,10734,review,49557801,does `prettyString` work for it?,non_debt,-
trafficserver,833,comment,237111945,Looks good. Please squash commits.,non_debt,-
arrow,4613,review,296764162,Needs to be restored,non_debt,-
spark,23272,description,0,"In `BytesToBytesMap.MapIterator.advanceToNextPage`, We will first lock this `MapIterator` and then `TaskMemoryManager` when going to free a memory page by calling `freePage`. At the same time, it is possibly that another memory consumer first locks `TaskMemoryManager` and then this `MapIterator` when it acquires memory and causes spilling on this `MapIterator`.
So it ends with the `MapIterator` object holds lock to the `MapIterator` object and waits for lock on `TaskMemoryManager`, and the other consumer holds lock to `TaskMemoryManager` and waits for lock on the `MapIterator` object.
To avoid deadlock here, this patch proposes to keep reference to the page to free and free it after releasing the lock of `MapIterator`.
Added test and manually test by running the test 100 times to make sure there is no deadlock.",non_debt,-
trafficcontrol,1480,comment,366315191,"@elsloo Fixed up the JSON.org dependency.
Also I think at this point this can go into 2.3, so updating the milestone",non_debt,-
incubator-mxnet,5336,description,0,"follows: https://github.com/dmlc/mxnet/blob/master/python/mxnet/module/sequential_module.py
training log:
and load checkpoint:",non_debt,-
druid,5278,review,164339035,done.,non_debt,-
spark,6157,comment,102176652,Merged build started.,non_debt,-
tvm,1403,review,201886571,"this warning seems can be a bit frequent, instead of this, do
Do CHECK_GE(sorted_order_.size(), threads_.size());",code_debt,low_quality_code
kafka,7799,review,364476808,"We only needed it here because the we were trying to ensure we had a chance to send the Abort before going into poll. After thinking about it, it seemed a little simpler to move the `hasAbortableError` check to `maybeSendAndPollTransactionalRequest`. Then we no longer need this change.",code_debt,low_quality_code
netbeans-website,473,review,430013474,Mmm.... infra will complain if we use jenkins builds as an update center. Also note this is extremely slow...,code_debt,slow_algorithm
geode-native,634,review,471802267,Why do we use a pre-baked key and certificates here? I though all the SSL tests generated certs on the fly but maybe I am wrong. In the past this has caused issues with hostname validation.,non_debt,-
flink,854,comment,114331577,"@StephanEwen @fhueske - apologies, I forgot to call collect. 
A dumb question - why do we need to sort the records before combining them? Should I just call collect on the oversized record or still pass it to the combiner? What do combiners do(or what are they suppose to do in general)?",non_debt,-
kafka,2314,comment,270717795,"@dguy When I was adding `LockException` in another PR, I was explicitly requested to add more ctors even if I did not use them (see https://github.com/apache/kafka/pull/2233/files#r92231489)",non_debt,-
incubator-mxnet,9810,review,170436506,Question*,non_debt,-
spark,16810,comment,278314048,Merged to master. @JoshRosen you can make any other Jenkins changes as needed,non_debt,-
kafka,3188,review,137854355,"There could be multiple joins within each reporting period, so this should be a `+=`, right?",non_debt,-
apisix,3894,review,600109033,Is it the password?,non_debt,-
netbeans,968,comment,430955215,"@geertjanw The XSDs/DTDs are used to generate code and my interpretation of the reply from apache legal is, that the license transcends to the generated code. So for weblogic we would distribute unlicensed code (i.e. we have no right to distribute) and for jboss/wildfly we would distribute LGPL code, which is against the ASF rules.
Getting the XSDs/DTDs at runtime would be perfectly doable, but the modules would need to be reworked to no rely on code generated at build-time.
@vikasprabhakar the DTD files are present here:
https://www.jboss.org/j2ee/dtd/
The files are missing an explicit license and as such I would assume they are covered by the projects LGPL license. That at least is reflected in some of the XSDs I saw. Asking redhat would most probably reveal the same. That still leaves us with the ""wrong"" license.",documentation_debt,low_quality_documentation
kafka,3269,comment,307775797,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5149/
Test FAILed (JDK 8 and Scala 2.12).",non_debt,-
hawq,90,review,44373709,"logic here is sleep 1s and reconnect when connect failed?
Is there a need to add max_retry_num?",non_debt,-
tvm,5044,summary,0,Set split node's range to minimum of ext and split factor or split np‚Ä¶,non_debt,-
druid,1862,summary,0,Add timeout to shutdown request to middle manager for indexing service,non_debt,-
spark,19468,review,148569836,Why do we need to fetch it? Seems we can always use `executorMemoryMiB.toString` instead.,non_debt,-
hudi,1114,review,360699643,Move to checkWriteStatus's javadoc.,non_debt,-
skywalking,5685,comment,711190648,"@wu-sheng  thx for check the structure this time.  If this version looks good, I will modify the doc and add the unit test later.",documentation_debt,outdated_documentation
airflow,1270,description,0,pypi can use categories for better description and version number was out of sync,code_debt,low_quality_code
kafka,4805,review,183132149,"nit: `isolate ... form other client configs` -> 
Ditto below for other two.",non_debt,-
kafka,2525,summary,0,KAFKA-4484: Set more conservative default values on RocksDB for memory usage,non_debt,-
incubator-weex,1794,description,0,78186814-1794 description-0,non_debt,-
incubator-pinot,2304,review,160811762,Nice catch. I meant `===` here. Thanks.,non_debt,-
carbondata,3208,review,282835607,done,non_debt,-
spark,24382,comment,517798684,"By the way, I also found another issue of the current foreach sink due to the API limitation in Data Source V2: https://issues.apache.org/jira/browse/SPARK-28605",non_debt,-
kafka,2675,comment,285844277,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2127/
Test PASSed (JDK 8 and Scala 2.11).",non_debt,-
spark,8823,comment,141555821,Merging to master and branch 1.5. Thanks @yjshen,non_debt,-
activemq-artemis,2626,comment,484288172,"We could make it abstract. And add Prometheus as a one of the implementations. It would be nice to have something out of the box.  Enabled during server creation. 
Using the argument of external repo could be used against everything.  Web server for instance. 
I don‚Äôt really think it applies the same thing. 
We should move the discussion to the dev list.  It‚Äôs bexomong out of the scope of code review now.",non_debt,-
arrow,6804,comment,607813046,This will be very useful! Once this lands I'll see about wiring this up to the gRPC async APIs.,non_debt,-
beam,535,comment,228586262,"LGTM, merged.",non_debt,-
spark,28442,comment,625187631,Shell we backport this to 3.0 too? SPARK-30541 was merged to master so the flakiness should be there in branch-3.0 too.,non_debt,-
arrow,2979,comment,440724090,+1,non_debt,-
activemq-artemis,2312,comment,429021094,"@michaelandrepearce ok all fix now, let me know if you want me to change anything else!",non_debt,-
zookeeper,924,review,299043313,Unused default constructor.,code_debt,dead_code
iceberg,2048,review,555974043,"I'm not sure about having this be a static method since it would end up constructing an instance of the object anyways, but I don't have strong feelings here. I lean towards giving it a name like `validateReferencedColumns` personally.",non_debt,-
beam,12444,comment,669140081,Run Whitespace PreCommit,non_debt,-
rocketmq,610,comment,448893600,@RongtongJin Splitting the large PR into many small ones maybe a good choice.,non_debt,-
spark,14753,review,76152607,"The inputBuffer is a safeRow in SortAggregateExec
`inputBuffer.getBinary(inputAggBufferOffset)` and `getField[Array[Byte]](inputBuffer, inputAggBufferOffset)` are equivalent.
Yes, it is better to use `inputBuffer.getBinary(inputAggBufferOffset)` directly",non_debt,-
druid,5492,review,203139893,"for batch ingestion, I would expect if user submitted supervisor task fails then no segments are generated. however, from this it appears that segments are published as subtasks finish rather than at the end.",non_debt,-
ignite,8142,review,469236353,Ok for now.,non_debt,-
incubator-doris,283,review,231052560,"Implicit CastExpr can't call analyze() outside , so targetTypeDef will not throw NullPointerException.",non_debt,-
superset,5946,description,0,"- Remove `lodash.throttle` from dependency since there is now `lodash`.
- Add `babel-plugin-lodash` that helps optimize bundle output by taking only necessary part from lodash instead of the entire bundle.
https://github.com/lodash/babel-plugin-lodash
- Replace `underscore` calls with `lodash` where applicable. 
@williaster @xtinec @conglei",code_debt,complex_code
ambari,3208,comment,667902138,@payert please review this PR,non_debt,-
spark,3099,review,20111063,"Doc needed!  Something like:
""A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer.  When [[fit()]] and [[transform()]] are called, the stages are executed in order, and each stage may modify the dataset before it is passed to the next stage.""
Maybe say something about what happens when there are no stages too.",documentation_debt,outdated_documentation
skywalking,2056,comment,450463672,"emm. i agree with u.so i deleted it. btw, i think its time to review @YunaiV @wu-sheng @JaredTan95",non_debt,-
openwhisk,1783,comment,276930895,Excellent!,non_debt,-
spark,9656,comment,157867300,@fayeshine #9814 was merged. Do you mind closing this PR manually? Thanks for your contribution!,non_debt,-
couchdb,284,comment,64910702,"I spoke too soon it doesn't decide just tells you that it supports both so maybe if it supports html then give them html otherwise not, should work.",non_debt,-
incubator-mxnet,10450,comment,379818396,@hetong007 can you please merge this PR.,non_debt,-
kafka,247,comment,143836715,LGTM,non_debt,-
incubator-mxnet,19154,review,488901945,This was not working before #18704 . I just reverted the changes on that.,non_debt,-
camel,3018,comment,509512324,"Thanks, merged on master",non_debt,-
qpid-dispatch,288,summary,0,Refactor cmake to use modern feature,non_debt,-
ambari,2848,comment,470072514,cc: @g-boros,non_debt,-
kafka,734,comment,191717397,"@ewencp Ôºå thanks a lot for your comment, ~I modified this again, please check",non_debt,-
airflow,4887,comment,471372376,"Hi @mans2singh Seems this sensor only checks whether there is message in the queue, or not.
Do you think adding feature to check if specific body contents is in the queue would be a good idea?",non_debt,-
phoenix,516,review,292095594,Do you mean I should remove the `addPKColumns` variable and change the if statement to something like the following:,code_debt,low_quality_code
servicecomb-java-chassis,846,review,208209539,done,non_debt,-
incubator-mxnet,10375,comment,380219820,See: https://github.com/apache/incubator-mxnet/pull/10486/files,non_debt,-
hadoop,802,review,291163702,nit: stick at the top (line 20) with the gab above the org.apache stuff. We can at least try to not make import ordering worse,code_debt,low_quality_code
carbondata,1322,comment,328775869,retest this please,non_debt,-
kafka,7321,review,323386539,"They are not added as allowed values of the UPGRADE_FROM_CONFIG yet, is it intentional?",non_debt,-
fluo,413,review,24015731,changed in a9572d8,non_debt,-
thrift,972,summary,0,THRIFT-3770 Implement Python 3.4+ asyncio support,non_debt,-
ambari,3108,comment,546878696,retest this please,non_debt,-
guacamole-client,564,review,484444952,"This can be configured on the oidc implementation side.  For example, in Keycloak you can configure it to just return the email and username in the token.
Edit: We may just need to introduce a configuration options that lists out what the expected claims are, and put this method call behind that.",non_debt,-
incubator-dolphinscheduler,4930,description,0,"fix #4929 
fix ClassCastException when run hive sql with udf",non_debt,-
spark,5743,review,29453868,"I think we can avoid the new field by doing
here and
Allowing the removal of the other connectionTerminated().",non_debt,-
carbondata,2970,summary,0,[CARBONDATA-3142]Add timestamp with thread name which created by CarbonThreadFactory,non_debt,-
spark,16228,comment,265945422,cc @rxin @srinathshankar @cloud-fan,non_debt,-
beam,2018,review,102343720,Done,non_debt,-
hive,1478,description,0,"Thanks for sending a pull request!  Here are some tips for you:
  3. Ensure you have added or run the appropriate tests for your PR: 
  5. Be sure to keep the PR description updated to reflect all changes.
  6. Please write your PR title to summarize what this PR proposes.
  7. If possible, provide a concise example to reproduce the issue for a faster review.
-->
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
-->
-->
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If possible, please also clarify if this is a user-facing change compared to the released Hive versions or within the unreleased branches such as master.
-->
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->",non_debt,-
spark,8040,comment,129081793,I saw it as false for a moment for some reason. Nvm. Lgtm,non_debt,-
spark,9610,review,44743137,17165658-9610 review-44743137,non_debt,-
guacamole-client,375,review,261795920,No translation...,non_debt,-
spark,4188,review,24266009,"Oof, sorry, can't believe I left that in!",non_debt,-
incubator-mxnet,17956,description,0,"To add a nightly build Jenkins pipeline for v1.x, we need to create a new Jenkinsfile capable of building all the components without the ""Publish"" step.
- A ci/jenkins/Jenkinsfile_website_nightly
Tested successfully on Jenkins dev instance (see build output [here](http://jenkins.mxnet-ci-dev.amazon-ml.com/job/docs/job/connor-website-build-master/38/)).
Pipeline is available [here](http://jenkins.mxnet-ci.amazon-ml.com/job/restricted-nightly-website-build-1.x/).",non_debt,-
beam,13809,description,0,"R: @emilymye 
cc: @chamikaramj 
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.",non_debt,-
spark,20081,comment,353993348,Thanks! Merged to master.,non_debt,-
spark,29868,review,494947145,"This is still 'required' right? we're not making it an error, but it won't have any effect if not in inputCols.",non_debt,-
beam,10455,review,362682454,Is it possible to reverse this (`SqlTypeFamily.BINARY.equals(family)`)? That is the preferred style and also eliminates the need for a null check.,code_debt,low_quality_code
flink,272,description,0,Simple code cleanup to remove full package name for FlatMapOperatorBase for FlatMapOperator#translateToDataFlow method,non_debt,-
pulsar,5592,description,0,"Fixes #5589 
It seems that there is a memory leak in the pulsar-function-go library.
I implemented a simple pulsar function worker that just write logs using pulsar-function-go/logutil for sending logs to log topic. I tried to long-term test by sending request messages consecutively to the input topic to check the feasibility.
During the test, I faced `ProducerQueueIsFull` error with `--log-topic` option. And I observed indefinitely grown memory usage of the pulsar function worker process.
Clear the `StrEntry` variable after finish addLogTopicHandler() function regardless of the log messages are appended to logger or not. If it is not cleared, it causes memory leak because StrEntry has grown indefinitely. Moreover, if the function set --log-topic, then the topic could get accumulated huge messages that cause ProducerQueueIsFull error.
Verified it by reproducing step described in the issue #5589.",design_debt,non-optimal_design
spark,5760,review,29388845,"OK, yeah I see that in the docs, though it's not set up that way in CDH (at least, maybe my installation never needed to configure that file a certain way, dunno). It seems bad to silently ignore unreadable files, so at least log it maybe? then... should it be a warning? because it sounds like there's one file that could reasonably be expected to be unreadable. Do we special case it and warn on anything else? fail on anything else?  i'd rather tighten this up in some way from doing this silently.",code_debt,low_quality_code
kafka,1489,review,69977778,ack,non_debt,-
spark,30254,comment,722014962,Woops wrong repo. Sorry!,non_debt,-
beam,1816,comment,277707399,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7107/
--none--",non_debt,-
pulsar,7054,comment,638564431,"@merlimat Thanks for your comment, I will fix it later. I have created an issue to track this problem. https://github.com/apache/pulsar/issues/7161",non_debt,-
spark,14204,comment,235246702,@JoshRosen Thanks for pointing out the compatibility issue. I have fixed it now.,non_debt,-
arrow,7885,review,465480283,Added more test cases for negative numbers and boundary conditions.,test_debt,low_coverage
airflow,7138,review,368222674,For the AIP I would expect building on top of FAB or equivalent. That gives OpenAPI endpoints out of the box.,non_debt,-
nifi-minifi-cpp,1028,review,596846391,done in 141917b,non_debt,-
hbase,1337,comment,638256596,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
beam,12509,comment,681653149,Run Java PreCommit,non_debt,-
airflow,9297,summary,0,graph view mouseover task should increase stroke width of upstream / downstream,non_debt,-
spark,25845,comment,533413162,I think we can just leave it without other options. It's rather corner case and I think it's fine to break such stuff since we're moving to Spark 3.,non_debt,-
incubator-dolphinscheduler,4743,review,578115679,Why is it not a Map here,non_debt,-
spark,17251,review,105818676,"what about `1 + 2`? The requirement is a foldable int type expression, not have to be int literal.",non_debt,-
trafficserver,469,description,0,356066-469 description-0,non_debt,-
hbase,2498,comment,706549446,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
storm,215,review,15836939,"We should remove Config.UI_ACTIONS_ENABLED, or at least deprecate it.",code_debt,dead_code
dubbo,1205,comment,357946437,@chickenlj Done,non_debt,-
kylin,206,summary,0,KYLIN-3505 Fix wrong usage of cache in DataType,non_debt,-
incubator-pinot,272,summary,0,TE: Refactor code to support more metric functions such as 'average' and 'count',non_debt,-
superset,4941,description,0,"# Summary
* Add functionality to wrap SQL Lab queries with a limit
A common problem people have is that they input a query in SqlLab to preview their data before hitting visualize to create a table, and then it takes forever to run. This can be a pain (on them and potentially on the database).
Currently one workaround is to manually enter a limit in the query editor, visualizing (which creates the datasource), and then later removing it from the base query (by editing the created datasource). With this in mind, an additional/alternative solution might be to allow users to edit the sub-query in the visualize modal before creating their datasource.
So here's a way to have a limit only in the context of SQL editor. The limit is only applied as a wrapper for select queries, and not saved in any persistent database.
https://github.com/apache/incubator-superset/pull/4834 - I like this idea, but not sure if every database supports prefetching. If going with this route, I'd have a configurable page size.
https://github.com/apache/incubator-superset/issues/4588",non_debt,-
skywalking,2730,review,287595280,"**RunningContext** designed for in thread propagation to avoid one object accessed by multiple threads. **Async APIs**  designed for the span across threads to finish and set tag.
Choose the right API fitting your scenario.",non_debt,-
flink,7976,comment,472939243,"@sunjincheng121 Thanks a lot for your review. I have addressed all your comments.
@twalthr @sunjincheng121 I have also rebased to the deprecating window pr and delete the deprecated methods and classes in this one. Would be great if you can take a look. 
Best, Hequn",non_debt,-
airflow,528,description,0,33884891-528 description-0,non_debt,-
storm,1717,description,0,Update eventhub client dependency. Move Storm-Eventhubs dependency vesion definitions to parent pom. Introduce new schemes for message handling,non_debt,-
flink,4383,comment,347669679,@zentol @fhueske I am merging the change with the extra space since this looks to have been the original intent. I've looked at both forms without finding a strong preference.,non_debt,-
tajo,123,summary,0,TAJO-1013: A complex equality condition including columns of the same table is recognized as a join condition.,non_debt,-
spark,23117,comment,457023033,"Ah, for PRB, it will be disabled by default unless it's manually enabled like I did in this PR:
https://github.com/apache/spark/pull/23117/files#diff-9657d18e98a7dc82ca214359cfd6bdc4R630
I'm gonna try to fix it anyway. I know it's discouraged for other people excpet the build manager (you) to touch the job like that in general so wanted to have a way to test it in PRB.
So, setting `SPARK_TEST_KEY` environment variable alone in PRB should be okay .. It won't affect the other PRs if you don't manually change the build script as i did here to enable it.",non_debt,-
nifi,2561,review,175764796,"How do nested records end up being represented in the Solr document? Not saying anything is wrong here, just asking to understand how it works.
Lets say we have a person schema with top-level fields for ""firstName"" and ""lastName"", and ""address"", and the address field is of type record and then has it's own fields ""street"", ""city"", ""zip""...  
Does the resulting Solr document contain ""firstName"", ""lastName"", ""street"", ""city"", ""zip""? 
Would it make sense to have an option to include the parent field in the field names, so it ends up being ""address_street"", ""address_city"", and ""address_zip"" so that you know where those fields came from?",non_debt,-
incubator-pinot,883,review,90938493,you can set the defaultValue in the java code. PathParam can have the default value as well.,non_debt,-
spark,19717,comment,350180764,"I agree service creation should be done by the scheduler backend code so to get rid of dependency on the submission client. 
Yes, I agree that we eventually should allow client mode including use cases that directly create `SparkContext`. But until we have a solid well-tested solution, I think we should disable it for now. We can always revisit this once we have a good solution. Regarding `kubernetes.default.svc`, yes, it's a good indication. But again, the driver service must exist. Unless we change to have the backend create that service, this still won't work.",non_debt,-
storm,1616,comment,238843260,Thanks @HeartSaVioR,non_debt,-
samza,213,description,0,32199982-213 description-0,non_debt,-
flink,12906,comment,665655285,@aljoscha Thanks. This time it is green.,non_debt,-
kafka,7378,review,331360311,`outputTopic2`,non_debt,-
kafka,863,summary,0,MINOR: Pin to system tests to ducktape 0.3.10,non_debt,-
incubator-brooklyn,193,comment,57035271,good move.  i think @andreaturli 's fixes from this class were incorporated in 1.8 so no more need for this.  can you confirm @andreaturli ?,non_debt,-
groovy,815,summary,0,GROOVY-8855: Matcher.asBoolean() does not rely on matchers internal search index anymore,non_debt,-
iceberg,1910,comment,743341217,Thanks @jackye1995!,non_debt,-
flink,6744,comment,424324375,"Hi @pnowojski , thanks for fixing the problem. Looks good to me. Same error exist in comments of `org.apache.flink.table.functions.TableFunction`.",non_debt,-
beam,12572,comment,719765475,Run Java_Examples_Dataflow_Java11 PreCommit,non_debt,-
trafficserver,3523,description,0,"Removes ""disallow"" pattern support
Addresses issue #1645",non_debt,-
spark,19094,comment,326445912,I close this issue. Thank you again.,non_debt,-
spark,19468,review,152633977,Done.,non_debt,-
thrift,963,summary,0,THRIFT-3758 TApplicationException::getType and TProtocolException::getType should be const,non_debt,-
nutch,331,summary,0,NUTCH-2581 Caching of redirected robots.txt may overwrite correct robots.txt rules,non_debt,-
spark,3945,comment,69397383,@jkbradley I've fixed it up. Thanks!,non_debt,-
superset,9733,summary,0,[perf logging] Add timing event when browser tab is hidden,non_debt,-
apisix,2001,review,467670387,"This code file is copied from https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/balancer/ewma.lua, so we MUST keep the oirgin license and copyright, this is respect for other open source projects and a rule that Apache project must follow.",non_debt,-
trafficserver,3559,comment,386419709,I thought you said it would be `TSRemapConfigLoadStart` for future symmetry concerns.,non_debt,-
phoenix,936,comment,724872173,"1. Not sure about it but we can introduce additional constraints like the total size of scanned bytes as you suggested to further improve this feature later. 
2. This is correct. By itself, it does not eliminate. However, the client can wait for all the page operation to complete or fail before returning to the application, as an additional improvement. This will further reduce the race conditions. I think we have to enforce the client side timestamp to make the race almost impossible.
3. I expect this feature will improve the overall performance and availability since paging limits the memory usage and the time to hold server resources. My experience with paging on a real cluster is very positive.  I have not seen any negative impact yet as long as the page size is not very small (e.g., less than 1000).",design_debt,non-optimal_design
spark,8897,comment,142793223,@weberxie this seems to be opened by mistake. Would you mind closing this patch?,non_debt,-
dubbo,2124,summary,0,fix dubbo-rpc-http resouces setting name,non_debt,-
storm,1139,review,53633738,You need not use Persistent\* classes. Instead you can return lists/map etc. and in the clojure code call clojurify-structure on the result e.g.,non_debt,-
pulsar,399,comment,301202227,"I see : 
We use a tree-set to redeliver in order, though it shouldn't be strictly necessary. 
I guess we could either: 
 * Use ConcurrentLongPairSet and forget about order
 * Implement a `Queue< Pair<Long, Long> >` that is actually implemented as an array of primitive longs",non_debt,-
tinkerpop,1353,comment,723240681,I just noticed that my IntelliJ code formatter may have been a little over-zealous with the TestSupport file. I can fix that if it bugs anyone.,non_debt,-
gobblin,3214,review,565722354,"Yeah, it's a newly added method which will be called in kafkaStreamingExtractor.shutdown(), and the kafkaStreamingExtractor.shutdown() method did not call super.shutdown(), so I did not add it here as well. Also super.shutdown is meaning to check it's a decorator, which is not applied to this case as well.",non_debt,-
superset,1612,review,88320722,can this be `camelCase`?,non_debt,-
trafficserver,4770,review,246241031,"I don't think we can do this because `maximum_frame_size` can be smaller than the frame popped. We don't need to think about a window for flow control in this case, but we still need to think about a size of packet. MTU may be changed. You wrote `split()` for it, right?",non_debt,-
carbondata,3157,comment,475329184,"Build Failed with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/3012/",non_debt,-
beam,11843,comment,635455832,Run Python PreCommit,non_debt,-
iceberg,2430,summary,0,[WIP]Spark: Bump Spark 3 version to 3.1.1,non_debt,-
skywalking,1506,review,206031702,"I am afraid that we misguide people to think we or zipkin traces an OP, which needs 1ms but actually it is just a simple `+`(or others). Anyway, both ways should be fine. SkyWalking sometimes duration is 0ms too.
You can make the decision, I just ask and give you suggestions.",non_debt,-
beam,9765,comment,550584252,I'm not sure if RetryPolicy and BackoffStrategy apply to LimitExceededException / ProvisionedThroughputExceededException but I can look into that. If so I think it makes sense to just configure this in the Kinesis client instead of having a BackoffRateLimitPolicy. What do you think @aromanenko-dev and @lukecwik ?,non_debt,-
kafka,9200,comment,678666729,Ok to test,non_debt,-
hudi,1664,description,0,"Default value of number of delta commits required to do inline compaction (hoodie.compact.inline.max.delta.commits) is currently 1. Because of this by default every delta commit to MERGE ON READ table is doing inline compaction as well automatically.
I think default value of 1 is little overkill and also it will make MERGE ON READ work like COPY ON WRITE with compaction on every run. I am propsing to increaset the value to old default of 10.
This pull request is a trivial rework / code cleanup without any test coverage.",design_debt,non-optimal_design
spark,31451,comment,810792543,Okay. Let me have a simple test DS v2 locally and capture some screenshots of the web UI.,non_debt,-
ozone,1014,comment,638315154,"Updated the sonar report. Will see how does it work after the next build on **apache** repo:
https://github.com/apache/hadoop-ozone/actions?query=branch%3AHDDS-3710",non_debt,-
spark,11581,review,56293008,"Sorry, I thought this was in a `private[ml]` trait that could be inherited by a public class.",non_debt,-
openwhisk-wskdeploy,811,comment,374763837,"@beemarie `exec` deals with the set of env. variables specified in execOptions, its nothing to do with `wskdeploy`. Its more about injecting those env. variables (cloudant username etc) into env. where `wskdeploy` is being called:",non_debt,-
storm,845,review,46338107,"Minor, but I think we do not need to `setup-blob-permission` here, since we are doing it next unconditionally.",non_debt,-
ignite,2068,description,0,31006158-2068 description-0,non_debt,-
spark,17186,comment,549686880,"May I ask how to solve this issue in spark2.3?
I see that this flag has been removed in PruneFilters and EliminateOuterJoin. @viirya @gatorsmile",non_debt,-
flink,15139,comment,796517842,Test failure is unrelated (FLINK-21647). Merging. Thanks again.,non_debt,-
flink,6970,comment,450542738,"I'm excited to see this merged, because it will help with registering a suite of objects (e.g. the taxi tables and functions) succinctly.",non_debt,-
druid,1044,review,26169017,"why not do the check for post-aggregators naming here, before adding it to combinedAggNames?",non_debt,-
openwhisk,1071,review,75486973,"As noted in personal discussion:
I found it hard to grasp the code because we use `Unit` returning functions which resolve a `Promise` at some point. This reads like it blocks the code until the result is there (which makes no sense at this point).
I'ld prefer to use `Future` composition if possible so it becomes more apparent that this is in fact returning a Future which is resolved as soon as the ack-message is there.",code_debt,low_quality_code
incubator-pinot,522,comment,246064224,19961085-522 comment-246064224,non_debt,-
incubator-weex,3304,summary,0,[ios] callNativeModule support unicode characters,non_debt,-
incubator-mxnet,18319,review,427142694,We need use them in `.cu` file.,non_debt,-
iceberg,1587,review,526543001,This is another test I think could be broken into distinct cases with a `@Before` to set up the default branch and table.,non_debt,-
ozone,1371,review,486098006,increase the counter after successfully created the pipeline ? Might throw exception during the creation.,non_debt,-
pulsar,2888,comment,441582678,"The ""data:"" parser you wrote would be a duplicate code because it's already implemented in the in-house URL class I wrote.  I'm not going to make a change request because I don't want to delay this feature, but either of the parsers should be removed eventually to eliminate duplicate code.
Also, because environment variables are general enough as data sources, I think ""env:"" parser should be added as a `URLStreamHandler` so that other places can use it too via the in-house URL class.
As for ""token:"", I'm ok with having it in this plugin as a special case. But if you renamed it to ""raw:"", it could be used on other places.",code_debt,duplicated_code
spark,11136,comment,182905997,"Jenkins, test this please.",non_debt,-
airflow,8875,review,426287475,"It _may_ be doable, but is not documented/clear. I think when I added timezones I got it working for display side (rather i got it using my custom control to decorate the fields so they get converted)",documentation_debt,low_quality_documentation
spark,21102,review,203322643,For accuracy sake - my example snippet above will fail much earlier - due to `OpenHashSet. MAX_CAPACITY`. Though that is probably not the point anyway :-),non_debt,-
ozone,1392,review,485765534,"Yes, sure.",non_debt,-
superset,5445,comment,457472346,"This PR fell in a crack. Let's revive it. Seems like the long term solution is not using WTForms at all as we rip out more of the MVC and FAB scaffolding over time.
So I get the `NULL` vs `''` problem with unique constraints not applying properly, but other than that, does it affect users directly? I'm guessing it may affect the filtering functionality in the CRUD list view UI?",design_debt,non-optimal_design
griffin,442,review,226866876,"LGTM üëç make this cache field explicit, instead of implicit property",non_debt,-
cassandra,200,description,0,206424-200 description-0,non_debt,-
spark,7440,summary,0,[SPARK-9052][SparkR] Fix comments after curly braces,non_debt,-
flink,5901,comment,387376186,"By the way, the test still doesn't catch the bug that you're fixing (https://issues.apache.org/jira/browse/FLINK-8286). I think we need proper end-to-end tests that really test Flink-Kerberos integration on an actual YARN cluster. I have started looking onto using Docker Compose for that, i.e. bringing up a hadoop cluster in docker with Kerberos and then running Flink on that as an end-to-end test.",test_debt,low_coverage
trafficserver,1457,comment,280944148,"This introduced a new Coverity issues:
Assuming we don't revert this change, we need to fix this.",non_debt,-
spark,13421,review,65285433,That's sad ...,non_debt,-
kafka,7098,summary,0,KAFKA-8599: Use automatic RPC generation in ExpireDelegationToken,non_debt,-
spark,3073,review,19971716,Then maybe we should augment this to say that this stack trace recursively includes the causes of the exception,non_debt,-
hadoop,1404,review,363390848,add a newline after. thanks,non_debt,-
hadoop,1707,review,351791207,we don't do the put here. what if implementation changes and there will be two puts? do we need to keep these in sync?,non_debt,-
beam,12326,summary,0,[BEAM-9865] Cleanup Jenkins WS on successful jobs,non_debt,-
nifi,3217,comment,448107698,"LGTM +1, merging. Thanks @mattyb149!",non_debt,-
openwhisk,3812,review,204137242,"The `TestKubernetesClient` is shared with `KubernetesContainerTests` - so for now, changed to implicit ActorSystem (and left object/classes in current places). Good?",non_debt,-
carbondata,2320,comment,390190760,"Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/4811/",non_debt,-
trafficcontrol,4011,summary,0,Implement GET /api/1.1/roles handler,non_debt,-
phoenix,735,review,394609771,Any reasons to have these empty methods?,non_debt,-
pulsar,5124,summary,0,[doc] fix the wrong file extension name,non_debt,-
openwhisk,2968,description,0,"This PR ads two additional metrics:
- loadbalancer_activations_<namespaceId>_count : counter for activation starts in load balancer
- invoker_container_start_<containerState>_<namespace>_<action>_ count
I also adds another info level log statement with container start information",non_debt,-
spark,25214,comment,559282566,"So its one test being failed in a different Hive version. I assume the test is being failed as expected because Hive fixed it in 2.0. Why didn't we just fix the test? Seems root cause was clear, easy to fix the test, and didn't break PR build.",non_debt,-
beam,3572,summary,0,[BEAM-1286] DataflowRunner handling of missing filesToStage,non_debt,-
iceberg,2218,comment,774379247,"Thanks, @shardulm94! Great to have this fixed!",non_debt,-
ignite,1914,description,0,31006158-1914 description-0,non_debt,-
helix,642,review,359655490,"From the code in waitUntilConnected, if false is returned, it indicates the deadline has elapsed(timed out).",non_debt,-
kafka,241,review,40615287,"Typo. ""Unless the..."" Don't leave us hanging!",documentation_debt,low_quality_documentation
cloudstack,2118,review,117468112,"If create VM snapshot fails immediately after instance creation, the issue might exists.",non_debt,-
dubbo,3162,review,245860102,"Personally think that this place is worth discussing. Most of the exceptions have been handled in connect and disconnect, and there is a log. I think the log here can be removed.",code_debt,low_quality_code
carbondata,1919,review,165807745,"suggest change ""dmClassName"" to ""providerName""",non_debt,-
spark,1612,description,0,"SPARK-2710 Build SchemaRDD from a JdbcRDD with MetaData
and a small bug fix on JdbcRDD, line 109
it seems conn will never be closed",non_debt,-
airflow,2622,comment,331186798,"@chie8842 Please review the commit guidelines:
My commits all reference JIRA issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from ""How to write a good git commit message"":
- Subject is separated from body by a blank line
- Subject is limited to 50 characters
- Subject does not end with a period
- Subject uses the imperative mood (""add"", not ""adding"")
- Body wraps at 72 characters
- Body explains ""what"" and ""why"", not ""how""
Your commit should start with `[AIRFLOW-1331]` and it should be a single commit (not two as now).",non_debt,-
nifi-minifi-cpp,835,review,455554004,"Yes, most of the time it is, but now always. There other places includes happen like this, might worth the investigation why, but I don't think it is relevant for this PR.",non_debt,-
airflow,11194,summary,0,Fixes image tag readonly failure,non_debt,-
flink,213,comment,63619839,"Very good.
+1 to merge",non_debt,-
beam,3397,comment,309600706,"As mentioned in person, LGTM.",non_debt,-
activemq-artemis,2783,comment,522017246,"@Premik, can you please squash your commits and rebase? A defect was recently fixed that prevented the PR test build from finishing properly. Thanks!",non_debt,-
hbase,2138,review,460302572,Should the thread count be related to number of cores for the processor ?,non_debt,-
airflow,3592,comment,404091019,@yrqls21 PTAL,non_debt,-
apisix,2177,review,518045336,last commit fix it,non_debt,-
beam,13592,comment,749735258,"That should not be a problem as in most cases, processing a single `element` in this case means emitting quite many elements downstream. The cost of the structuralValue should be pretty much amortized I would say. We could do something to calculate it only once per element?
I don't think we can use a reference either, because the UnboundedSource is Serializable and any runner is free to clone it (which is what DirectRunner does, afaik). The best solution would seem to be to mark each initially split (via @SplitRestriction) restriction with unique ID and then transfer this ID to all residual restrictions. There should be always be at most one ""active"" (either currently being processed or having non-null residual) restriction, so we could use that for identifying the reader without referencing Source (which might be problematic, as implementing hashCode and/or equals for UnboundedSource will not be a common practice, I'm afraid).",code_debt,low_quality_code
hbase,1280,review,395122159,No? It is the Table implementation for rest.,non_debt,-
geode-native,56,description,0,"- section title is Interoperability of Language Classes and Types
- corrected namespaces (packages)
- removed duplicate table captions
@davebarnes97 @joeymcallister @dgkimura @mmartell @echobravopapa @PivotalSarge Can you please review these (mostly namespace) changes in the docs?",code_debt,duplicated_code
arrow,7421,description,0,This is some routine scrubbing that deals with some remnants of the prior dual py2/3 codebase.,non_debt,-
arrow,4397,comment,504091482,"Well, this is one step in the right direction. 
But we also have https://issues.apache.org/jira/browse/ARROW-2136
If we're going to respect the `nullable` flag, we also should not permit nulls to pass through silently
I also opened https://issues.apache.org/jira/browse/ARROW-5668 as a usability improvement",non_debt,-
zeppelin,3045,comment,401314522,@prabhjyotsingh @zjffdu @gss2002 Can you please help review this ?,non_debt,-
parquet-mr,51,review,17205549,"what about 
#immutable",non_debt,-
trafodion,24,comment,121346636,"done.
On Tue, Jul 14, 2015 at 1:15 PM, Steve Varnau notifications@github.com
wrote:
Regards, --Qifan",non_debt,-
airflow,5027,comment,479026532,The two K8S tests failed again after my restarting.,non_debt,-
nifi,4738,review,547946268,This property descriptor is missing a `displayName()` field.,non_debt,-
spark,3543,review,21571141,"I think this should be using the ""hadoopConfiguration"" object in the SparkContext. That has all the hadoop related configuration already setup and should be what is automatically used. @marmbrus should have a better idea.",non_debt,-
storm,1783,comment,263910453,"@ambud 
The code looks good except what @vesense commented. 
Two things more to address:
1. It would be better to document new configurations. Without documentation, end-users have no idea about added feature. `external/storm-hbase/README.md` and `docs/storm-hbase.md`.
2. The code already uses JDK 8 API (Map.getOrDefault()), so can't get it as it is for 1.x. Could you provide a new PR for 1.x branch?
It would be also great if you can test it (with Caffeine) on JRE7 (expected to not work but we can document the precondition for JRE version) and JRE8 (expected to work).
cc. @ben-manes Is my expectation right?
Thanks in advance!",documentation_debt,outdated_documentation
tvm,2232,review,244103695,70746484-2232 review-244103695,non_debt,-
beam,2369,comment,290527661,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8959/
--none--",non_debt,-
spark,5232,comment,88369109,@mengxr fixed!,non_debt,-
spark,15627,review,85951050,"If we can cleanup the variables names above I think it would help a lot, the test is confusing.  I know you just copy and pasted but would be nice to clean up.
Also can we have 3 tests or 3 asserts,  
- one for same file in --files
- one for same file in --archives
- one for same file in --files and --archives",code_debt,low_quality_code
qpid-dispatch,624,description,0,"Bumps [angular](https://github.com/angular/angular.js) from 1.5.11 to 1.7.9.
*Sourced from [angular's changelog](https://github.com/angular/angular.js/blob/master/CHANGELOG.md).*
- Additional commits viewable in [compare view](https://github.com/angular/angular.js/compare/v1.5.11...v1.7.9)
This version was pushed to npm by [petebacondarwin](https://www.npmjs.com/~petebacondarwin), a new releaser for angular since your current version.
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/qpid-dispatch/network/alerts).",non_debt,-
beam,7462,comment,453257346,Run Java PostCommit,non_debt,-
dubbo,2228,review,214629281,"@lexburner @kimmking i think three for statement can be combined too,my impl:
    public <T> List<Invoker<T>> route(List<Invoker<T>> invokers, URL url, Invocation invocation) throws RpcException {
        // filter
        List<Invoker<T>> result = new ArrayList<>();
        // all invokers that don't have ""tag"" parameter in url is a normal invoker
        List<Invoker<T>> normalResult = new ArrayList<>();
        try {
            // Dynamic param
            String tag = RpcContext.getContext().getAttachment(REQUEST_TAG_KEY);
            for (Invoker<T> invoker : invokers) {
                if (StringUtils.isEmpty(invoker.getUrl().getParameter(TAG_KEY))) {
                    // all invokers that don't have ""tag"" parameter in url is a normal invoker
                    normalResult.add(invoker);
                } else {
                    if (invoker.getUrl().getParameter(TAG_KEY).equals(tag)) {
                        result.add(invoker);
                    }
                }
            }
            // If no invoker be selected, downgrade to normal invokers
            if (result.isEmpty()) {
                return normalResult;
            }
            return result;
        } catch (Exception e) {
            logger.error(""Route by tag error,return all invokers."", e);
        }
        // Downgrade to all invokers
        return invokers;
    }",non_debt,-
airflow,1881,review,88095097,You're right.  That one is my bad.  I forgot to go and update all of these after I created the sign_in function.,non_debt,-
incubator-mxnet,11502,review,200206922,Needs more explanation on what this is used for and how do users use this,documentation_debt,low_quality_documentation
druid,2602,comment,200630606,"@binlijin for example, under docs/content, you can run `jekyll serve` to render the docs and the html page should be rendered",non_debt,-
spark,204,review,11469526,"Very minor - but I'm guessing the first error condition will happen way more often than the second - and getting a message that says it's ""invalid"" is a lot less clear than just saying it doesn't exist. What about breaking these out and including more specific error messages in either case? Also it might be nice to print the value of `fileSystem` in the error message, so it knows what filesystem we inferred the path to be from.",code_debt,low_quality_code
beam,12572,description,0,"Changes include:
- Add `KafkaCommitOffset` transform, which expands to 
PCollection<KV<KafkaSourceDescriptor, KafkaRecord>> -> Map to KV<KafkaSourceDescriptor, (Long)offset> -> WindowInto(5 min FixWindow) -> Max.longsPerKey() -> CommitOffsetDoFn
- Change `ReadFromKafkaDoFn` to output KV<KafkaSourceDescriptor, KafkaRecord> instead of KafkaRecord only.
- Add commit offset expansion to KafkaIO.ReadSourceDescriptors:
ParDo(ReadFromKafkaDoFn) -->
Reshuffle() --> Map(output KafkaRecord) --> output
        - -> KafkaCommitOffset
**Note** that this expansion is not supported when using x-lang on Dataflow.
r: @aromanenko-dev 
cc: @lukecwik 
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.",non_debt,-
arrow,1704,summary,0,ARROW-2265: [Python] Use CheckExact when serializing lists and numpy arrays.,non_debt,-
attic-apex-core,479,review,108591304,"This method doesn't always create a new DeployRequest - can we name it appropriately? Say ""addOrModifyDeployRequest"" or ""getDeployRequestForContainerId"". By the way, would it be an error in some cases to find an existing DeployRequest for a given containerId? For example when you are calling this from StreamingContainerManager.scheduleContainerRestart(), wouldn't it be an error to find an existing request? If true, we should add code to detect such errors. The caller passes a parameter to indicate that only new request is expected to be created and so on...",code_debt,low_quality_code
drill,844,review,123631660,"You may add comments why we need such non-trivial project, due to Calcite's behavior of ProjectRemoveRule. Also, it might be worth mentioning that the added Project may not be sitting right under Screen/Writer operator in the physical plan, as we would add additional Projects, for case likes * column expansion, or partition by column processing.",non_debt,-
kafka,7887,comment,577819902,"hi @mimaison understand that this is not super critical, but since it is a simple fix, just want to check in and see if you think this approach is acceptable. would like to hear your thoughts and comments.",non_debt,-
daffodil,262,review,324346636,"I think this is fine, as it only gets called by the unordered parser, which means it will only sort the children in the unordered sequence.  In the tests I have, including tests where the unordered sequence is part of a larger sequence, childNodes only contains the elements of the unordered sequence.",non_debt,-
spark,13310,review,64671567,can we put this in SparkSessionBuilderSuite?,non_debt,-
helix,510,review,334200441,Why are we removing this?,non_debt,-
rocketmq,145,comment,321778238,Coverage decreased (-0.2%) to 38.742% when pulling **c818e364fe910821913ffcc83414c1b77241a374 on lindzh:fix_test_exeception** into **ccc2235ae9509f101971915ba0521109a82894b0 on apache:develop**.,non_debt,-
lucene-solr,345,review,179737921,"let's validate that all terms have `field` as a field, or directly take a list of BytesRefs?",non_debt,-
tvm,3515,review,301662897,Why is opt_level here? Isn't defining it in PassInfoNode sufficient?,non_debt,-
spark,1358,comment,48630513,Can one of the admins verify this patch?,non_debt,-
flink,13351,review,496457872,Removed `catch InterruptedException` and added to the method signature after not converging in an offline discussion.,non_debt,-
nifi-minifi-cpp,797,review,443776103,"Good point, I take my suggestions back.",non_debt,-
hbase,2044,comment,660410619,"The prob is that here we want change the multiplier everyday when the current hour switch between peak and offpeak, there will not a config changed event.
Thanks. @virajjasani",non_debt,-
spark,29654,review,488150906,"@viirya Sure. Here's complete graphs
Before:
After:",non_debt,-
arrow,1201,summary,0,"ARROW-1674: [Format, C++] Add support for byte length booleans in Tensors",non_debt,-
dubbo,2741,summary,0,Fixing flaky tests in PortTelnetHandlerTest,test_debt,flaky_test
arrow,8401,review,522113945,I think it might be worth `assert!` ing that there are no child data arrays so we don't have a silent (and hard to debug failure),non_debt,-
druid,7528,review,278316219,"I do not think it makes sense to merge SupervisorActionsDialog and TasksActionsDialog into one high level TableActionDialog. You should have a TableActionDialog that is like the snitch dialog (never directly used but used by a `SupervisorActionsDialog` and a `TasksActionsDialog`. The `TableActionDialog` should take care of the (grid) layout and the tabs, the specific dialogs should add their own views.",non_debt,-
beam,3852,review,139203104,+1,non_debt,-
spark,1222,review,15478958,"We shouldn't remove this... This is intended to be a general purpose class for logging that handles compression and buffering and other logic. I notice that you replaced this with PrintWriter elsewhere. Even with the new naming scheme you can still use this class for event logging, and it will simplify `EventLoggingListener` a bunch.",code_debt,low_quality_code
nifi,3894,review,365769826,Unused import.,build_debt,over-declared_dependencies
flink,12746,summary,0,[FLINK-15416][task][network] Retry connection to the upstream,non_debt,-
geode,4755,review,387979913,Done,non_debt,-
incubator-mxnet,16859,summary,0,Mixed-type mx.np.power,non_debt,-
incubator-dolphinscheduler,3417,review,475078725,"Thx a lot for your review~
Got it, because @SneakyThrows is the feature of lombok, I will suggest community to introduce the lombok in the future.",non_debt,-
samza,490,review,184773952,"Do we need the stateChangedCounter in-addition to the stateChangedLatch? It looks like stateChangedLatch.countDown is exactly equivalent to ""stateChangedCounter == 1""",non_debt,-
kafka,8483,review,421863398,There is no global table changelog topic -- this test really only write into the global input topic and your verification step was to verify that this write is picked up. Will add a corresponding test in a follow up PR.,non_debt,-
pulsar,366,comment,296908563,"@rdhabalia Maybe the name `NonDurableCursor` doesn't fully convey the intended semantic for the new class. The context here is just to have a way to read through a topic (eg: support `TopicReader`) and reuse as much code as possible from the regular cursor implementation. Basically all the cache code and the logic for how to switch to ""next valid position"", plus the asyncReadOrWait stuff.
About non impeding messages to be deleted, consider that in case of non-durable topic, during a disconnection the cursor will go away and data will get potentially deleted anyway. So I prefer it to be explicitly ingrained into the API. Same thing about naming the cursor. It will go away in any case after a restart, so I don't see the advantage of naming it.
When data gets deleted, the cursor will just skip over it. The intended usage for the non-durable cursor and topic reader is in conjunction with the message retention, to make sure data sticks around for the intended amount of time",code_debt,low_quality_code
avro,336,review,221416112,"My C++ skills are not so great. After university I haven't done much, so I'm not familiar with the different versions. My preference would be to update #335 after this one gets merged and then enable the test again üëç",non_debt,-
incubator-pinot,5266,description,0,"1. Defines Enum ServiceRole: `CONTROLLER`/`BROKER`/`SERVER`/`MINION` for the Pinot Components to manage
2. Defines interface: `ServiceStartable`, so each role could implement its own way to start/stop it.
3. Make `ControllerStarter`,`HelixBrokerStarter`, `HelixServerStarter`, `MinionStarter` to implement interface `ServiceStartable`.
4. Implement `PinotServiceManagerStarter` as a new entry point to manage all pinot roles lifecycles.
5. Make `ServiceStatus` could handle multiple Pinot roles.
6. Provide REST API to start/stop Pinot Roles with default configs.
7. Move `ControllerStarter`/`BrokerStarter`/`ServerStarter` commands to use new `ServiceManagerStarer` command. Hence all Quickstarts will use it transparently.
* Start PinotServiceManager along with controller/broker/server by default.
 * Start PinotServiceManager along with Broker&Server with default configs
 * Start PinotServiceManager along with Broker&Server with bootstrap configs.
* Once PinotServiceManager is up, it exposes APIs using swagger.
- `HelixBrokerStarter`, method `shutdown()` is replaced as `stop()`
- `HelixServerStarter` requires an explicitly call of `start()` to start it. In
the old behavior, there is no `start()` method and the constructor will also take care of start server.",non_debt,-
calcite,1626,comment,561980030,"Thanks for review and merge, I'll delete the branch.",non_debt,-
iceberg,2078,review,558255721,Tried my best üòÑ Please check the new one.,non_debt,-
trafficcontrol,4747,review,443827162,"idk what ""my-directive"" is or why it's in there. I'll get rid of it.
as far as ""acting weird,"" no, I don't think I have noticed that, but I'll look at it.",non_debt,-
kafka,6536,review,275588869,"Well, my point is, that the check can be simplified. I don't think that `record.headers() == null` can be true; it's guaranteed that there is a headers object.
Not sure if we can simplify the second check. It iterators over the headers map and does String comparison to find a header with key `v` -- seems to be rather heavy.",code_debt,complex_code
calcite,1075,review,261510267,"I would suggest to change the name `PRETTY_JSONISE` to `JSON_PRETTY`, also the string value and test method names below. Using a unified name convention will benefit new developers when they do a code searching.",code_debt,low_quality_code
streams,276,description,0,changes in https://github.com/apache/incubator-streams-master/pull/2 should merge first,non_debt,-
incubator-mxnet,7452,description,0,@tqchen,non_debt,-
pulsar,9329,comment,777186013,/pulsarbot run-failure-checks,non_debt,-
kafka,2007,comment,267234396,Thanks for rebasing!,non_debt,-
spark,13969,review,70201841,"So what's a good name? I am not attached to Reflect, but I think Reflect should be in the name, if the function is called reflect.",non_debt,-
bigtop,143,review,79561910,fixed,non_debt,-
skywalking,5055,comment,655316356,Has this fixed your issue locally?,non_debt,-
pulsar,7601,description,0,"-->
Fixes #<xyz>
Master Issue: #<xyz>
This mr adds an api to check if the worker is ready to serve requests.
*Describe the modifications you've done.*
This change is a trivial rework / code cleanup without any test coverage.
This change is already covered by existing tests, such as *(please describe tests)*.
This change added tests and can be verified as follows:
  - If a feature is not applicable for documentation, explain why?
  - If a feature is not documented yet in this PR, please create a followup issue for adding the documentation",documentation_debt,low_quality_documentation
zeppelin,2753,summary,0,ZEPPELIN-3138. checkstyle for zeppelin-interpreter,code_debt,low_quality_code
nifi,2194,comment,337127297,"@patricker All LGTM, +1. Merged to master, thank you!",non_debt,-
arrow,965,summary,0,ARROW-759: [Python] Serializing large class of Python objects in Apache Arrow,non_debt,-
spark,15868,comment,262039578,Let me find the correct code. (It's weird I attached a wrong code example.),non_debt,-
flink,13410,comment,697080083,"Hi, @klion26 
This PR is ready to be reviewed. Could you help to review it at your convenience?
Thanks~",non_debt,-
spark,15664,review,94098595,"`getSchema` will throw an exception when the schema contains an unsupported type. Now we use it to check if the table exists. Does it change current behavior? E.g., the insertion working before now fails.",non_debt,-
arrow,6257,comment,577263359,"Just to understand: is there a fundamental reason to not allow it there as well, or is it just ""didn't tackle that here""? 
Currently, in the python datasets code, I am not passing each path through `FilesystemFromUri`, so ideally a path passed to eg `GetTargetStats` would also be sanitized IMO.",non_debt,-
beam,5433,comment,395838547,"OK, I'm working toward using the `RexToLixTranslator` starting with #5544 to move to calc and make it easier. It will probably drag on for a while, so I'm thinking I'll revive this. But instead of the painstaking work of keeping these low-level unit tests, I will move to `SqlStdOperatorTable` or `SqlFunctions` at least, and just delete the tests and transform to SQL DSL smoke tests only.",non_debt,-
beam,13252,summary,0,[BEAM-9547] Provide some top level pandas functions.,non_debt,-
hawq,1169,summary,0,HAWQ-1381. fix Core dump when execute 'select * from hawq_toolkit.__h‚Ä¶,non_debt,-
incubator-doris,4984,review,557010119,99919302-4984 review-557010119,non_debt,-
kafka,7248,review,338615161,"What I settled on is that the `validVersions` would be a range encompassing all the supported protocol versions. The individual fields would use an open range when they're added (like `3+`), and then use a closed range when they are removed. For example, in KIP-441, we're going to set `validVersions: 1-6` and then set (for example) `prevTasks: 1-5`, while adding new fields `6+`.
This scheme would give us the smallest diff when we do have to bump the protocol version without changing any fields (which happens with some regularity). It also makes the most intuitive sense, IMHO, to use open ranges for the fields when you add them, because you would just assume they'll remain part of the protocol indefinitely.",non_debt,-
kafka,7616,description,0,"Adding a dynamically updatable log config is currently error prone, as it is easy to set them up as a val not a def and this would result in a dynamically updated broker default not applying to a LogConfig after broker restart.
This PR adds a guard against introducing these issues by ensuring that all log configs are exhaustively checked via a test.
For example, if the following line was a val and not a def, there would be a problem with dynamically updating broker defaults for the config.
https://github.com/apache/kafka/blob/4bde9bb3ccaf5571be76cb96ea051dadaeeaf5c7/core/src/main/scala/kafka/server/KafkaConfig.scala#L1216",non_debt,-
openwhisk,3072,comment,350013610,"Okay, that makes sense.
The major difference from the better-known clients is, that their error behavior is more principled (i.e. controllable via a `SupervisionStrategy`).
In any case: If we make sure that:
1. Errors we know of  are handled
2. Errors we don't know of just result in a restart of the consumer/producer
I guess we are fine.
I'll review more thoroughly tomorrow. Testing new clients/completely moving away from our own ones is a much larger task that should be seen orthogonal.",non_debt,-
spark,29660,summary,0,[SPARK-32808][SQL] Fix some test cases of `sql/core` module in scala 2.13,non_debt,-
beam,8381,comment,487884491,Run Python PostCommit,non_debt,-
dubbo,2209,review,208792691,Remove this line,non_debt,-
spark,14079,review,92103779,"Ok cool can you add a comment here with the first thing that you said (""For locality preferences, ignore preferences for nodes that are blacklisted"")",non_debt,-
flink,1640,review,52993965,"I think you can create the callback once in the open method and then pass the instance to all async calls.
This way, you save a lot of instance creations",code_debt,low_quality_code
storm,1113,description,0,14135470-1113 description-0,non_debt,-
guacamole-client,79,summary,0,GUACAMOLE-116: Replace minified AngularJS with non-minified AngularJS.,non_debt,-
airflow,4751,review,346970934,"This works, but it asks for a lot more columns and rows than we need.
We could try changing the return inside this function from `return tis.all()` to just `return tis`, and this line could become:
https://docs.sqlalchemy.org/en/13/orm/loading_columns.html#load-only-and-wildcard-options
Do you think this is worth it or not worth it?",code_debt,low_quality_code
hudi,191,comment,309609191,"Sounds good. Will take a closer look sometime this week. 
What I meant was, have you been able to test this out on any real production datasets yet?",non_debt,-
spark,30987,review,550769916,"The logic same to SizeInBytesOnlyStatsPlanVisitor#default, just add rowCount:
https://github.com/apache/spark/blob/711d8dd28afd9af92b025f9908534e5f1d575042/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/SizeInBytesOnlyStatsPlanVisitor.scala#L50-L58",non_debt,-
beam,5028,review,181501125,"yes, if max input is sys.maxint, then we will get 57. I increase bucket size by 1 just for safe purpose, maybe unnecessary. But I don't think one more bucket per counter will cost too many memory.",code_debt,low_quality_code
spark,17582,comment,294933906,"so we should definitely fix the /api/v1/applications/<app-id>/logs to go through the acls.  It looks like it should be protected in ApiRootResource.java. You have the app id so it needs to do something like the withSparkUI to get the acls included in that application.
Like I mentioned the listing (/api/v1/applications) and /api/v1/applications/<app-id> (which is same info I believe as listing) were intentionally left open.  I don't really see a reason to change that but if other people have a use case for it then perhaps we should make which pages are protected by acls configurable.  
on the history server I would expect spark.acls.enable=false and spark.history.ui.acls.enable=true, I can see where that could be confusing, perhaps we should document this better. spark.acls.enable on the history UI really is protecting the root UI, not the app level ui's.  We could explicitly turn this off.",code_debt,low_quality_code
accumulo,402,comment,451432262,OBE'd by cached thread pool,non_debt,-
trafficserver,3900,review,201539689,Was this supposed to be a scope resolution operator?,non_debt,-
hadoop,1772,summary,0,YARN-10036. Install yarnpkg and upgrade nodejs in Dockerfile,non_debt,-
spark,3200,review,23271303,"The partitioner comes into play during operations like `add` and `multiply`. This will later call the repartition method which does nothing if the properties match, but partitions the matrix otherwise.",non_debt,-
spark,20787,review,175982564,"Hm, this should have been caught by Scala linter because we follow Java style comment. See ""Code documentation style"" in http://spark.apache.org/contributing.html",code_debt,low_quality_code
cloudstack,2606,description,0,"Domain admins should not be able to assign the role of root admin to new users. Therefore, the role ‚Äòroot admin‚Äô (or any other of the same type) should not be visible to domain admin accounts.
I created unit test that cover all of the modified code. Moreover, I tested locally.
Testing",non_debt,-
kafka,8706,review,432145121,"As above: `stores` should never be null, and thus we don't need this change? Also the check for `isEmpty` does give us much, we can still call `addAll` even it `stores` is empty?",code_debt,low_quality_code
nifi,4099,comment,612131178,"Is there no way to smoke test anonymous authentication with this PR, as there are no available anonymous authorization providers?",non_debt,-
spark,1172,comment,46767431,Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/15998/,non_debt,-
kafka,2849,review,111517818,Could we rename the method to sth like getTransactionState?,non_debt,-
fluo,1050,comment,426470056,It seems Travis is killing jobs due to some memory issues. I'm not sure how to resolve this... but the branch builds locally just fine.,non_debt,-
beam,6306,comment,427053619,"@aaltay like I said I don't know HadoopInputFormat enough to do a good review so I left general comments. I just took a look at last modifications regarding my previous comments and they are fine. 
For a more in depth review I suggested @iemejia last time, but he seems busy. @timrobertson100 I think you know HadoopInputFormat well, can you take another look?",non_debt,-
spark,1958,comment,52810248,Correct. This patch uses a method which was introduced in that one.,non_debt,-
beam,2101,comment,282419722,Run Dataflow RunnableOnService,non_debt,-
spark,24357,comment,483576370,retest this please.,non_debt,-
trafficcontrol,1799,comment,369348511,"@limited - what types of delivery services does this relate to? You should update the Traffic Portal as well.
There are 4 html forms in TP for a delivery service. One for any_map, dns*, http* and steering*. You should add this field to each form where it applies
https://github.com/apache/incubator-trafficcontrol/tree/master/traffic_portal/app/src/common/modules/form/deliveryService
also, you'll need to add an entry for fqpacing to:
https://github.com/apache/incubator-trafficcontrol/blob/master/traffic_portal/app/src/traffic_portal_properties.json#L124",non_debt,-
skywalking,845,comment,375132498,@carlvine500 I think @ascrutae 's question is for you. Can you provide some answers?,non_debt,-
druid,9810,comment,657778727,@jihoonson @clintropolis Could you let me know your comments if there's any? Thanks!,non_debt,-
cloudstack,1495,summary,0,Add perl-modules as install dependency for cloudstack-agent,non_debt,-
spark,21121,review,183237073,Are we sure the input is always unsafe-backed array? If it is `GenericArrayData`?,non_debt,-
beam,4127,review,152140809,Fixed,non_debt,-
spark,26379,review,342354865,Got it. I missed that during check this part only. Thanks.,non_debt,-
flink,8001,review,266840147,OK,non_debt,-
zeppelin,1574,comment,257544952,@Leemoonsoo @jongyoul @felixcheung  Please help review.,non_debt,-
calcite,859,review,219699380,"oh, yes, sorry, trivial error !!!
So it should be ""c >= 0"" strictly speaking",non_debt,-
trafficcontrol,2246,comment,389553758,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/incubator-trafficcontrol-PR/1584/
Test PASSed.",non_debt,-
incubator-brooklyn,475,comment,71678232,"Motivated by the fix in https://github.com/apache/incubator-brooklyn/pull/472 by @michaeldye - tests that scenario, and various others.
@michaeldye If you get a chance to review + comment on this PR, that would be great!",non_debt,-
spark,25706,comment,531537457,#25794 for branch-2.4,non_debt,-
spark,30621,comment,739511784,Merged to master.,non_debt,-
pulsar,4120,review,278435859,@sijie Key_shared subscription is enabled by default.  so i think we need to tell users can disable it at broker.config,non_debt,-
beam,8169,description,0,"This backports #8168 and #8162.
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) <br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.",non_debt,-
tinkerpop,406,summary,0,TINKERPOP-1391 issue with where filter,non_debt,-
nifi,3260,summary,0,NIFI-5790 removed the last test as it's causing a race condition intermittently,test_debt,flaky_test
hive,785,comment,535053319,Oh I see thanks @nandorKollar I will move this there then.,non_debt,-
dubbo,1906,description,0,"#1682: Enhance the test coverage part-4 : dubbo-common/src/main/java/com/alibaba/dubbo/common/status(store|threadpoolutils) modules
dubbo-common/src/test/java/com/alibaba/dubbo/common/utils/UrlUtilsTest.java
XXXXX",test_debt,low_coverage
zeppelin,118,comment,114974261,@Leemoonsoo thanks. I will check it tomorrow. And what do you think of making a profile for yarn-pyspark?,non_debt,-
druid,4217,description,0,"Modify the default value of `druid.server.http.numThreads`  to `max(10, (Number of cores * 17) / 16 + 2) + 30`",non_debt,-
ozone,1635,comment,757432381,Merged. Thanks the review @adoroszlai and @jojochuang,non_debt,-
airflow,5010,review,272785021,Can you share how you derive `yesterday_ds` with `execution_date` with macros in the docs?,documentation_debt,outdated_documentation
kafka,6517,comment,484309340,"@hzxa21 Thanks for the code review.
1)	This consistently happens on Windows platform. I have seen few people complaining similar issues for some Linux flavors, but consensus is this is not an issue for most of the popular Linux flavors. This is a classic windows problem where if a handle is open for a file, NTFS does not allow it to be renamed / moved / deleted. All the file handles open on the file are required to be closed before renaming them. This is not much of an issue with Linux because of the way Linux file system is implemented. The way files are implemented in Linux, it allows to rename / move / delete files even if there are open handles to them.
The way I think about this patch is, before renaming any files / directories, we should always close the file channels. This is applicable to Linux as well. There are 2 fundamental scenarios. 
a)	Deletion of directories since the topic is deleted and deletion of segment files by cleaner thread. We believe, we should close the log file in case of log deletion and close the segment in case of segment file cleanup. Since no one should be reading these files, it is safe to close.
b)	 During swapping flow, where ‚Äò.cleaned‚Äô files are renamed to ‚Äò.swap‚Äô files and then they are renamed to actual log files. Here also we believe segment should be closed during renaming and re-opened after the swap.
Overall gist is, files should be closed before renaming them and should be opened back if required. This should be applicable to Linux as well
2)	Yes, initially we implemented the change in rename files method. But that change turned out to be 
ugly. Here is the abandoned pull request done such way.
https://github.com/Microsoft/kafka/pull/17/files
Problem was, there was no need to re-open ‚Äò.deleted‚Äô files. So we had put an if check to not to re-open ‚Äò.deleted‚Äô file. Also, it resulted in changes at many places.
Also, I felt like it was not a cleaner fix where we are not solving the root problem. Which is to close the segment before they are renamed / deleted. Also, we wanted this to be a generic fix, rather than specific to Windows.
3)	Yes, I can add some test cases. We wanted to get a initial feedback on the idea before adding tests.",code_debt,low_quality_code
spark,28028,comment,604186979,"ping @cloud-fan @jiangxb1987 @xuanyuanking Please take a look, thanks!",non_debt,-
geode,1086,review,152657597,Could this be renamed to GatewayReceiverXmlParsingValidationsJUnitTest.java,non_debt,-
spark,26973,review,365779363,actually we need `Option[IntervalRow]`?,non_debt,-
hudi,2328,review,548085273,"2 cents. feel free to take a call. Should we explicitly check only if spark version  is 2 or 3 we will proceed, if not, will throw an exception. just in case, in future, when we have spark 4, we don't need to make any code fixes.",non_debt,-
spark,7348,review,34648784,that happens during CheckAnalysis when we report errors.,non_debt,-
flink,9795,comment,536207542,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit 1055410a5888dcada8045f57f5100dc09f095db9 (Wed Oct 16 08:32:58 UTC 2019)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* ‚ùì 1. The [description] looks good.
* ‚ùì 2. There is [consensus] that the contribution should go into to Flink.
* ‚ùì 3. Needs [attention] from.
* ‚ùì 4. The change fits into the overall [architecture].
* ‚ùì 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier",non_debt,-
guacamole-client,168,comment,311453669,"On Jun 27, 2017 11:59, ""necouchman"" <notifications@github.com> wrote:
Okay, merged it to staging, now just need to merge staging to master. Is
this going to require a force?
No - never ever ever force push to the main repos.
Or create duplicate entries?
Yep.",non_debt,-
dubbo,1327,comment,362996179,4710920-1327 comment-362996179,non_debt,-
incubator-pinot,4774,review,345466343,"Which part does not seem right? For single-value string-type virtual columns, the default null values are specified during initialization. e.g. for $segmentName, getValue should return the name of the segment. Previously in SegmentNameVirtualColumnProvider, getValue returns context.getSegmentName(). Now the segment name is initialized in addBuiltInVirtualColumnsToSegmentSchema for each segment and stored in the segment schema, so fieldSpec.getDefaultNullValue should still return the sgment name.",non_debt,-
hudi,1678,review,439871892,"We should clarify somewhere that bootstrap file scheme and the real base path cannot be different. We can bootstrap fine but while querying don‚Äôt we need them to be the same hdfs, hdfs or s3,s3",non_debt,-
incubator-pinot,4664,review,330708381,"This filesystem should be same as where we want to store the segment tars.
If we use s3a path as segment tar dir. Then this should be s3a filesystem.",non_debt,-
flink,2916,review,90420632,I will check the shard iterator type in the new method by merging these two 'getShardIterator' methods.,non_debt,-
thrift,2241,review,495058967,"Missing Hyphen after `#service`
`#service` > `#service-`",non_debt,-
druid,4550,review,143922833,Sounds good. I'll do some benchmark in a follow-up pr.,non_debt,-
flink,9689,comment,539299262,"@HuangZhenQiu thanks for your effort on this. And @bowenli86 the syntax is consistent with FLIP-69.
@walterddr  I think ddl syntax proposed in FLIP-69 is less possible to be challenged, while the changed API to TableEnvironment to support returned ddl eg `show tables` maybe need to discuss further. We are discussing in our inner team and plan to complete this flip before 1.10 release. I estimate there maybe two weeks or longer before the flip ready to vote.",non_debt,-
openwhisk,4531,comment,507185550,"Now server starts fine on Jenkins
However one test fails due  to older docker version
@houshengbo Whats the docker version on Jenkins machine? Per [docker docs](https://docs.docker.com/engine/reference/commandline/logs/#options) support for `--until` flag was part of engine api v1.35. So we may need to update the Docker version on Jenkins",non_debt,-
beam,1849,comment,275319429,Coverage increased (+0.04%) to 69.79% when pulling **b0b91c842e09aa7fdb5c1dc216574daa43b437ea on dhalperi:dataflow-runner-speedup-2** into **1c6e667414788fe99f583fac39d458a4984ae162 on apache:master**.,non_debt,-
carbondata,3045,comment,451909156,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2194/",non_debt,-
flink,1153,review,41024851,Maybe we could harmonize the null checking as you've done it.,non_debt,-
lucene-solr,1251,review,377712392,I'll put this lf in,non_debt,-
camel,2387,comment,400360563,"With the change to check the boolean, I still get the following 3 tests in error. Illegal State error occurs when the test tries to start the aggregationProcessor again after a stop
[ERROR] org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest.testAggregateProcessorTimeoutExpressionRestart(org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest)
[ERROR]   Run 1: AggregateProcessorTimeoutCompletionRestartTest>TestSupport.runBare:58->testAggregateProcessorTimeoutExpressionRestart:132 ¬ª IllegalState
[ERROR] org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest.testAggregateProcessorTimeoutRestart(org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest)
[ERROR]   Run 1: AggregateProcessorTimeoutCompletionRestartTest>TestSupport.runBare:58->testAggregateProcessorTimeoutRestart:87 ¬ª IllegalState
[ERROR] org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest.testAggregateProcessorTwoTimeoutExpressionRestart(org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest)
[ERROR]   Run 1: AggregateProcessorTimeoutCompletionRestartTest>TestSupport.runBare:58->testAggregateProcessorTwoTimeoutExpressionRestart:189 ¬ª IllegalState",non_debt,-
spark,7402,review,34640556,"Do we need this? If we are in interpretation mode, we might as well not do anything Unsafe.",non_debt,-
ozone,1573,description,0,"Currently, all background deletion services use the same intervals for deletion. It should ideally use different configs and the default value can be the same for all the configs.
https://issues.apache.org/jira/browse/HDDS-4367
Tested Manually",non_debt,-
nifi,3110,review,228428297,Good point. Instantiating in every iteration is wasteful. I'll fix it!,non_debt,-
hbase,2584,review,515768061,Where do we call this method?,non_debt,-
storm,2433,review,152703836,Let's elaborate the part of comment : `if is a kill`. I don't get it.,non_debt,-
ignite,3921,description,0,‚Ä¶tions,non_debt,-
spark,21535,comment,411371364,LGTM too,non_debt,-
netbeans,2359,review,486307632,"Compatible, Services Lookup is recursive.",non_debt,-
spark,12790,review,75229100,Sure,non_debt,-
cloudstack,1139,summary,0,"CLOUDSTACK-9092: L10n fix in ""Add LDAP Account page""",non_debt,-
tvm,5483,review,418378401,"If `v` has been used, then it will be `v`. Otherwise, it will be `v_1`.",non_debt,-
myfaces-tobago,334,description,0,"Bumps [maven-compiler-plugin](https://github.com/apache/maven-compiler-plugin) from 3.8.0 to 3.8.1.
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually",non_debt,-
flink,10633,summary,0,[FLINK-14683] Fix RemoteStreamEnvironment's constructor,non_debt,-
attic-apex-core,512,comment,294055210,"I asked on #508 that several references are changed to 2.x. Also, review comments are to be addressed on the same PR, not by opening another one.",non_debt,-
camel,594,description,0,206317-594 description-0,non_debt,-
zeppelin,3883,summary,0,[ZEPPELIN-4987]. Fix the interpreter dependency conflict caused by the CLASSPATH environment variable,non_debt,-
kafka,1753,review,78307057,"The subscription to ZK change notification path was made in ZkNodeChangeNotificationListener. So, in order not to miss any changes, we should apply the bootstrap logic after ZkNodeChangeNotificationListener is initialized.",non_debt,-
reef,300,comment,122118613,"[Reef-pull-request-ubuntu #109](https://builds.apache.org/job/Reef-pull-request-ubuntu/109/) SUCCESS
This pull request looks good",non_debt,-
spark,27770,review,386955069,"SPARK-13450, commit ID: 02c274eaba0a8e7611226e0d4e93d3c36253f4ce#diff-9a6b543db706f1a90f790783d6930a13",non_debt,-
avro,60,description,0,206459-60 description-0,non_debt,-
ignite,8568,review,547207662,Renamed,non_debt,-
airflow,5178,comment,486870974,LGTM,non_debt,-
airflow,13461,comment,756467767,The community doesn't have any technical writer. We rely only on the contributions of other people and they are mostly developers.,non_debt,-
zeppelin,2495,review,130212409,"I'm not if it is though. in your link, there is no 'SYNONYM'",non_debt,-
thrift,1223,description,0,310611-1223 description-0,non_debt,-
incubator-mxnet,14145,summary,0,v1.3.x - backports nightly test fix,non_debt,-
kafka,9007,summary,0,KAFKA-10120: Deprecate DescribeLogDirsResult.all() and .values(),non_debt,-
trafodion,704,comment,247092715,Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/1138/,non_debt,-
camel,2088,summary,0,CAMEL-11954: Renamed ha packages to cluster,non_debt,-
hive,663,review,291371794,"The constraints are not deep copied though, you are right. Those should not change in any case; I will change the comment accordingly.",non_debt,-
spark,26750,description,0,"Thanks for sending a pull request!  Here are some tips for you:
-->
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
-->
The `TableProvider` only accepts table schema and properties. It should accept table partitioning as well.
This is extracted from https://github.com/apache/spark/pull/25651, to only keep the API changes and make the diff smaller.
-->
Although `DataFrameReader`/`DataStreamReader` don't support user-specified partitioning, we need to pass the table partitioning when getting tables from `TableProvider` if we store tables in Hive metastore with v2 provider.
-->
not yet.
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests",non_debt,-
spark,17090,comment,282962487,the same as  https://github.com/apache/spark/pull/12574 ?,non_debt,-
gobblin,1066,review,71230879,Ditto.,non_debt,-
tvm,325,comment,322545894,"This is the problem of upstream, it shall be fixed by https://github.com/dmlc/tvm/pull/332",non_debt,-
spark,29810,comment,695416683,ok to test,non_debt,-
flink,2063,comment,224066428,"looking at the code right now; i may have figured out why the files aren't copied, but i find it odd that it supposedly works with hdfs. it actually should never copy additional files if no parameters are given.",non_debt,-
spark,25002,review,299768415,@advancedxy . We need a test case which failed without this PR. Please add a test case to show the validity of your PR.,non_debt,-
iceberg,2163,comment,770747783,@pvary @lcspinter could you please review when you get the chance? Thanks!,non_debt,-
incubator-brooklyn,820,comment,132144643,I've rebased this PR again. Can someone test and merge it before new conflicts arise?,non_debt,-
incubator-brooklyn,272,review,19438455,Why do we need this line? Have called `cluster.addMemberChild(spec)` to create `node1` originally.,non_debt,-
flink,7306,summary,0,[hotfix][docs][udfs.md]fix the example of user-defined function in udfs.md,non_debt,-
flink,145,comment,58411645,"Thanks for doing this.
+1",non_debt,-
superset,3222,summary,0,[sqllab/cosmetics] add margin-top for labels in query history,non_debt,-
trafodion,845,comment,262056810,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1380/,non_debt,-
jena,568,comment,497097832,"Yes, @neumarcx, I just wanted to establish the exact code you are using from Commons Math. What I linked is what you are importing in the current state of this PR.",non_debt,-
hbase,1726,comment,629892413,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
spark,19793,review,153080692,we might not be able to set this here - we need to have it working with multiple versions of scala,non_debt,-
druid,279,comment,27363341,"High level comment because I have not yet had time to look at this, but I think this pull request should directly be committed into Druid 0.6 to save the time to port things over.",non_debt,-
geode,5064,description,0,"Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.",non_debt,-
helix,219,summary,0,"[HELIX-717] Add api for get / set quota type, ratio and participant capacity",non_debt,-
druid,4815,review,146068538,Can you add some tests around early publishing and loading sequence data from disk?,test_debt,lack_of_tests
geode,1855,description,0,"Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.",non_debt,-
pulsar,9215,comment,761581957,/pulsarbot run-failure-checks,non_debt,-
calcite,2110,comment,675242513,"Oops, i didn't saw that ~",non_debt,-
nifi-minifi-cpp,62,comment,285357651,reviewing,non_debt,-
cloudstack,3306,comment,490885118,@ustcweizhou we will pick up your change ne try to fix the sort of the project list with jQuery.,non_debt,-
nifi,3843,review,363330803,Update to 1.11.0-SNAPSHOT,non_debt,-
pulsar,9520,comment,774674468,"I found there's an existed Python test for multiple topics, I'll fix the test.",non_debt,-
spark,20025,comment,359982527,@gatorsmile @liufengdb Anything else? Thanks!,non_debt,-
ignite,1909,description,0,31006158-1909 description-0,non_debt,-
spark,5992,description,0,17165658-5992 description-0,non_debt,-
commons-lang,625,description,0,"Bumps [junit-pioneer](https://github.com/junit-pioneer/junit-pioneer) from 0.9.2 to 1.0.0.
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)",non_debt,-
spark,3157,comment,67773705,"The `NoSuchMethodError` was caused by wrong datanucleus-core version. The target method is defined in datanucleus-core 3.2.2 but not in 3.2.10. The former is used when compiling against Hive 0.12.0, while the latter is used when compiling against Hive 0.13.1. Currently on Jenkins we first do a clean compile against 0.12.0, then build the assembly jar _without_ clean against 0.13.1 and run the tests.
This behavior left both versions of datanucleus-core in the `lib_managed` directory, and may sometimes mess up class paths. I'll open a PR to clean `lib_managed` before compiling Hive 0.13.1 in `dev/run-tests`.
I've seen this error once several days before right after the most recent Jenkins upgrade. Not sure why this issue wasn't detected before.",non_debt,-
beam,11500,comment,618010564,Run Dataflow PostRelease,non_debt,-
druid,8857,comment,553427929,@jihoonson let me know if you agree with the proposed wording and general page structure.,non_debt,-
calcite,1143,comment,479400812,"hi @zhztheplayer @michaelmior  I don't know if all the considerations are incomplete. I hope you can review and give me some Suggestions. Thank you very much.
best
qianjin",non_debt,-
superset,4791,review,180883434,should add payload => `action.payload.key`,non_debt,-
airflow,5661,review,307279581,"Enter is required. Otherwise, documentation is not rendered correctly.",documentation_debt,low_quality_documentation
spark,1689,comment,55458226,"@erikerlandson  thanks for looking at this.
A few questions:
1. After this pull request, does anything still use SimpleFutureAction?
3. This is not always lazy still right? See a test case",non_debt,-
incubator-mxnet,11320,comment,397914927,"I ran the failed test thousands of times to collect a few failures. I checked the core dumps. It seems it always fails exactly in the same place. Please check my comment in https://github.com/apache/incubator-mxnet/issues/11171. It's unclear why there is such a memory error. Since it always fails in the same operator, it's less likely that the error was introduced by this PR.",non_debt,-
spark,31638,review,586286229,"Let's focus on ""glob path"" here - from the quick look on org.apache.hadoop.fs.FileSystem javadoc, both `isDirectory()` and `exists()` seem to require the exact path, not glob path. The method which accept glob path is `globStatus()`, and there the API clearly names the parameter as `pathPattern`.
https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html
And intuitively, it sounds very odd to me if someone can say the glob path is a directory while matching paths can be both files and directories, and same for checking existence as well. I don't think it's feasible to ""expect"" the meaningful value from calling these methods with glob path. That sounds to me as ""undefined behavior"" even any weird file system could return true for the case.
That said, I'd rather consider the input of glob path as ""wrong one"" and always return false (some sort of debug log message is fine if we really like to log). If there's a code relying on such behavior, I don't think that is correct. I'd rather say the possible paths should be populated before, and this method should be called per each path.
I still need to hear voices from others, but if the consensus goes to just disallow the glob path here, we won't need to introduce the new configuration.
@tdas @zsxwing @jose-torres @viirya @gaborgsomogyi Would like to hear your voices on this. Thanks!",non_debt,-
spark,19675,comment,372560208,Does that mean no chance of 2.12 support for ~6 months?,non_debt,-
airflow,2460,comment,435672834,"Hello
I've rebased the branch to current master and modified the CLI to use the new property so that everything is clean.
LGTM now, tested OK",non_debt,-
kafka,815,comment,179419066,"@hachikuji Ok, that sounds like we might have a bug in the packaging scripts in connect-hdfs and connect-jdbc then, although the development and packaged versions differ. Probably worth reviewing.
Based on this update and that we confirmed system tests are passing, LGTM.",non_debt,-
camel-quarkus,1149,summary,0,Upgrade Activemq to version 5.11.12,non_debt,-
qpid-dispatch,1047,description,0,"* Do not write new buffers if connection is CLOSED_WRITE
* Do not call connection_wake if CLOSED_READ or CLOSED_WRITE
This fixes crashes but there is still work left with leaking messages and buffers when server connections close before client connections.",design_debt,non-optimal_design
incubator-pinot,4253,review,290839617,"interesting, so each reducer no longer just generates one file, how do we determine that?",non_debt,-
superset,5664,comment,420773552,Yeah I definitely understand we probably don't want to introduce new functionality at the moment. I'll close this for now. @xtinec let me know if you want to chat about how it works if you decide to integrate cascading filter options!,non_debt,-
kafka,10121,description,0,"As discussed in the mailing list thread, this PR removes the AdminClient changes pertaining to `deleteTopicsWithIds` and `DeleteTopicsWithIdsResult`",non_debt,-
flink,12283,review,431556916,Does this mean only bulk formats can work with checkpoints?,non_debt,-
tvm,7612,review,589839657,70746484-7612 review-589839657,non_debt,-
spark,1241,comment,53686699,"Jenkins, test this please",non_debt,-
incubator-mxnet,12157,review,212681251,"If `prop_name` doesn't exist, we need to handle the failure of creating a subgraph property.",non_debt,-
arrow,8755,review,532731697,This also needs to check that the type returned by `read(...)` is no longer bytes?,non_debt,-
tomee,713,comment,734780620,"I gave it a try at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 but I am quite unsure if this is what we were talking about. Consequently, I dropped some FIXME/TODO questions.
For this reason, I didn't open a related (new) PR yet. Maybe you can have a look at it?",requirement_debt,requirement_partially_implemented
carbondata,500,description,0,Add CI building status and Apaches license icon,non_debt,-
couchdb,1894,comment,459648857,Sure. I'll take a look next week.,non_debt,-
beam,3124,comment,314234150,I'm closing this as to-be-preempted.,non_debt,-
cloudstack,1578,comment,238838070,"Marvin test code PEP8 & PyFlakes compliance:
CloudStack$
CloudStack$ pep8 test/integration/plugins/nuagevsp/_.py
CloudStack$
CloudStack$ pyflakes test/integration/plugins/nuagevsp/_.py
CloudStack$",non_debt,-
tvm,5365,description,0,"In #5288, we moved Inline after the second fusion to avoid fusion checks the OpPattern of certain ops that can be eliminated by simplify inference pass, i.e. batch_norm, as external codegen needs to keep them. However, as memory passes should happen as a whole, this may introduce bugs. For example, we may have to bind constants first, like the change in the unit test. This fix reverts the change, but stopping fusing the function that should be handled by external codegen in fuse_op. 
This indicates that we may need to think about a more systematic way to skip functions inside a pass. Although outlining and inlining was used for the functions that are control by pass manager, it is not sufficient in such case.
@vegaluisjose @jroesch @comaniac @tqchen",non_debt,-
flink,14070,comment,726970610,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit cbf96e7bb990402b6e591d1f829d55d1e3b2cd01 (Fri Nov 13 18:55:08 UTC 2020)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* ‚ùì 1. The [description] looks good.
* ‚ùì 2. There is [consensus] that the contribution should go into to Flink.
* ‚ùì 3. Needs [attention] from.
* ‚ùì 4. The change fits into the overall [architecture].
* ‚ùì 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier",non_debt,-
flink,6236,review,241972720,Format the code to be consistent with old functions,code_debt,low_quality_code
incubator-pinot,103,description,0,"sample config insideÔºö
  ""tableIndexConfig"" : {
    ""invertedIndexColumns"" : [],
    ""loadMode""  : ""MMAP"",
    ""lazyLoad""  : ""false"",
    ""sortedColumn"" : [""secondsSinceEpoch""],
    ""starTreeIndexSpecConfigs"" : {
      ""enableStarTree"": ""true"",
      ""maxLeafRecords"": ""50000"",
      ""skipStarNodeCreationForDimensions"": ""uuid"",
      ""skipMaterializationCardinalityThreshold"": ""10000""
    },
    ""streamConfigs"" : {
      ""stream.kafka.consumer.type"": ""highLevel"",
      ""stream.kafka.decoder.class.name"": ""com.linkedin.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder"",
      ""stream.kafka.hlc.zk.connect.string"": ""localhost:2181/kafka"",
      ""stream.kafka.topic.name"": ""pinot-test"",
      ""stream.kafka.consumer.prop.auto.offset.reset"":""smallest"",
      ""streamType"": ""kafka"",
      ""realtime.segment.flush.threshold.size"":""500000"",
      ""realtime.segment.flush.threshold.time"":""18000000""
    }
  },",non_debt,-
shardingsphere,3692,review,355335872,Can you replace this class by `ShardingScalingJob`?,non_debt,-
kafka,1884,review,95041682,Handling the IO Exception in InternalTopicManager.close() now.,non_debt,-
reef,1117,description,0,"Moving WaitingForRegistration from constructor to Call method in tasks
Add Cancellation token to WaitingForRegistration method
Enable test case TestFailedMapperOnLocalRuntime to trigger the cancelation scenario in WaitingForRegistration
JIRA: [REEF-1549](https://issues.apache.org/jira/browse/REEF-1549)
This closes #",non_debt,-
dubbo,2822,review,235845838,"1. DEFAULT_FAIL_RETRY_SIZE maybe changed to DEFAULT_FAILBACK_TIMES
2. 100 is also too large. maybe 3 is enough.",non_debt,-
calcite,2369,review,593096896,"Makes sense. I removed these assertions completely, as they are unrelated to the fix. Also added comments to the tests.",code_debt,low_quality_code
ignite,7450,summary,0,IGNITE-12701 : Disallow silent deactivation in CLI and REST.,non_debt,-
pulsar,9799,review,589346299,"The URL was contained in this doc.
The brokers method of the {@inject: javadoc:PulsarAdmin:/admin/org/apache/pulsar/client/admin/PulsarAdmin.html} object in the Java API",non_debt,-
trafficserver,369,comment,164002946,Updated with url_len removed.,non_debt,-
accumulo,675,summary,0,Fix script to build native libs,non_debt,-
fineract,1458,comment,718052221,"I just feel increasing timeouts won't cut it. Doesn't sound like a real solution. Unfortunately, I don't understand much about schedular jobs to have a proper say in this but I think we have some performance issues. Need to figure out how to deal with that.",design_debt,non-optimal_design
reef,1402,description,0,"Summary of changes:
   * Make `ProtocolSerializer` constructor injectable
   * Create named parameters for the constructor's input
   * Fix the `ProtocolSerializerTest` unit tests to use injection
   * Bug fixed: register the `Header` class regardless of the message namespace parameter
   * Minor improvements and refactoring
**P.S.** We'll need to merge those changes back into [REEF-1763](https://issues.apache.org/jira/browse/REEF-1763) at some point
JIRA: [REEF-1936](https://issues.apache.org/jira/browse/REEF-1936)",non_debt,-
incubator-pinot,857,description,0,19961085-857 description-0,non_debt,-
incubator-mxnet,10696,review,195640524,This is formatted message which worker node will send to coordinator to synchronize ndarray allreduce order across all nodes.,non_debt,-
druid,3889,summary,0,Monomorphic processing of TopN queries with 1 and 2 aggregators (key part of #3798),non_debt,-
pulsar,2613,description,0,"Handle heartbeat function if owner-worker is not available. 
Function scheduling handles heartbeat function if owner-worker is not available.",non_debt,-
spark,22944,comment,437336856,ping @cloud-fan,non_debt,-
tvm,7537,review,597102389,"@hogepodge is working on a fairly extensive refactor of the TVM tutorials--I think we should ensure there is a place for stuff like this. it would still be helpful to see your tutorial or perhaps the python scripts you're using with this code! perhaps there are other pieces we are missing or should checkin, or perhaps there is a particular way we should structure our docs to make this use case more straightforward.",non_debt,-
spark,9247,comment,150918873,"Hi @jliwork , thanks for working on it!
But sorting on array of null type doesn't make sense to me, can you check the behaviour of other SQL system like Hive? And how about struct type? It's also order-able.",non_debt,-
druid,9422,review,448046112,@clintropolis - can you provide me a snippet of what needs to be done. I don't see a method available in `NilVectorSelector` that would taking in `ReadableVectorOffset`,non_debt,-
beam,3833,review,139338762,Done.,non_debt,-
spark,25367,description,0,"In the PR, I propose to use existing expressions `DayOfYear`, `WeekDay` and `DayOfWeek`, and support additional parameters of `extract()` for feature parity with PostgreSQL (https://www.postgresql.org/docs/11/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT):
1. `dow` - the day of the week as Sunday (0) to Saturday (6)
2. `isodow` - the day of the week as Monday (1) to Sunday (7)
3. `doy` - the day of the year (1 - 365/366)
Here are examples:
Updated `extract.sql`.",non_debt,-
openwhisk,4921,review,447727733,52039373-4921 review-447727733,non_debt,-
spark,15059,comment,246957960,"@srowen I've reverted the previous change and inlined the relative tolerance logic, thank you!",non_debt,-
camel-k,930,comment,529910322,@dobozysaurus you can just force push your branch and keep this PR open if you want.,non_debt,-
superset,5753,summary,0,[SIP-6] Refactor WordCloud,non_debt,-
helix,1447,comment,704707243,"minor: suggest to change the PR (commit) title to ""Replace Thread.sleep() with TestHelper.verify() to fix the flaky unit tests"".",test_debt,flaky_test
spark,3144,comment,62077677,LGTM,non_debt,-
drill,1988,review,384897019,Oh my. Good catch.,non_debt,-
skywalking,6085,description,0,"    ‚ö†Ô∏è Please make sure to read this template first, pull requests that don't accord with this template
    maybe closed without notice.
    Texts surrounded by `<` and `>` are meant to be replaced by you, e.g. <framework name>, <issue number>.
    Put an `x` in the `[ ]` to mark the item as CHECKED. `[x]`
-->
     ==== üêõ Remove this line WHEN AND ONLY WHEN you're fixing a bug, follow the checklist üëÜ ==== -->
     ==== üîå Remove this line WHEN AND ONLY WHEN you're adding a new plugin, follow the checklist üëÜ ==== -->
     ==== üìà Remove this line WHEN AND ONLY WHEN you're improving the performance, follow the checklist üëÜ ==== -->
     ==== üÜï Remove this line WHEN AND ONLY WHEN you're adding a new feature, follow the checklist üëÜ ==== -->",non_debt,-
kafka,6122,comment,469241170,Looks like there are conflicts. Are we still trying to merge this to 2.0?,non_debt,-
cloudstack,1659,comment,255777805,"Environment: kvm-centos7 (x2), Advanced Networking with Mgmt server 7
Total time taken: 24863 seconds
Marvin logs: https://github.com/blueorangutan/acs-prs/releases/download/trillian/pr1659-t168-kvm-centos7.zip
Test completed. 45 look ok, 3 have error(s)",non_debt,-
hbase,3001,review,594754063,s/give/given/,non_debt,-
spark,16989,review,115884613,"Yes, I should refine this :-)",non_debt,-
shardingsphere,3158,review,329463176,"We may not add a class only for convenience, may need more design or inline into original class",design_debt,non-optimal_design
carbondata,613,review,103185123,Good catch. I'm fixing.,non_debt,-
flink,11536,comment,604555169,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit 02744178a91af131c0be0337a18e7cc65303495d (Thu Mar 26 17:11:23 UTC 2020)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* ‚ùì 1. The [description] looks good.
* ‚ùì 2. There is [consensus] that the contribution should go into to Flink.
* ‚ùì 3. Needs [attention] from.
* ‚ùì 4. The change fits into the overall [architecture].
* ‚ùì 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier",non_debt,-
openwhisk,3507,comment,396270670,LGTM üëç,non_debt,-
kafka,5101,review,213142665,It seems that the last one is enough?,non_debt,-
druid,4388,review,121026129,"I don't think you need the `@JsonProperty` annotations here since only the main constructor is `@JsonCreator`, this legacy constructor would only be used when directly instantiating",non_debt,-
drill,819,review,113834302,Seems to be no test for the Decimal precision mapping. That may be why the issue mentioned in the code slipped through: there is no unit test to catch it...,test_debt,lack_of_tests
kafka,2938,review,116911699,Done,non_debt,-
spark,10993,review,51332430,"AFAICT, this is never used, so I removed it.",code_debt,dead_code
kafka,5634,comment,420895699,I am going to go ahead and merge. The build failures are due to a test regression in streams scala.,non_debt,-
bookkeeper,2355,comment,703966200,"Messed up the PR, closing this and reopening under new PR.",non_debt,-
attic-apex-core,194,review,50607181,"@davidyan74 : Do we need to care about following scenario
Operator C has two upstream A and B. Current latency of A > B and based on that C adds A as it's slowest upstream but then B's stats come and now latency of B > A but C is still pointing A as it's slowest upstream...",non_debt,-
netbeans,1312,comment,517993362,"With a fresh install of NB11.1 and enabling all Java + JavaEE features, where the Amazon Beanstalk entry is missing is:
New Project -> Java with Maven -> Java Web -> Server & Settings -> JavaEE7 -> Add
Amazon Beanstalk should be in the list of Server types to add to the Project, but isn't anymore (Only Tomcat, Glassfish & Payara).  
I remember seeing something on the NB Mailing list about duplicate entries in this dialog after Payara was merged in, so suspect it may be related to this (rather than JavaEE8), but without tracing the related commits/PRs and going through the code I can't be certain at this point.
I will have a look myself if I get time.",non_debt,-
druid,4634,review,132473645,changed,non_debt,-
madlib,29,comment,195613825,@fmcquillan99 New commit should return the original symbols,non_debt,-
druid,6094,review,221371033,"It's a pretty standard way in Druid of differentiating realtime and non-realtime servers. See CoordinatorBasedSegmentHandoffNotifier, DruidSchema, and CachingClusteredClient, all of which use this method to determine if segments are served by realtime servers or not. Maybe we could make this clearer by adding a new ""isRealtimeServer"" method.",non_debt,-
camel-quarkus,49,comment,509595442,This was just an attempt. With the standard ASF infra approach we may need to use a personal access token of a personal account to be able to scan the github repo and check for PR. So it's not really feasible. I'm testing a different approach and probably we may remove the jenkinsfile later.,non_debt,-
incubator-heron,2496,comment,340094782,LGTM.,non_debt,-
beam,9509,summary,0,[BEAM-8156] Add convert_to_typing_type,non_debt,-
incubator-mxnet,15820,review,312694681,Fixed. Thx.,non_debt,-
spark,19616,review,241883955,added thread name,non_debt,-
kafka,7045,review,309475036,Nicely done - I like how you validate the objects at the start.,non_debt,-
nifi-minifi-cpp,707,description,0,"Implemented in terms of std::runtime_error
Thank you for submitting a contribution to Apache NiFi - MiNiFi C++.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
spark,19267,comment,337257920,"just reading through your description here all the yarn pieces aren't in place so you have an admin type command to signal spark that a node is being decommissioned. But that means someone has to run that command on every single spark application running on that cluster, correct?  That doesn't seem very feasible on any relatively large cluster.  One big question I have is, are there enough use cases where just the command is useful?  Otherwise we are temporarily adding a command that we will have to keep supporting forever (or perhaps only the next major release).    Or does it make sense to wait for YARN (or another resource manager) to have full support for this?",non_debt,-
druid,1862,review,43036861,missing `|` at the beginning of the line,non_debt,-
flink,14839,review,586411292,Why `rethrow` as `RuntimeException`?,code_debt,low_quality_code
nifi,3536,review,300057978,Is this commented out code no longer needed?,code_debt,dead_code
cloudstack,768,review,39469736,Why are deprecated methods being used?,code_debt,low_quality_code
kafka,9628,review,535829042,"`TestUtils.alterClientQuotas` called before `adminClient.close` can throw an exception. Admin extends AutoCloseable, so you can use try-with-resources.",non_debt,-
flink,10146,comment,552447888,"Rebased for conflict resolving.
Travis passed: https://travis-ci.org/xintongsong/flink/builds/610258110",non_debt,-
drill,1251,review,187210052,I just removed this detail since we are saying these libraries are deprecated and we no longer want to use them.,code_debt,dead_code
spark,16728,description,0,"This PR is to revert the changes made in https://github.com/apache/spark/pull/16700. It could cause the data loss after partition rename, because we have a bug in the file renaming. 
Not all the OSs have the same behaviors. For example, on mac OS, if we renaming a path from `.../tbl/a=5/b=6` to `.../tbl/A=5/B=6`. The result is `.../tbl/a=5/B=6`. The expected result is `.../tbl/A=5/B=6`. Thus, renaming on mac OS is not recursive. However, the systems used in Jenkin does not have such an issue. Although this PR is not the root cause, it exposes an existing issue on the code `tablePath.getFileSystem(hadoopConf).rename(wrongPath, rightPath)`
--- 
Hive metastore is not case preserving and keep partition columns with lower case names.
If SparkSQL create a table with upper-case partion name use HiveExternalCatalog, when we rename partition, it first call the HiveClient to renamePartition, which will create a new lower case partition path, then SparkSql rename the lower case path to the upper-case.
while if the renamed partition contains more than one depth partition ,e.g. A=1/B=2, hive renamePartition change to a=1/b=2, then SparkSql rename it to A=1/B=2, but the a=1 still exists in the filesystem, we should also delete it.
N/A",non_debt,-
incubator-mxnet,8279,comment,336701383,"@piiswrong , thanks. Yes, the use case is to manage a HybridBlock with changing graph topology. The idea is to provide an easy interface so that the user does not has to keep track of a HybridBlock for each graph topology but instead manage the graph topologies internally/within the mxnet framework code.
Instead of modifying the definition of HybridBlock, what about introducing a separate class that provides this functionality:",non_debt,-
arrow,8961,summary,0,ARROW-10885: [Rust][DataFusion] Optimize hash join build vs probe order based on number of rows,non_debt,-
flink,13475,review,494810110,Of course. I have created a JIRA https://issues.apache.org/jira/browse/FLINK-19412 to track it.,non_debt,-
spark,16152,description,0,"Our existing withColumn for adding metadata can simply use the existing public withColumn API.
The existing test cases cover it.",code_debt,low_quality_code
cloudstack,2427,comment,360067539,"Yes, well that bit of code does dnsmasq's job. I think we should merge this and then find a way to call a release on a single lease by mac or ip.
LGTM ^^",non_debt,-
skywalking,4740,comment,623178580,"# [Codecov](https://codecov.io/gh/apache/skywalking/pull/4740?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/4740?src=pr&el=continue).",non_debt,-
superset,3533,review,142584977,"Let's have `S3` in there as ""bucket"" is unclear, as in ""The S3 bucket...""",non_debt,-
arrow,9314,comment,766853907,Looking clean and green üíö,non_debt,-
cloudstack,2231,comment,336177773,@rhtyd Done!,non_debt,-
trafficserver,3044,description,0,"Hello,
As far as I can see, and understand, the option `--enable-docs` doesn't work as expected in 7.1.2.
In `configure.ac`:
`enable_doc_build` is set to `no` if option is missing. If option is provided, nothing is done.
Then, `BUILD_DOCS` is set to true only if `enable_doc_build` is set to `yes` which seems to be unlikely to happen.
In `configure`:
If `--enable-docs` option isn't provided, `enable_doc_build` is set to `no` and never set to `yes` if option is provided.
When testing `enable_doc_build` to set `BUILD_DOCS_TRUE` and `BUILD_DOCS_FALSE`, first case is never triggered, leading to `enable_doc_build` to be always empty.
This means BUILD_DOCS_TRUE will be always equal to '#', thus leading man pages generation to be disabled in `doc/Makefile.in`:",non_debt,-
kafka,8836,comment,643961078,@chia7712 @guozhangwang I updated the code which using KafkaConsumer.metrics() to get rebalance time.  Could you please review it?,non_debt,-
nifi,1719,review,114141863,"It forces cluster scope, which still works locally if there is no cluster state management.  Is there a different way that you'd like this to be addressed?  Take a look at: https://github.com/apache/nifi/pull/1636#discussion_r109812402",non_debt,-
spark,4690,review,25098634,"I think we can back that out if there's any question to keep this limited. @JoshRosen Yes that's where we ended up again, now that it's clear that there are several ways and several places this can happen. It's easiest just to ignore the exception.",code_debt,low_quality_code
spark,8246,comment,132291972,"This sounds great, thanks!  I'll need to finish up with QA for 1.5 before taking a look, but please ping me if I don't return to review before long.",non_debt,-
gobblin,2327,review,180183357,Remove since the same as the default implementation.,non_debt,-
spark,18607,comment,319839403,@viira can you review for me again please,non_debt,-
accumulo,654,comment,423239004,the relevant change is going from `FileSystem.get(conf)` to `FileSystem.getLocal(conf)`; the formatting changes were done by maven.,non_debt,-
infrastructure-puppet,1499,description,0,Add graphviz to all jenkins nodes. INFRA-17902,non_debt,-
trafodion,355,comment,191494859,"Actually, at second thought: Could we separate the mk.sh and cp commands into two lines and just ignore the exit code of cp? It would be good to catch checkins that break the GUI build. We had that a while ago and had a hard time getting it back to work when we didn't notice the problem immediately.",non_debt,-
spark,6986,review,35582006,"also update the scaladoc to give an example, and explain what the acceptable values are for dayOfWeek",non_debt,-
kafka,5735,comment,427204839,"I was assuming we'd just merge this to trunk since we tend not to do new test development on the older branches.  I don't have a strong opinion, though.",non_debt,-
nifi,2686,summary,0,NIFI-5166 - Deep learning classification and regression processor wit‚Ä¶,non_debt,-
jena,463,review,210921706,"These leading space arise in Eclipse. After a closing ""}"" the cursor is padded to the same depth as the ""}"". Is there a way to change that without using a save action?",non_debt,-
incubator-dolphinscheduler,1187,description,0,173335706-1187 description-0,non_debt,-
hbase,2167,summary,0,HBASE-24791 Improve HFileOutputFormat2 to avoid always call getTableRelativePath method,code_debt,low_quality_code
spark,11162,review,52561026,i dont know about this. Could you explain briefly how does this annotation help?,non_debt,-
spark,22500,review,220414353,"Although this is not related to this refactoring, ping @rxin and @kiszk because @kiszk seemed to want to fix the root cause of the failure.",non_debt,-
attic-stratos,116,comment,70448716,"Thanks Imesh. This is a improvement. We will merge this improvement after the Alpha release.Hence closing the PR
Thanks,
Gayan",non_debt,-
tvm,5035,comment,597439447,@tqchen updated. PTAL. Thanks.,non_debt,-
storm,2480,review,159413588,Updated the PR.,non_debt,-
kylin,352,description,0,‚Ä¶Scanner.,non_debt,-
spark,7048,comment,115984726,"Thanks for fixing this! `ENUM` is not specified in the Parquet format spec, but according to parquet-mr 1.7.0, it is only used for converting Avro, ProtoBuf, and Thrift files. Double checked that in all cases `ENUM` is mapped to UTF8 string. So I think it's OK to always map `ENUM` to `StringType` regardless the value of `assumeBinaryIsString`.",non_debt,-
spark,15573,description,0,"Jira: https://issues.apache.org/jira/browse/SPARK-18035
In HiveInspectors, I saw that converting Java map to Spark's `ArrayBasedMapData` spent quite sometime in buffer copying : https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala#L658
The reason being `map.toSeq` allocates a new buffer and copies the map entries to it: https://github.com/scala/scala/blob/2.11.x/src/library/scala/collection/MapLike.scala#L323
This copy is not needed as we get rid of it once we extract the key and value arrays.
Here is the call trace:
Also, earlier code was populating keys and values arrays separately by iterating twice. The PR avoids double iteration of the map and does it in one iteration.
EDIT: During code review, there were several more places in the code which were found to do similar thing. The PR dedupes those instances and introduces convenient APIs which are performant and memory efficient
The number is subjective and depends on how many map columns are accessed in the query and average entries per map. For one the queries that I tried out, I saw 3% CPU savings (end-to-end) for the query.
This does not change the end result produced so relying on existing tests.",code_debt,duplicated_code
airflow,151,summary,0,Adding event callbacks to BaseOperator (all tasks)!,non_debt,-
hadoop,575,review,268897325,Cut them out; will keep maintenance down,non_debt,-
spark,20229,summary,0,[SPARK-23045][ML][SparkR] Update RFormula to use OneHotEncoderEstimator.,non_debt,-
trafficcontrol,3379,comment,470204804,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/3300/
Test FAILed.",non_debt,-
hadoop,2556,summary,0,HDFS-15731. Reduce threadCount for unit tests to reduce the memory usage,design_debt,non-optimal_design
kafka,5474,comment,411231844,@vvcephei this PR lgtm.,non_debt,-
camel,2010,summary,0,CAMEL-11881: Handling special arguments when declaring queues,non_debt,-
activemq,556,comment,691129531,Closing this as it was since fixed by: https://github.com/apache/activemq/commit/4450c17c1c836a1daa7541dcb944f35798258197,non_debt,-
drill,1640,review,261588501,Thanks for suggestion but I find it harder to read.,non_debt,-
arrow,2936,review,233587937,Don't the time and date cases need to be handled differently (calling TreeExprBuilder_MakeInExpressionDate etc.?),non_debt,-
trafficcontrol,3766,comment,541298688,retest this please,non_debt,-
trafficcontrol,3766,comment,542325854,retest this please,non_debt,-
kafka,2320,review,94887536,"I thought about it before, but it is a bit tricky to do since it could be dynamic based on which class the `toString` function is triggered first.  Let me think about it more and see if I can come with a clean solution.",code_debt,low_quality_code
beam,8270,review,274875921,Seems it'd be worth factoring this out into a context (including the expansion service jar test above).,code_debt,low_quality_code
trafficserver,7281,review,508918650,"This locking strategy looks like it could block multiple event threads while a new config is being loaded.  I suggest letting transactions use the old config while the new one is being loaded, using https://godbolt.org/z/3bq43z or something like it.",non_debt,-
druid,6702,review,335746585,why not use `dig +short $HOSTNAME` everywhere?,non_debt,-
spark,186,comment,38623099,Only if there is no rc2 :),non_debt,-
hbase,2440,review,493689369,"Yep and this is I.A.private, so..",non_debt,-
couchdb,169,description,0,206417-169 description-0,non_debt,-
spark,16381,comment,268861717,LGTM. Merging to master/2.1. Thanks!,non_debt,-
incubator-mxnet,11357,summary,0,[MXNET-331] Single machine All Reduce Topology-aware Communication,non_debt,-
flink,15439,comment,810469613,"Meta data
{
  ""version"" : 1,
  ""metaDataEntries"" : [ {
    ""hash"" : ""de081e28e7377785ea255b466988d4f0419c31f2"",
    ""status"" : ""DELETED"",
    ""url"" : ""https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=15818"",
    ""triggerID"" : ""de081e28e7377785ea255b466988d4f0419c31f2"",
    ""triggerType"" : ""PUSH""
  }, {
    ""hash"" : ""12851b775e442319ddb5ada5e2f1f6cf5df73ef4"",
    ""status"" : ""SUCCESS"",
    ""url"" : ""https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=15841"",
    ""triggerID"" : ""12851b775e442319ddb5ada5e2f1f6cf5df73ef4"",
    ""triggerType"" : ""PUSH""
  } ]
}-->
* 12851b775e442319ddb5ada5e2f1f6cf5df73ef4 Azure: [SUCCESS](https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=15841) 
  The @flinkbot bot supports the following commands:
 - `@flinkbot run travis` re-run the last Travis build
 - `@flinkbot run azure` re-run the last Azure build",non_debt,-
carbondata,3183,comment,486091530,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2955/",non_debt,-
cloudstack,4776,comment,806067992,Packaging result: :heavy_multiplication_x: centos7 :heavy_multiplication_x: centos8 :heavy_multiplication_x: debian. SL-JID 242,non_debt,-
incubator-pinot,3856,review,258720423,"This will only be called when it's a new segment. You can call getAssignedInstancesForSegment() when assignedInstances are not provided in addNewSegment(), but it should not be null here. I would recommend add a Preconditions.checkNotNull() because when we call this, the segment ZK metadata has been updated.",non_debt,-
skywalking,3365,comment,526215529,"Take your time. SkyWalking is a large ecosystem, even PMC member is not familiar all parts of the project. So, don't worry. With your further contributions, you will learn more.
And welcome to be 151th contributor of this repo.",non_debt,-
beam,10227,comment,558882339,"We should still get rid of setup_requires, but this just might not be the _complete_ solution.",design_debt,non-optimal_design
beam,7844,comment,463755217,R: @Ardagan,non_debt,-
spark,8780,comment,140936357,"I'm not sure if oracle can be associated with anything _reasonable_, but sometimes you have to play the hand you are dealt. :)
I can only answer your question with a question... Would there ever be a use case in the Decimal() class where the precision and/or the scale would be set to a negative value?
I'd have to assume that there isn't a use case for negative values given the way precision and scale are used and defined, but you'll have to forgive any ignorance on my part as I'm still fairly new to scala. I hadn't even browsed the source for spark until about one week ago. I'm still in the alpha stages of even testing spark in general, so while it's seemingly solved the problem for me in my testing, I could easily be overlooking something.",non_debt,-
beam,13112,comment,740158473,retest this please,non_debt,-
nifi-minifi-cpp,979,review,569451513,Updated the LICENSE.txt in [3dba6a8](https://github.com/apache/nifi-minifi-cpp/pull/979/commits/3dba6a8440d962cd546a7975e4fa9b21386ab6d0),non_debt,-
zookeeper,548,comment,402444134,@mjeelanimsft Just to clarify my latest comment: I'd to see a new class `ConfigUtilsTest` in which you explicitly verify the behaviour of `splitServerConfig`. Thanks.,non_debt,-
spark,14355,review,72452143,Put the If in the guard.,non_debt,-
servicecomb-java-chassis,1767,review,427921194,"seems useless to record context for web container?  
and i don't know what is the context for web container, üòÇ",non_debt,-
airflow,2372,review,140193220,This is redundant.,code_debt,complex_code
beam,12145,review,453061806,Sure! Done.,non_debt,-
spark,27664,comment,608282010,Seems unrelated.,non_debt,-
ambari,2330,comment,421452133,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/3974/
Test FAILed.
Test FAILured.",non_debt,-
trafficcontrol,3688,review,330266503,I think the standard way to do this is to use `strings.Join`.,non_debt,-
couchdb,3414,comment,797069943,"@bessbd thank you!
Would you be able to fetch the logs in `src/couch_views/.eunit/couch.log` and share in gist or txt attachment.",non_debt,-
trafficserver,1863,comment,301370821,[approve ci],non_debt,-
hadoop,582,comment,667318471,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
spark,23747,review,269640058,Nit: Redundant conversion,code_debt,complex_code
hadoop,63,comment,519413862,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
cloudstack,1578,review,77754965,"Per our coding standards, please wrap all `if` blocks in curly braces.",code_debt,low_quality_code
shardingsphere,912,summary,0,new zk client use native zk,non_debt,-
carbondata,2417,comment,403277466,retest sdv please,non_debt,-
flink,11839,comment,624800779,"That depends on whether they share a filename prefix, which would be reasonable though. Could be a problem at the point of rolling though as we could miss some messages ü§´",non_debt,-
flink,7696,review,257630219,Are we planning to use this in different snapshots? So far it's only used for `CopyableValue`.,non_debt,-
openwhisk,2662,description,0,"If invokers have been deployed success.
Then, it will report error when execute `ansible-playbook -i environments/<env> invoker.yml ` again.
because the invoker0 or invoker1 alreay exist, so below errors will appear.
So it is necessary to add the `judge condition` that whether the invoker has been deployed,
if already deployed, there has no need to deploy again.",non_debt,-
tinkerpop,330,comment,226003425,ok - it wasn't a big deal - i could have just merged your stuff into a local branch and tested.,non_debt,-
spark,450,review,11879995,"ha, good catch",non_debt,-
spark,15237,review,90548133,"this seems a little more complicated than is really necessary for the what you're doing here.  couldn't you achieve the same thing by leaving the original code and changing the one line above the original to:
not exactly the same -- it also allows whitespace around the scheduling mode, but maybe a good thing?",code_debt,complex_code
spark,29104,review,460275749,removed,non_debt,-
hbase,1684,comment,626303991,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
superset,10385,summary,0,Feature/merge apache 0.37,non_debt,-
trafficserver,351,comment,199878590,ping on API review?,non_debt,-
superset,11499,review,537844305,Background job for running async chart queries in the context of the new `/api/v1/chart/data` API,non_debt,-
trafficcontrol,4423,review,391129633,"Instead of using `_.find`, consider using the Javascript built-in [`Array.find`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find) method.
Also, though, you could avoid iterating the whole array of assigned parameters for every parameter if it's made into just a Set of ids first:",non_debt,-
druid,4885,description,0,"Add a strategy that allows slot restrictions for tasks with different prefixes, avoiding the task of different data sources to interact with each other because of slot resources. Although the current affinity strategy can achieve similar functionality, but in the cloud environment, there are still many restrictions, such as expansion or shrinkage will lead to ip and port changes, need to re-configure. Restrictions based on slots can be more flexible.",non_debt,-
phoenix,508,review,285231247,"if all is null --> code will use indexes list
if all is not null --> code will get all the indexes on that table. 
will be implemented in the upcoming PR.",requirement_debt,requirement_partially_implemented
spark,23647,comment,457541376,"@srowen @vanzin @squito @HyukjinKwon +Potetial reviewers, could anybody give some suggestions?",non_debt,-
flink,10195,review,350633864,Thanks a lot for the explanation. +1 for the PR.,non_debt,-
carbondata,3894,comment,675868683,retest this please,non_debt,-
iceberg,796,review,383089633,Fixed.,non_debt,-
trafficcontrol,4547,summary,0,Add default sort to phys_locations,non_debt,-
geode,2916,description,0,Delete SingleHopClientExecutorWithLoggingIntegrationTest_log4j2.xml,non_debt,-
flink,1813,review,58112951,JavaDoc,non_debt,-
kafka,5044,comment,390452297,"Hey @mjsax @guozhangwang, before we start the discussion thread, maybe you could give me some initial feedback on the code or KIP?  Especially the logic has been a little deviated from our initial discussion on Jira, and I reflected those in the KIP. Let me know if you need more clarification, thank you!",non_debt,-
karaf,807,summary,0,[KARAF-6226] Expose org.apache.felix.utils.properties package in Karaf config core bundle,non_debt,-
kafka,2382,comment,273980480,retest this please,non_debt,-
iceberg,1893,review,547982631,ditto,non_debt,-
incubator-mxnet,16215,comment,533361303,sure,non_debt,-
incubator-mxnet,15277,review,296085060,nit: add this blank line back in.,code_debt,low_quality_code
superset,7085,comment,475441559,no problem!,non_debt,-
spark,14112,comment,231556655,"Thanks @GayathriMurali for the PR. I think we'll need to override the default behavior of getAndSetParams. Meanwhile, we need to invoke both convertVectorColumnsToML and convertMatrixColumnsToML. 
I'll send a PR to your repository for reference.",non_debt,-
spark,28641,review,435012863,Is this TaskSet index really needed? We should avoid rely on the ordering of TaskSets to verify the results.,code_debt,complex_code
cloudstack,2298,comment,355495161,Packaging result: ‚úîcentos6 ‚úîcentos7 ‚úîdebian. JID-1588,non_debt,-
kafka,182,review,38580655,Fine. Gonna change accordingly.,non_debt,-
flink,879,comment,118008845,"Also maybe it is completely unnecessary to automatically attach a source timestamp if we don't have any windowing operators. 
One other thing that came into my mind: in order to keep ""deterministic"" results after failure we should persist data with the timestamp attached at the sources. Are we planning to do this? I guess this question goes hand in hand with the automatic source level backup even without kafka. I just wanted to bring it up.",code_debt,complex_code
incubator-mxnet,988,description,0,"@piiswrong In kvstore, 
it requires optimizer to be dumped, but current ccSGD doesn't support. So revert first.",non_debt,-
spark,15303,comment,250590621,@srowen @ajbozarth I created this PR without adding any new API. Just rewrote the way getApplicationList constructing the iterator. Can you guys take a look? Thanks!,non_debt,-
storm,1173,review,54728326,"""{}"" syntax",non_debt,-
tvm,6391,review,483906265,Good point. Thanks for pointing this. I will update implementation for rpc.,non_debt,-
spark,9611,review,44488912,endpoint.get != null,non_debt,-
arrow,6271,description,0,Also fix the dependency relationship between Bison and Flex,non_debt,-
ozone,1233,review,482676002,"need space before ""+""",code_debt,low_quality_code
carbondata,1321,review,138830621,"`null != sortScope` can be removed, it is checked inside `CarbonUtil.isValidSortOption`",code_debt,complex_code
hbase,954,review,362696454,Done.,non_debt,-
cloudstack,1572,comment,253562333,"1. the current set of integration tests doesnt test the new functionality added in this PR. Its merely a check to see nothing else is broken.
2. when did we freeze? I skimmed through the mails didnt see anything about master frozen. I do not see any blocker defects for 4.10.0.0. The latest understanding I have is anybody can merge with required code LGTMs and a BVT run.
3. Which smoke tests are broken? Is it you environment issue you are talking about? If not, are these new or did we release 4.9.0 with these broken tests/features?",test_debt,lack_of_tests
hadoop,1478,review,326718672,"So, this is not currently written to the image?",non_debt,-
trafodion,893,comment,269052296,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1493/,non_debt,-
spark,16038,comment,263308381,@srowen here is the companion PR to #16037.,non_debt,-
flink,12176,review,426131381,"Move this before DELETE, because insertions are more than deletions.",non_debt,-
nifi,2237,description,0,"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
spark,6189,comment,102571697, Merged build triggered.,non_debt,-
flink,178,description,0,20587599-178 description-0,non_debt,-
tajo,181,comment,64865141,"+1
Looks great to me!",non_debt,-
druid,2960,summary,0,add available query granularity strings,non_debt,-
flink,10059,summary,0,[FLINK-14543][FLINK-15901][table] Support partition for temporary table and HiveCatalog,non_debt,-
spark,6682,comment,109564743,cc @yhuai @chenghao-intel,non_debt,-
spark,29537,review,476211215,It's the formatted explained plan. So it doesn't really have `Partition Statistics` and `codegenStageIds`.,non_debt,-
cloudstack,872,comment,144036889,"I will file a ticket for the rightsubnetwithin configuration.
I will run the BVT tests and update the results here.",non_debt,-
spark,2442,description,0,Addresses the problem pointed out in [this comment](https://github.com/apache/spark/pull/2441#issuecomment-55990116).,non_debt,-
zeppelin,96,description,0,"This PR implements https://issues.apache.org/jira/browse/ZEPPELIN-74.
Plus, changing existing interpreter's name
spark.spark -> spark.scala
spark.pyspark -> spark.py
hive.hive -> hive.hql
tajo.tajo -> tajo.tql
All changes are backward compatible except for spark.pyspark -> spark.py.
User need either %spark.py, %py (when Spark is selected first) instead of %pyspark.",non_debt,-
spark,14524,review,74780485,"To complete the thought here: I discovered that most of the Pyspark API methods accepted a `java.lang.Long`, in order to only optionally accept a seed. A few did not however. I also made those consistent, so that the Pyspark default of seed=None results in a null to this argument, which results in a default seed in the JVM, which means a random seed.",non_debt,-
incubator-dolphinscheduler,2145,summary,0,Adapting partial code(file name start with H) to the sonar cloud rule,non_debt,-
flink,3574,review,107107801,+space,non_debt,-
kafka,4959,review,188774183,"OK, great.",non_debt,-
openwhisk,4870,review,399976882,"I'd think we explicitly do not want to have the prewarm pool survive, if other actions would benefit from using that space. After all, it's a performance optimization, not a guarantee.
Wouldn't this also be plumbed into the controller for it to not send requests down a path where they might not get executed?
I think this warrants a dev-list discussion ü§î",code_debt,slow_algorithm
incubator-mxnet,16223,summary,0,[mkldnn-v1.0] Add MKL-DNN LRN,non_debt,-
hbase,2747,review,541310426,"For instance, what if a Cell implementation had data members that cached all lengths... a column family length data member and a row length data member, etc. These methods wouldn't make sense to it?",non_debt,-
nutch,219,review,136047841,üëç  the if expression on the `filter` method could also be removed then,non_debt,-
flink,10911,review,368829260,Add a blank before and after the `>`,non_debt,-
camel,5121,comment,785170374,Ok I will try to create a new component camel-slack-sdk,non_debt,-
spark,5355,review,28042923,`np.uint64` -> `np.int32` (to be consistent with Scala implementation),code_debt,low_quality_code
trafficcontrol,3601,review,297263939,"maybe just add a ""finally"" instead of a ""then""?",code_debt,low_quality_code
hbase,1162,comment,584527412,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
spark,16355,comment,269544809,"Jenkins, retest this please",non_debt,-
kafka,3425,review,173644384,"Rather than describing the attributes in prose, could we represent them in the schema as we did above for the old message format:
The explanation of the use of the timestamp type is useful though.",non_debt,-
cloudstack,2594,description,0,"While working on other issues, I found this empty class. I am proposing its removal as it is not used and can only cause confusion.
Locally
Testing",code_debt,dead_code
activemq-artemis,3365,comment,744761907,"@clebertsuconic I fully understand your concerns, I could also just check for presence of characters on the right side of the separator. This would strengthen the validation by always checking for the queue part at least but ignoring the address which would still allow that test to succesfuly pass.
I guess I'd like to hear more opinions though, such as @jbertram's and @michaelandrepearce's, before closing the PR as something that would cause more harm than good üòú
Regarding the PR, I do not ""want"" the PR, I was just looking to contribute. I picked up this Jira issue as it seemed like a good first issue in a large project such as this one. If you can direct me to other good Jira's for starting up artemis contributions I'd be delighted x)",non_debt,-
beam,5696,review,196893936,"Hmm where does the ""token"" here come from? It ought to be the retrieval token, obtained from CommitManifest. I'm surprised that this argument even existed here, before the PR that introduced the retrieval token.",non_debt,-
spark,10427,comment,175997712,@yhuai ping,non_debt,-
flink,9866,review,333911282,"The scala code is almost the same as Java, so no need to create a separate tab for scala , we have also the case that Java/Scala share the same code demo, such as  Filesystem connectors..",code_debt,low_quality_code
jena,337,review,159239355,"Doing just one operation for fluent seems inconsistent. `Context` manipulation isn't (shouldn't) be that common an operation.
See also the quite recent `Context.mergeCopy`.",code_debt,low_quality_code
airflow,11132,review,495856534,"Yea default values can be hardcoded.
And also agree that showing a precise message for the used config will be better",code_debt,low_quality_code
pulsar,1029,review,164277238,"@merlimat After second thought, I found you are right! Excuse me!",non_debt,-
zookeeper,983,comment,504269773,Thanks for your reviews!,non_debt,-
flink,1052,comment,134567455,"Really good work @r-pogalz. I had only some minor comments concerning style and test cases. 
I like your approach to split the implementation of FLINK-687 into multiple parts. This makes it far easier to review. Concerning the description of FLINK-2106, you haven't integrated the outer sort merge join into the optimizer and the API, yet. I guess this will happen as a next step. Maybe you can update the description of FLINK-2106 accordingly.
Other than that, the PR looks good to me :-)",code_debt,low_quality_code
spark,12601,review,67582037,"I can easily do a simpler getOrElse as is done in [spark-xml](https://github.com/databricks/spark-xml/blob/9f681939d16508abf4a12a129469ffebf87a2fa4/src/main/scala/com/databricks/spark/xml/XmlRelation.scala) which has more of a benefit of being lazier. But if an error does occur due to a mismatch, then the error is further from the original issue. I'm fine with either scenario, but at least wanted to give the other side for this one. Thoughts?",non_debt,-
spark,30671,description,0,"This reverts commit 1de3fc42829187c54334df1fb2149dc4aeb78ed9.
https://github.com/apache/spark/pull/30643#issuecomment-740454543
Thanks for sending a pull request!  Here are some tips for you:
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
-->
-->
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
-->
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->",non_debt,-
carbondata,3091,comment,466028784,LGTM,non_debt,-
iceberg,1793,review,553700047,Same with these other methods. Should these be primitives?,code_debt,low_quality_code
druid,6911,comment,460336824,I think it might be worth calling this out on the user group too in order to ask if anyone's life has ever been saved by the `InsertSegmentToDb` functionality.,non_debt,-
kylin,1538,review,555469177,Make 1000 configurable .,non_debt,-
flink,6076,review,192981069,`TwoInputStreamOperator` and `OneInputStreamOperator` are internal classes. We should not mention them here but use a generic `operator with one input` term.,code_debt,low_quality_code
kafka,2054,comment,271061611,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/600/
Test FAILed (JDK 7 and Scala 2.10).",non_debt,-
spark,29231,comment,710633885,This lands at `branch-3.0` now.,non_debt,-
spark,25863,comment,533796440,"@cloud-fan 
In this PR, I set a unique output dir for partition overwrite operation, both dynamic and static partition overwrite.
cc @advancedxy @wangyum",non_debt,-
storm,1781,comment,262494323,"That should do it - the batching option has been removed from MapState, in favor of parallel processing with existing opaque and transactional logic to handle consistency. Cassandra batch statements are more trouble than they're worth.",design_debt,non-optimal_design
flink,6924,review,230024940,In that case you should probably fix it in `flink-kafka-connector/pom.xml`,non_debt,-
flink,11491,review,398874253,I'd rather choose stick to convention.,non_debt,-
ignite,8456,description,0,"Thank you for submitting the pull request to the Apache Ignite.
In order to streamline the review of the contribution 
we ask you to ensure the following steps have been taken:
The description explains _WHAT_ and _WHY_ was made instead of _HOW_.
The following pattern must be used: `IGNITE-XXXX Change summary` where `XXXX` - number of JIRA issue.
the `green visa` attached to the JIRA ticket (see [TC.Bot: Check PR](https://mtcga.gridgain.com/prs.html))",non_debt,-
incubator-doris,3369,review,421548232,No throw exception? `dbTransactionMgr` could be `null`,non_debt,-
arrow,8890,review,540609789,"Like `ConcreteRecordBatchColumnSorter`'s `next_column_`?
It would work.",non_debt,-
spark,11601,review,59323059,Yes. added support for null values.,non_debt,-
trafodion,1036,review,109077021,Would this change be needed if we touch the package name in T2 source to include apache ? Should we change the package name (org.apache.trafodion...) in the source and the file layout  to make it consistent ?,code_debt,low_quality_code
carbondata,1515,comment,345413264,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1254/",non_debt,-
gobblin,1422,review,91768891,"No. Those are not random numbers. Those empirical number took me a long time to achieve the best balance among speed, retries and reliability. Check details at this PPT https://iwww.corp.linkedin.com/wiki/cf/display/DWH/Google+Search+Console+Data+Ingestion#GoogleSearchConsoleDataIngestion-PPTPresentation",non_debt,-
carbondata,1196,comment,318106840,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/3202/",non_debt,-
hbase,3024,comment,791866049,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
openwhisk-wskdeploy,564,description,0,Closes #550,non_debt,-
iceberg,1145,review,449363672,A `reset` in `TaskWriter` is just clear complete files?,non_debt,-
spark,4537,comment,97585868,"Actually this branch has merge conflicts with the master. May be that is the reason? Please update your branch and test again. 
Also I saw python streaming tests handing in another PR of mine last night. I wonder if this is a Jenkins issue. Nonetheless please update your branch.",non_debt,-
flink,3715,review,126987096,"`DataStreamWindowJoin` should support outer joins, but at the moment it does not. 
Until it supports outer joins, I would not translate to `DataStreamWindowJoin`.",non_debt,-
trafodion,1753,comment,443198539,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/3046/,non_debt,-
kafka,5548,review,211911555,"Indeed it changes the behavior of how we close the producer as it won't be a force close after the refactor, however I don't think it changes the behavior of the test itself. Since we're not setting any response on the mock client, the initTransaction should timeout regardless of how do we close the connection at the end.
@huxihx as you're the author of the original code, would you please help us whether this change is correct or are there any details I'm missing?",non_debt,-
spark,24518,comment,489824088,"I think I have addressed all the comments, please review this again.
@gatorsmile @HyukjinKwon @mengxr @dongjoon-hyun @WeichenXu123",non_debt,-
hadoop,2470,review,534462323,This class is audience private so presuming it ok changing this public static's name.,non_debt,-
druid,1576,review,53042783,fixed,non_debt,-
beam,812,review,75028654,Done,non_debt,-
carbondata,1483,description,0,"          UT Added
Issue:
When having duplicate values in the data, filter results are wrong
Scenario:
Load data like below
a,11234567489.7976
b,11234567489.7976000000
Filter query on double_column = 11234567489.7976
Result - only either one of the row is selected
Actual Result - all the two rows should be selected(both the values will be same while parsing)
Logic of binary search while applying filter is changed (in case of duplicates in DOUBLE column)",non_debt,-
openwhisk,871,review,70184998,aren't these used by a test?,non_debt,-
activemq-artemis,2484,comment,451563907,"@franz1981 did you send a pr to @qihongxu branch so he can merge it and this pr picks it up?
Be great to see a final stat in @qihongxu test env",non_debt,-
druid,4271,summary,0,Extension points for authentication/authorization,non_debt,-
incubator-pinot,5021,summary,0,Added execution command for prestoUI in the readme,non_debt,-
spark,1484,review,23672440,It might be easier to read if we use multiple lines:,code_debt,low_quality_code
flink,13795,comment,717237283,Thanks for the comments @aljoscha. I integrated them and I will merge soon.,non_debt,-
trafficserver,3820,comment,396753626,@dragon512 Is this crashing too ? Do we have some issue on 7.1.x branch ?? @bryancall,non_debt,-
spark,28952,review,447482241,"let's put `10` as a parameter, to make this method a bit more general.",code_debt,low_quality_code
airflow,10103,comment,667608032,The PR title doesn't match the changes,non_debt,-
hadoop,641,comment,477725418,"Thanks for the review @ajayydv , the checkstyle issue is not introduced by this patch. I've opened a separate JIRA: https://issues.apache.org/jira/browse/HDDS-1350 so that we can get a clean cherry-pick for ozone-0.4 here.",non_debt,-
spark,13410,review,65192185,"`seed` is not necessary since we use `approxQuantile`. It was added after Spark 1.6, so removing it will not involve breaking change.",code_debt,low_quality_code
superset,6936,comment,485498379,"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/6936?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/6936?src=pr&el=continue).",non_debt,-
carbondata,1359,review,139122030,I did not get the meaning of index. it is supposed to be independent of other indexes. I think onBlockEnd event is enough for writing the index file.,non_debt,-
trafficserver,6978,summary,0,Add option for hybrid global and thread session pools,non_debt,-
spark,2919,comment,60703036,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/22315/
Test PASSed.",non_debt,-
fineract,810,comment,623127986,"@maektwain just iteratively follow the process described on https://github.com/apache/fineract#pull-requests ... :smiling_imp: (so add that to this PR as well). If this could be written more clearly in the README, then improve it. smiley_cat",documentation_debt,low_quality_documentation
kafka,5049,comment,390888101,"@ijuma Sure! It seems that the last commit `e70a191d3038e00790aa95fbd1e16e78c32b79a4` in trunk caused `./gradlew checkstyleMain checkstyleTest` to fail. It prevents this and future PR from running tests. 
Looking at that patch, the check style failure seems to be due to the following change. Can we remove the log.isDebugEnabled() check?",non_debt,-
spark,22524,comment,423874035,@xuanyuanking Thanks.,non_debt,-
spark,24057,comment,474356670,@squito what do you think of this much of the change? Don't know if it just works on Hive 2 but I think this much is necessary in any event,non_debt,-
trafficcontrol,5643,review,596212676,Same as above,non_debt,-
activemq-artemis,106,comment,127426120,"[ActiveMQ-Artemis-PR-Build #583](https://builds.apache.org/job/ActiveMQ-Artemis-PR-Build/583/) UNSTABLE
Looks like there's a problem with this pull request",non_debt,-
spark,30392,comment,729168362,"I think so. In some cases, unnecessary executor-side reduce might invoke an additional map task although it just returns the single element. So this is just a minor concern for me.
The other thing is, `takeOrdered` is introduced into RDD API earlier than `treeReduce`. So I guess we may not consider to use `treeReduce` in this place before. For the nature of `takeOrdered` that sequences of elements are collected to the driver and reduced. It sounds a good fit for `treeReduce` as we can partially reduce before collecting to the driver. There is an overhead but sounds like a trade-off. Driver side usually a bottleneck for such case. The driver-side reduce is not parallel and also bound to local memory for all data.
I'm totally okay to close this if it doesn't sound good direction to go.",design_debt,non-optimal_design
drill,1749,review,339008196,"There is a handy method to define format out-of-box, please see https://github.com/apache/drill/blob/master/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/writer/TestTextWriter.java#L250 as example.",non_debt,-
spark,11071,comment,180201265,"The map key could be like ""UTC+01:00"". ""American/Los Angeles"", ""PST"", etc., they are already cached in `getTimeZone`, but the method itself is a synchronized one.",non_debt,-
kafka,217,comment,142877943,"[kafka-trunk-git-pr #521](https://builds.apache.org/job/kafka-trunk-git-pr/521/) FAILURE
Looks like there's a problem with this pull request",non_debt,-
spark,5267,comment,126521482,@freeman-lab It's alright. Thanks!,non_debt,-
arrow,3416,review,252272868,Pass linkers to the gtest cmake. This might fix the trusty failure maybe:,non_debt,-
hive,1221,summary,0,HIVE-23786: HMS server side filter with Ranger,non_debt,-
spark,14638,comment,265441330,Retest this please,non_debt,-
spark,24374,review,284258115,that api would work for now when we only have addresses but I don't think will be as nice if we add in count and the addresses are optional. Since its private we could have this and change later if needed but just thinking ahead perhaps we call them acquireAddresses and releaseAddresses,non_debt,-
superset,3652,comment,336312708,Coverage increased (+0.08%) to 70.191% when pulling **4e9263e804d4297a94edf81aaeaf1b1f39cfd04d on afernandez:afernandez_impersonate** into **52a9f2742b5003e81604da1494a452f677cbe33c on apache:master**.,non_debt,-
beam,10767,review,393907154,Perhaps it's a moot point with the [switch to docsy](https://lists.apache.org/thread.html/r7fa6d710c0a1959cce5108e460d71c306ce5756cf96af818b41cb7ca%40%3Cdev.beam.apache.org%3E) going on,non_debt,-
spark,4588,review,26089697,super nit: order. swap with import above.,code_debt,low_quality_code
netbeans,2125,description,0,Proposal of commit author validation using a GitHub action: https://github.com/marketplace/actions/check-author-name-and-email,non_debt,-
tvm,4586,description,0,"`SaveDLTensor` function declaration is `const DLTensor* tensor`, but implementation is `DLTensor*`, if we pass `const DLTensor*`, we could compile successfully. However, when we run tvm, we will meet symbol can not find. We need to unify the interface and implementation so that we could avoid this runtime error.
@zhiics @tqchen please help to review",non_debt,-
streams,254,comment,135895944,:+1:,non_debt,-
arrow,4515,description,0,"At this point the function is not exported or documented and threads are always used, users would need to set `options(arrow.use_threads)` to turn them off.",documentation_debt,outdated_documentation
spark,26080,review,347911028,ditto,non_debt,-
nifi,2991,review,222560898,"""It is used to prevent denial of service type of attacks, to prevent filling up the heap or disk space."" is not applicable here I believe. The statement is for ""Max Request Size"".",non_debt,-
spark,4105,comment,103159431,"This is reasonable, but it doesn't handle `Error`. I don't think you need a new 'inner' method? it also duplicates the job cleanup code.",code_debt,low_quality_code
trafodion,21,comment,120188280,Looks good ! Merging it in now...,non_debt,-
trafficcontrol,4106,comment,556237556,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/4802/",non_debt,-
camel,4514,comment,738593426,Updated PR after discussion with @zregvart and @jeremyross . I will add `rawPayload` support in other composite operation.,non_debt,-
ignite,6069,summary,0,ignite-11261: [ML] Flaky test(testNaiveBaggingLogRegression),test_debt,flaky_test
tvm,1905,summary,0,[Relay][Test] remove redundant test cases in test_op_level4.py,code_debt,dead_code
arrow,5451,review,329113827,"This is ""bad"", according to the tidyverse style guide, which I believed we were trying to follow: https://style.tidyverse.org/functions.html#long-lines-1
I can get used to whatever style conventions we decide, just want to make sure we're in agreement.",code_debt,low_quality_code
spark,26219,review,345304681,ok let's leave it.,non_debt,-
trafficserver,2407,description,0,This is minor fix to correct debug info.,non_debt,-
incubator-dolphinscheduler,4188,summary,0,[Improvement-4187][Master] Add log exception stack information,non_debt,-
kafka,2444,comment,276345941,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1353/
Test PASSed (JDK 8 and Scala 2.11).",non_debt,-
arrow,3963,review,273177077,"This is a refactor oversight, it used to be size / 2.",non_debt,-
pulsar,5721,comment,557240314,retest this please,non_debt,-
infrastructure-puppet,610,description,0,INFRA-15440: allow self-serve to update qmail files,non_debt,-
spark,21031,review,182622583,"I see. I will update this PR with only `expression[Cardinality](""cardinality"")`. I will also update the description and JIRA later.
Is it OK with you? @gatorsmile cc: @ueshin",non_debt,-
druid,50,review,2541441," Also, there's no need for the String.format(), preconditions will do %s interpolation for you (I realize the String.format()was there before, but let's take this chance to get rid of it).",code_debt,low_quality_code
ambari,2441,summary,0,AMBARI-24762. Ambari server continues to send request updates after all commands were completed.,non_debt,-
couchdb,835,comment,331552644,"Looks good to me, but changing `?term_size/1` scares me. @davisp - second opinion, please?",non_debt,-
cassandra,224,review,194812259,fixed,non_debt,-
nifi,3174,comment,439389815,"it built with java 8.  switched to 11.  nifi starts up and appears to be working great.
I did notice this on startup
nifi.sh: JAVA_HOME not set; results may vary
Java home: 
NiFi home: /Users/joe/development/nifi.git/nifi-assembly/target/nifi-1.9.0-SNAPSHOT-bin/nifi-1.9.0-SNAPSHOT
Bootstrap Config File: /../development/nifi.git/nifi-assembly/target/nifi-1.9.0-SNAPSHOT-bin/nifi-1.9.0-SNAPSHOT/conf/bootstrap.conf
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.nifi.bootstrap.util.OSUtils (file:/../development/nifi.git/nifi-assembly/target/nifi-1.9.0-SNAPSHOT-bin/nifi-1.9.0-SNAPSHOT/lib/bootstrap/nifi-bootstrap-1.9.0-SNAPSHOT.jar) to method java.lang.ProcessImpl.pid()
WARNING: Please consider reporting this to the maintainers of org.apache.nifi.bootstrap.util.OSUtils
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release",non_debt,-
flink,12042,review,422441179,"in fact, the table qualified name may be long. in addition, we may need to add catalog name and database name to distinguish between tables with the same name.",code_debt,low_quality_code
kafka,7389,review,379859074,"I think it's fine, I think I was seeing it a lot because I was running tests using JDK 8 but I had built with JDK 11 , so the metadata calls were constantly erroring out. Normally I would expect this to log a few times and then resolve",non_debt,-
lucene-solr,134,comment,270461615,@markrmiller can you take a look?,non_debt,-
incubator-mxnet,14769,description,0,"This adds a fine tuning example for BERT based off of the following Gluon NLP example
[https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html](https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html)
There is a PR in the GluonNLP repo for the export script https://github.com/dmlc/gluon-nlp/pull/672
The original BERT infer example directory has been changed from `bert-qa` to `bert` to accomodate both the QA example and this one.
A walkthrough in the form of a Jupyter notebook has been provided for the sentence pair classification based off of the Gluon NLP one. A markdown version of the notebook has also been provided for those who don't have the `lein-jupyter` plugin.
Example results for sentence pair classification after 3 epochs:
which is comparable to the sentence embedding results in the gluon nlp notebook
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- Move BERT QA infer example within a broader BERT example
- Add BERT Sentence Pair Classification Example along with notebook
- Tweak the Callback Speedometer so that the results will be printed out in the notebook instead of having to look at the console for the logging.
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here",non_debt,-
flink,4559,review,157693477,sure,non_debt,-
gobblin,3099,review,487298251,I might miss something obvious here: why this line (used for exclusion) is no longer required?,non_debt,-
incubator-mxnet,16881,comment,571043129,@Kh4L @wkcn  What is the reason for moving the exisiting function to mshadow?,non_debt,-
kafka,5093,comment,395983207,My suggestion is to only remove things that have been deprecated in 0.11.0 or older. One year seems like the minimum we should provide for users to migrate.,non_debt,-
flink,12268,review,430016877,"hmm, not sure. Maybe this happens because the JVM writes that to the STDERR. That's annoying :/
This is probably fine then; I was initially worried we might get so long filenames that you wouldn't be able to extract the logs on Windows.",code_debt,low_quality_code
spark,6959,review,33198256,"AFAIK we don't have a style guide for Java code, but I think we should put spaces after the casts.",code_debt,low_quality_code
incubator-mxnet,14136,comment,462960508,"@zhreshold In Horovod case, each training process is attached to a GPU. If we do not specify device_id for the cpu_pinned context, all processes will use the memory in GPU 0 (because the default device_id for cpu_pinned context is 0) and cause out of memory error. I had a similar [enhancement](https://github.com/apache/incubator-mxnet/pull/13980) for ImageRecordIter.",non_debt,-
kafka,413,review,43915597,"I see, sounds good.",non_debt,-
druid,5649,comment,383651021,@gianm thanks I have added tests and the start intervals,non_debt,-
kafka,2405,review,97622033,Feels like this collection is redundant. You can get the name from `InternalTopicConfig` perhaps?,code_debt,complex_code
spark,27612,review,380444937,"~nit. If possible, can we have additional test statements for `minute` and `second` together below line 743 to make it sure?~
Never mind. I missed the other PR which landed already on `branch-2.4`.",code_debt,low_quality_code
kafka,1791,review,80134212,Rather than using `!muted` I think it would be much clearer to alias this to a boolean that is named: `guaranteeExpirationOrder` similar to the `guaranteeMessageOrder` boolean in `Sender`,code_debt,low_quality_code
daffodil,407,review,470752272,I'd also like to see a test for leading space,test_debt,low_coverage
flink,3770,review,113919931,"Yes, that is what I also expected. Just wanted to be really sure.",non_debt,-
trafficcontrol,4786,description,0,"- Traffic Ops
- Traffic Portal
- Run the topologies API tests in CDN-in-a-Box, make sure they pass.
- In TP, create a topology where one or more edge_loc cache groups have a edge_loc parent or secondary parent. Ensure the warnings are displayed and can be save successfully. Also ensure that edge_loc cache groups can only parent edge_loc children.
Includes a bug fix. `GET /topologies` used to return `null` for 0 topologies, now it returns an empty array.
- master (2dd9a0cdf1)
Otherwise, not a bug fix.
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->",non_debt,-
activemq-artemis,87,comment,122921574,"One the other hand I can get my PR submitted tonight provided I can get the test suite to run some sane way. I'm an Eclipse user and I guess I found the place where to run the desired test suites, problem is that when I do right-click ""Run as"" -> ""JUnit test"" for the stomp package in integration-tests I get a lot of errors about classes not found, some ExceptionInInitializerErrors due to <code>Caused by: java.lang.IllegalArgumentException: Invalid logger interface org.apache.activemq.artemis.tests.integration.IntegrationTestLogger (implementation not found in sun.misc.Launcher$AppClassLoader@409a44d6)</code> etc, so I suppose my setup needs some changes, unfortunately I'm not sure what...",non_debt,-
thrift,520,summary,0,Remove moot `version` property from bower.json,non_debt,-
spark,1313,review,15563577,"Ah okay, makes sense.",non_debt,-
nifi,1523,comment,281262286,"Hi @automaticgiant , unfortunately, 'exec' command added into nifi.sh by #966 broke `nifi.sh restart` to work properly. I tried fixing it with this PR.
I confirmed that 'exec' is taking its effect, but I am not sure if it still work as you expected, i.e. whether `nifi.sh run` can be supervised by runit ... etc. Would you try this PR and check if it works?
Thank you in advance.",non_debt,-
carbondata,1707,comment,354043499,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/2340/",non_debt,-
druid,5102,review,158433249,"@jihoonson If you mean the actual ""CompactionTask"" etc classes, I think probably moving something so heavy from druid-indexing-service all the way down to druid-api would probably require collapsing a ton of druid modules into one giant druid-core module. I guess we could do that but it seems like a big change. Do you think it's worth it?",architecture_debt,violation_of_modularity
spark,4593,comment,75540678,"I have the same concern as @dbtsai in his comment. Most consumers of this API will already be caching their dataset before the learning phase. Without user care, this will introduce effectively double caching (in terms of data size of cached RDDs) and will cause many jobs to fail after upgrading by exceeding available heap for RDD cache. Furthermore, we are making assumptions about how to cache -- in-memory only in this case. Should we parameterise this? Perhaps that will help send the message in the API that there is caching also done before learning. (FWIW, in-memory is definitely the right default choice here.)
See email thread on dev for my specific encountering of this bug: 
http://mail-archives.apache.org/mod_mbox/spark-dev/201502.mbox/%3CCAH5MZvMBjqOST-9Nr9k1z1rUODfSiczr_fV9kwqDFqAMNLC2Zw%40mail.gmail.com%3E",design_debt,non-optimal_design
spark,5350,review,28019159,"We need more thinking about reuse the UTF8String object, it's not a trivial decision, so I'd like to leave this out of this PR.",design_debt,non-optimal_design
skywalking,6187,comment,759157679,"I fixed the problem you mentioned, what should I do next, do I check the ci again~",non_debt,-
cassandra,493,review,428233400,"if `static row` is the only outdated data, then `BTreeSet.Builder<Clustering> toFetch` would be empty..so `ClusteringIndexNamesFilter` will has no clusterings and query entire partition in `querySourceOnkey()`..",non_debt,-
flink,15389,summary,0,[FLINK-21609][tests] Remove usage of LocalCollectionOutpuFormat from SimpleRecoveryITCaseBase,code_debt,low_quality_code
spark,30312,review,527995000,"Sure, I will change the default to be `connectionTimeoutMs`",non_debt,-
kafka,9100,review,477786630,I still think we need a better name for `pendingInSyncReplicaIds` since it is misleading in this case. Maybe we could call it `overrideInSyncReplicaIds` or something like that?,code_debt,low_quality_code
tvm,6274,review,483721402,70746484-6274 review-483721402,non_debt,-
calcite,2076,review,457807922,Remove this sentence. It is not needed anymore.,code_debt,dead_code
reef,1412,comment,342832924,"The tests came back with good and bad news. The good news is that the newly added tests passed on the build server. The bad news is that the VS 2013 build failed at the compilation stage:
Do we still care about VS 2013 as a target?",non_debt,-
carbondata,3821,comment,653383759,"Build Success with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/3296/",non_debt,-
spark,21322,comment,429410774,"@cloud-fan (Also left on the JIRA ticket) - Sorry this has dropped off my radar for so long - work + life took me away from it for a while. So looking at the PR review comments and better understanding Broadcast Variable behavior (and some of the changes that took place in the 2.X series), it seems like simply trying to close Broadcast variables won't work as intended. However, I believe the underlying concept (driver-scoped shared variables, where the variable lives until the job is done or the driver removes it) is still worth pursuing. Being able to scope shared resources (like DB connection pools, which may need to change per phase of a job, or be able to be disposed of early in a process, which makes static variables not useful). Given that, I'd like to propose we add a new concept, similar to Broadcast Variables, called, perhaps, Scoped Variables. The intent would be for these to be scoped by the driver, be relatively small from a memory-consumption perspective (unlike broadcast variables, which can be much larger), and to be held in memory until explicitly removed by the driver. Most of the infrastructure work for broadcast variables supports this use-case, but we'd need to have either a ""non-purgable"" type in the MemoryStore, or some other store specific to these new scoped variables, in order to prevent them from being evicted like cached items are.
Thoughts on this? I'll start working on updating the PR to support something like this sometime today, but it might still take a while to get something workable put together, so I'd appreciate any feedback when someone has the time.",non_debt,-
spark,11505,review,55262278,Mostly because of the checkpoint stuff :(,non_debt,-
hbase,1747,description,0,20089857-1747 description-0,non_debt,-
superset,8629,summary,0,[SQL Lab] Wrap more logic with feature flag,non_debt,-
beam,4254,comment,352141618,LGTM. I will merge after tests. Thank you.,non_debt,-
cloudstack,4397,comment,708437967,"revert or fix forward @GabrielBrascher your choice for my part,, but with priority please.",non_debt,-
spark,27359,comment,579564520,"@shaneknapp TR;DL: Now SparkR supports both testthat 1.0.2 and 2.0.0. I will backport this to branch-2.4 at #27379 as well.
So, as a reminder:
- We should upgrade testthat in Jenkins to 2.0.0 at any earliest time you're available
- We should upgrade R version to 3.4.x after Spark 3.0 release.",non_debt,-
flink,705,review,30817476,Actually no. I think it's safe to have it fixed. We only need to adjust the values for tests. What is the easiest way to only allow internal configuration? The problem is that for integration tests it's hard to set configuration values for runtime components otherwise.,design_debt,non-optimal_design
ignite,7936,review,442743689,"Here, val is ""non-negative"" rather than ""positive""
Let's rename methods.",non_debt,-
kafka,2822,comment,292657993,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2831/
Test FAILed (JDK 7 and Scala 2.10).",non_debt,-
spark,28258,review,428693738,that's why I said we need to change the delay (e.g. 5s) instead of 0 for both submiting and killing.,non_debt,-
storm,2203,comment,346073151,"Here are my numbers, and no I didn't do it headless but I was not doing anything on the laptop at the time it was running except looking at Activity Monitor to be sure nothing else was using too much CPU.
The setup is
MacBook Pro (Retina, 15-inch, Mid 2015)
2.8 GHz Intel Core i7
16 GB 1600 MHz DDR3
macOS 10.12.6
java version ""1.8.0_121""
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)
The versions tested are 1.2.0-SNAPSHOT (b24f5d87cbcd4faeed651e45a8e673db52469a46)
Metrics V2 (78d3de44cf0705efd199fb64b7ef092e23c3285b) which is 1.2.0-SNAPSHOT + 99bcf68 merged in.
The Numbers:
1.2.0-SNAPSHOT
Metrics V2:
At 50,000 sentences per second on a single worker both can keep up, but the mean latency appears to be about 200 to 300 microseconds slower 9.6 ms vs 9.9 ms.  The bigger issue in my mind is the CPU time as it went up from about 79 seconds (max 30 second period) every 30 seconds to 92 seconds (min 30 second period after stabilizing). That was at least a 16% increase in CPU usage.
At 75,000 sentences per second metrics v2 was just able to keep up with that throughput while 1.2.0-SNAPSHOT could handle it easily. I will not compare the latency because metrics v2 didn't stabilize enough for me to feel good reporting the numbers, but the CPU time went up from 118 seconds every 30 seconds to 130 seconds.  That was an increase of at least 9%.
When I tried to do 100,000 sentences per second neither of the two could keep up, but 1.2.0-SNAPSHOT hit a maximum of 97k while metrics V2 hit a maximum of 78k or 20% less maximum throughput.  Only here did we see the CPU usage be lower for metrics V2 with 134 seconds vs 141 for 1.2.0-SNAPSHOT, but if we take into account the actual throughput and look at CPU seconds needed to process 1k sentences per second it is 1.7 for metrics v2 vs 1.5 for 1.2.0-SNAPSHOT.
I am not saying that these changes are a blocker for the code going in, but I really want to understand what is happening here",non_debt,-
gobblin,1226,review,76138754,not even for statics?,non_debt,-
bigtop,537,comment,519770209,"The test is good! +1
https://ci.bigtop.apache.org/view/Test/job/Build-Deploy-Smoke-Test-Pull-Request-All-Distros/",non_debt,-
incubator-pagespeed-ngx,436,description,0,4766638-436 description-0,non_debt,-
incubator-mxnet,13362,review,239277593,Will do.,non_debt,-
storm,103,review,12410447,I will fix this.,non_debt,-
airflow,5992,comment,568772499,cfg points to broken link https://airflow.apache.org/howto/enable-dag-serialization.html,non_debt,-
flink,10502,review,359579267,20587599-10502 review-359579267,non_debt,-
spark,1527,comment,49731850,Can one of the admins verify this patch?,non_debt,-
incubator-doris,4925,review,528201502,I think `DEL_NOT_SATISFIED` is more safe. Or you'd better change DCHECK to CHECK,requirement_debt,non-functional_requirements_not_fully_satisfied
trafodion,491,comment,220518141,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/735/,non_debt,-
attic-apex-core,234,summary,0,41348333-234 summary-0,non_debt,-
ignite,5761,description,0,31006158-5761 description-0,non_debt,-
spark,29196,description,0,"Updates to scalatest 3.2.0. Though it looks large, it is 99% changes to the new location of scalatest classes.
3.2.0+ has a fix that is required for Scala 2.13.3+ compatibility.
No, only affects tests.
Existing tests.",non_debt,-
ignite,269,description,0,31006158-269 description-0,non_debt,-
beam,13512,description,0,"This PR is dependent on https://github.com/apache/beam/pull/13495 - do not merge it before this one is merged.
Added integration test for Bigtable for BeamSQL and done some refactor to test utils to avoid package and classes errors in GcpApiSurfaceTest.
I've tested  BigtableTableIT with real Bigtable - everything seems to be working.
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.",non_debt,-
ozone,291,review,354282784,Removed.,non_debt,-
pulsar,6641,comment,606421366,/pulsarbot run-failure-checks,non_debt,-
trafficserver,834,review,78213185,"There is a `MIN()` macro you can use for this.
Also, I would use `sizeof(buffer)` instead of magical numbers.",code_debt,low_quality_code
nifi,1553,review,103938291,Are these comments necessary?,documentation_debt,low_quality_documentation
incubator-pinot,3633,review,245437779,remove this,non_debt,-
arrow,1529,summary,0,ARROW-1589: [C++] Fuzzing for certain input formats,non_debt,-
incubator-pinot,889,description,0,"Several fixes applied:
The auto-creation loop was skipping over segments if consecutive partitions had missing segments.
The auto-creation logic was always taking the earliest kafka offset (assuming the error is
that of offset not being found), but this may not be the case. So, if we have an older segment whose
end offset is higher than the earliest available kafka offset, we should pick the higher one.
Added tests for these cases.",non_debt,-
drill,400,review,54976762,Isn't an empty delegation block enough? Any reason to have a second kill switch?,non_debt,-
trafficcontrol,5164,summary,0,Topologies support for assigning ORG servers to Delivery Services,non_debt,-
beam,1605,description,0,"quickly and easily:
   `[BEAM-<Jira issue #>] Description of pull request`
       Travis-CI on your fork and ensure the whole test matrix passes).
       number, if there is one.
       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---
R: @dhalperi",non_debt,-
calcite,2230,comment,719449510,LGTM. Testing locally and gonna merge this :+1:,non_debt,-
activemq-artemis,1616,review,147312753,Should it not be directing people to Apache download as official place to get distribution,non_debt,-
echarts,11373,review,336850823,"`mktest` generate them for convenience when we need to use them (not find and add them manually).
I think it's OK to leave them in a test file.",non_debt,-
incubator-pinot,328,review,72877354,I'm guessing this is the new way to apply migrations. Never mind my comment above,non_debt,-
kafka,1554,comment,228579044,ping @ijuma @guozhangwang,non_debt,-
gobblin,172,comment,112519781,"Why are we removing `compaction.force.reprocess`? It seems to me it is a reasonable use case that users want to re-compact the input every run, even if the output was already compacted.",non_debt,-
spark,13729,summary,0,[SPARK-16008][ML] Remove unnecessary serialization in logistic regression,code_debt,complex_code
druid,3664,review,93462551,why this has to be done from lifecycle handler and not directly after creation of the provider ?,non_debt,-
beam,2001,comment,279578805,"Thanks! LGTM.
R: @aaltay, can you please merge?",non_debt,-
beam,12828,comment,691328440,50904245-12828 comment-691328440,non_debt,-
kafka,386,review,43579299,"Rep @onurkaraman : `removeGroup` should always be guarded by the group lock inside the `GroupCoordinator`, while `getGroup` and `addGroup` are not since the group object is not available yet. I will make that more clear in the comments.
Rep @junrao : ack.",documentation_debt,low_quality_documentation
jmeter,377,comment,367459103,"# [Codecov](https://codecov.io/gh/apache/jmeter/pull/377?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/jmeter/pull/377?src=pr&el=continue).",non_debt,-
activemq-artemis,1322,description,0,"Update ActiveMQConnection to change behaviour, getMetaData and stop methods.
Add test to avoid regression.",non_debt,-
arrow,3961,review,266555498,Can we put all these types in an array or other kind of data structure and do lookups to determine the least-common-ancestor?,non_debt,-
kafka,1782,comment,242832005,LGTM and merged to trunk.,non_debt,-
drill,299,comment,394010292,"@amithadke Thanks for making these changes. Unfortunately, there have been major changes to HashJoin since it now does spilling. So this code will have to be refactored significantly. Additionally there will have be a design discussion on how to handle schema changes with spilling, since I suspect handling that will be non trivial. Since this PR has been inactive for a few years and would require a good amount of work to update. I will close this PR. Please feel free to reopen this PR if you would like to continue forward.",non_debt,-
carbondata,1974,comment,365145726,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3534/",non_debt,-
spark,25626,review,320095359,"nit: we can use multiline string, e.g.",code_debt,low_quality_code
zookeeper,1257,comment,593204387,"looks good. I still have another thought(no binding for the following, you can ignore it):
- Since the root cause is a narrow windows between `queuePacket` and `cleanup`, so synchronized `objectLock` is also an alternative way? which one is better? Since `outgoingQueue` is a critical Queue for client to talk with server, synchronized `outgoingQueue` will have performance and future program extensibility issue?
Haha, I also test for the `global` and `inner` wording by the following way:
- new two different zookeeper clients, create some znodes, printing the `hashcode` of `outgoingQueue` and `state`. They really hold different `outgoingQueue`, but the same hashcode of `state`. it really confuses me.
- I believe different clients will have different `state` instance, otherwise when one client calls `close()`(set `state` to `CLOSE`), it will affect another client. However, using following ways cannot reason about it.
`javap` to see the bytecode, the value set to `state` is `public static final`, so it's really global-shared by multi-clients.
- synchronized `Enum` is also not thread-safe. Look at my demo attached in JIRA.
- In a word, `Enum` is a heresy:)",design_debt,non-optimal_design
kafka,8310,comment,600793844,"I've pushed a HOTFIX commit to address this issue, cc @mjsax @ableegoldman",non_debt,-
trafodion,1361,comment,355272318,New Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/2308/,non_debt,-
commons-lang,145,summary,0,LANG-1168: Add SystemUtils.IS_OS_WINDOWS_10 property,non_debt,-
trafficserver,411,summary,0,Doc: [TS-4113] Add value 4 to proxy.config.http.cache.cache_responses‚Ä¶,non_debt,-
spark,15009,review,84790527,You can simplify all this by doing:,code_debt,complex_code
pulsar,7421,review,449485668,62117812-7421 review-449485668,non_debt,-
spark,10515,comment,168564909,@marmbrus Can we trigger a test for this?,non_debt,-
flink,9984,review,354191588,Will remove.,non_debt,-
zeppelin,27,comment,115711290,"With introduction of interpreterGroup following code from the above example doesn't work anymore. I could track upto different InterpreterContextRunnerPool getting created for multiple interpreters and only one have interpreterContextRunners populated with all my paragraphs. So, sparkInterpreter when sends event to another RemoteInterpreter, it finds the list empty there.
z.angularWatch(""selectedTable"", (before:Object, after:Object) => {
    z.run(2, selectedTableContext)
    z.run(3, selectedTableContext)
    z.run(4, selectedTableContext)
})
It will be great if somebody can fix it asap (because we have a demo pending in another day or two).",non_debt,-
flink,1537,comment,174521693,"Wow, I see your point. Then a single abort message back to the Coordinator is enough in this case. Why 10 min btw?",non_debt,-
beam,3290,comment,306143926,retest this please,non_debt,-
spark,8467,review,38010586,"@rxin Please help verify whether this change for R is correct, thanks!",non_debt,-
attic-apex-malhar,330,review,72194687,"This is used only in AbstractManagedStateInnerJoinOperator.createStore().
In the interest of keeping the number of files less, can the content of method be directly used in createStore?
It just seem to create a new object of type ManagedTimeStateMultiValue.",non_debt,-
spark,14362,review,72187942,"I'm using whatever cast as decimal is using here, but I think it is a bug to by default cast to USER_DEFAULT, which has scale = 0.",non_debt,-
skywalking,5413,review,479768165,ok.,non_debt,-
kafka,3652,comment,321495817,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6668/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
trafficserver,7281,description,0,"Parse expiration time and reload config at time out, expiration is assumed to be a timestamp in seconds since epoch.
The reload is scheduled in the future after calculating the time difference between the expiration time and current time, the scheduled time can be up to 1 hour prior to the expiration time. 
- Expiration time more than 1 hour, schedule the reload 1 hour prior to the expiration time.
- Expiration time less than 1 hour but more than 15 minutes, schedule the reload 15 minutes prior to the expiration time.
- Expiration time less than 15 minutes, schedule the reload at the expiration time.
Setting the expiration to 0, or comment out the expiration time effectively disables the auto reload feature.",non_debt,-
spark,25811,review,324969028,I've just renamed this as it made me confused - I imagined DB as LevelDB but there's separate suite for LevelDB. KVStore sounds better to me.,code_debt,low_quality_code
calcite,1995,review,436380164,Left outer join's cardinality is always greater equal with its left input's cardinality. So this plan is always better than sorting on join's output. üëç,non_debt,-
pulsar,2259,comment,409360529,"in #2200 [first commit](https://github.com/apache/incubator-pulsar/pull/2200/commits/6757851bf86b1b8daa0137463ecbf44875075af8) had change to add `--subscription-type` but based on feedback we have added flag for `retain ordering`.
Isn't this PR address the same thing. if `guarantee is set to effectively once` then sub type will be set as FAILOVER else sub-type will be configured as per ordering-flag.  is there anything we are missing here?",non_debt,-
incubator-mxnet,13832,comment,455167506,@mxnet-label-bot update [pr-awaiting-merge],non_debt,-
cloudstack,2578,review,225156473,"This message looks a little bit misleading, or am I mistaken?",code_debt,low_quality_code
myfaces-tobago,2,comment,369352423,"There is still an issue with mojarra 2.0 PostAddToViewEvent, Head and StateHandling in mojarra 2.0. No more energy to find a workaround for the crap mojarra impl.",non_debt,-
arrow,1012,comment,327039669,@alphalfalfa can you rebase?,non_debt,-
kafka,3197,comment,305633218,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4733/
Test FAILed (JDK 8 and Scala 2.12).",non_debt,-
activemq-artemis,134,summary,0,Improving bootstrap,non_debt,-
trafficcontrol,4518,review,419527713,please change these to be `ENV.api['latest']`.  once #4633 gets merged that will be API v3 and root will still be v2.  You can also hardcode in /api/3.0 if you want with a TODO to fix it later if #4633 isnt merged as soon as we would like this to be,non_debt,-
spark,13286,review,64966890,Will make it a counter. :),non_debt,-
beam,9422,review,317321495,Should we add `@Expremental`? How to decide what piece of SDK is user facing?,non_debt,-
trafodion,1229,review,138769506,"Grammar. Should be ""... supports ...""",non_debt,-
tajo,308,comment,67804082,"Hi @sirpkt ,
+1
The patch looks good to me. I have one suggestion. Each test method name should have the prefix 'test'. For example, `lastValue1` should be `testLastValue1`. It's trivial, so you can immediately commit the patch after fixing them.",code_debt,low_quality_code
incubator-pagespeed-ngx,905,review,24528057,shouldn't this one and the ones below all have `?PageSpeed=off` ?,non_debt,-
nifi,3547,review,302243043,Any reason this is 3.8.0 and not 3.8.1?,non_debt,-
spark,23721,comment,460687703,"CAn you mention briefly how backwards compatibility will work after these changes?  I assume this is somehow handled within parquet itself -- just a pointer to the relevant info would help.
Also I assume I should not bother triggering tests yet, as automated builds will fail without a published version of parquet?",non_debt,-
spark,28909,comment,648623581,Merged to master.,non_debt,-
spark,27366,comment,579209583,"jenkins, retest this, please",non_debt,-
hudi,1687,review,434265606,Can we push the sorting to the spark shuffle machinery? repartitionAndSortWithinPartitions(). It cheap and practically free,non_debt,-
couchdb,302,comment,78156477,"Hi Jan, 
I could do the drill now with make release, create TAR, unpack tar, ./configure, make && make install
Anyhow: running Couch afterwards did fine. 
BTW: so did copying in 1.6.1 files, calling them from local port, replicating them up to the cluster port.
That's great so far!
     Sebastian",non_debt,-
cloudstack,1703,comment,254682050,"@nvazquez thanks for the update, yes it's known issue that when running tests all at once it might cause issues. If you have a look at our Travis runner, we're running each test one at a time:
https://github.com/apache/cloudstack/blob/master/tools/travis/script.sh#L43
Trillian runs them one at a time as well, you may take some hints from Trillian's smoketest runner:
https://github.com/shapeblue/Trillian/blob/master/Ansible/roles/marvin/templates/smoketests.sh.j2#L70",non_debt,-
incubator-pinot,5940,description,0,"This PR fixes the query like:
which raises exceptions:
The reason is that `HOUR` is a pinot UDF which CalciteSQL parser doesn't recognize and marked the SQL_KIND as `OTHER` with the real function name inside the function info.",non_debt,-
spark,24374,review,290419898,"we can remove it now if you want, but it will be needed for the UI work but we can add it back in there if you want",non_debt,-
beam,12881,review,494688827,"`BundleProcessor.ops` is `OrderedDict[str, operations.Operation]`.  Should it be `OrderedDict[str, operations.DoOperation]`?  Or do the transform ids in `BundleProcessor.timers_info` all point to `DoOperations`?",non_debt,-
flink,12260,review,429716465,"Could you add a short comment for this field to explain why we need this? The same to `CatalogManagerCalciteSchema`, `CatalogSchemaTable`, `DatabaseCalciteSchema`. For example:",documentation_debt,low_quality_documentation
airflow,10956,review,498315428,:heart:,non_debt,-
airflow,12867,description,0,"closes https://github.com/apache/airflow/issues/12832
This format is popular and supported by tools like Ansible.
vs
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.
Feel free to ping committers for the review!
closes: #ISSUE
related: #ISSUE
How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->
---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.",non_debt,-
spark,10904,comment,178286446,retest this please,non_debt,-
iceberg,2159,summary,0,AWS: Fail writes fast when S3OutputStream encountered error in async ‚Ä¶,non_debt,-
airflow,6809,comment,565637153,Done and manually tested it. Works fine.,non_debt,-
trafficserver,517,comment,193159857,+1,non_debt,-
trafficserver,7138,summary,0,Remove useless shortopt,code_debt,dead_code
spark,25029,review,300846882,cc @gatorsmile,non_debt,-
beam,2100,comment,282376576,R: @dhalperi,non_debt,-
incubator-pinot,2794,description,0,"* This PR creates a new metadataUpload path for segmentUpload on the controller side, server side yet to come
* Segment versioning will be supported to avoid handling the race condition in upload.",non_debt,-
nifi-minifi-cpp,821,review,454397770,"I thought yaml parsing ignored padding, so
was the same as 
**Will add trimming for the string.**",non_debt,-
spark,23185,comment,443106579,Thanks for skimming the whole doc. cc @srowen.,non_debt,-
cloudstack,4141,comment,664217085,@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.,non_debt,-
couchdb,3347,review,566349531,"better to say `-define(GB, (1024*1024*1024)).` I'm sure the compiler will do the right hing.",non_debt,-
storm,250,comment,63528271,"Sure, I will work on master branch.
Thanks,
Raghavendra Nandagopal",non_debt,-
zeppelin,3498,comment,549233652,"After debugging i realized that the reason my changes weren't working was because spark.app.name was getting overridden by the default value set in interpreter-setting. 
i did checkout your branch and got the following result.",non_debt,-
spark,13057,comment,219532924,"@srowen 
Pardon for the ping.",non_debt,-
beam,11461,comment,619290741,retest this please,non_debt,-
incubator-pinot,6216,comment,743425065,"@yupeng9 @mayankshriv Cleaned up the PR, added the tests and a JsonIndexQuickStart on the github events data. Please take another look",non_debt,-
spark,14079,review,88088492,"well, I think it depends what you mean by ""handled correctly"".  We use the time the taskset completes, so its OK if the failures happened long ago when the taskset started, we still count those failures in the app blacklist, so later failures can trickle in and push us over the limit.
OTOH, this also means that if we were already close to the limit on failures for the application when this taskset started, then a really long running taskset will fail to push us over the limit -- by the time the latest task set finishes, we've expired the old failures, so we only get failures from the new taskset.  So if your taskset time is longer than the blacklist timeout, you're unlikely to ever get application level blacklisting.
Clearly this is not great, but its not _that_ bad.  After all, even if it were app-level blacklisted, we'd hit still the timeout and remove the bad resources from the blacklist, so that we'd need to rediscover it in future blacklists.  One of the main reasons for the app-level blacklist is to avoid lots of failures when the tasksets are _short_.  If you really want an application level blacklist which is useful across really long tasksets, then you've got to crank up your timeout.
We could change this slightly by _first_ updating the application level blacklist, and _then_ expiring failures past the timeout.  But to me that behavior seems much less intuitive, for a pretty questionable gain.
Does that make sense?  What do you think?",non_debt,-
spark,29992,review,504358081,Quick question: can we avoid catching `NullPointerException`? It's a bit odd that we catch `NullPointerException`. We could just switch to if-else I guess.,code_debt,low_quality_code
kafka,9001,review,462716040,Could be moved to `UpdateFeaturesResponse` as a utility.,architecture_debt,violation_of_modularity
arrow,8648,review,567582052,"1.  Input first,
2. Input output next.
3. Output variables.",documentation_debt,outdated_documentation
dubbo,4220,description,0,"cache host_name to avoid repeat address resolve.
XXXXX
XXXXX",non_debt,-
madlib,230,review,165737045,can we pass the desp col to these unique_string calls ?,non_debt,-
flink,13571,description,0,"# What is the purpose of the change
Currently the error thrown from `runAsync()` method will be swallowed because Flink didn't handle all throwables with `AkkaRpcActor`. Here is a temporary fix for such cases in `YarnResourceManager`.
* Use try-catch to wrap the runnable that was invoked in `runAsync()` method, and reuse the `FatalErrorHandler` to handle the error.
* Add a new unit test",design_debt,non-optimal_design
incubator-brooklyn,531,comment,78172489,"[incubator-brooklyn-pull-requests #885](https://builds.apache.org/job/incubator-brooklyn-pull-requests/885/) SUCCESS
This pull request looks good",non_debt,-
cloudstack,1960,comment,315767628,@blueorangutan test,non_debt,-
storm,2911,review,283128941,Unnecessary whitespace,code_debt,low_quality_code
beam,4956,review,179756458,value to be returned,non_debt,-
samza,230,review,124421458,"@sborya I agree it is pretty convoluted how we are doing this. However, @prateekm and I have discussed this option extensively in the past when we are working on [SAMZA-1212](https://issues.apache.org/jira/browse/SAMZA-1212). 
As I explained before , it is not fully clear how to wire-in the source of the stop and cause of the stop of a streamprocessor across the components. I don't believe it is as straightforward as passing in a `Throwable`. It could work. It becomes particularly tricky for bounded jobs or jobs that decide to stop themselves using TaskCoordinator. The approach you are suggesting is equivalent to threading a needle , where we will end up passing around the state/source of the `stop` across the components' api and callbacks. 
The alternative and more straightforward solution, imo, is to clearly define a state model for each components and persist/manage the status in the streamprocessor. That way, it will make it easier for streamprocessor to take remediation steps.
We can discuss more on this. However, this PR is simply a bug-fix. We should scrutinize the refactoring/behavior of the APIs in a separate thread/JIRA. What do you think?",design_debt,non-optimal_design
incubator-pinot,5991,review,486580153,Why move this?,non_debt,-
incubator-pinot,4903,comment,563546851,"# [Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4903?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4903?src=pr&el=continue).",non_debt,-
spark,27625,comment,588562924,retest this please,non_debt,-
nifi,1886,description,0,"- Refactoring and cleanup
- Upgraded the client library from 5.0.0 to 5.2.0",non_debt,-
iceberg,2118,review,561101471,I am fine either way. It did seem like the logic was separate but we can refactor this part out once we have UPDATEs.,non_debt,-
beam,10269,comment,562022105,Run Seed Job,non_debt,-
trafficserver,4338,comment,428848993,"Sorry, I'm not familiar with hostdb.",non_debt,-
samza,702,review,224265787,"We currently don't support specifying broadcast inputs using input descriptors. 
SAMZA-1841 will address that later. Currently these stream IDs are those that were generated using the High Level API broadcast() operator, hence the renaming to clarify that this is only for intermediate streams.
Does that answer your question?",non_debt,-
brooklyn-server,258,review,70850684,"We need it because it makes the syntax the same between the SSH sensor and effector initialisers, and it has the normal and well understood semantics of a config key?",non_debt,-
carbondata,3875,review,491976179,What is missing here?,non_debt,-
trafficcontrol,1896,comment,367375760,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/incubator-trafficcontrol-PR/1073/
Test PASSed.",non_debt,-
kafka,5881,comment,437621853,retest this please,non_debt,-
spark,16439,summary,0,[SPARK-19026]SPARK_LOCAL_DIRS(multiple directories on different disks) cannot be deleted,non_debt,-
spark,19712,review,150160911,change it to `spark.sql.hive.metastore.version`?,non_debt,-
carbondata,2410,comment,402372940,retest this please,non_debt,-
storm,259,summary,0,[STORM-500] Add Spinner when UI is loading stats from nimbus,non_debt,-
incubator-pinot,596,comment,249335065,LGTM,non_debt,-
beam,1369,review,97186486,Is it useful to have display data for this DoFn?,non_debt,-
kafka,6521,review,273258127,"Could you add a check to verify that the returned iterator is empty. Something along the lines of `assertThat(iterator.hasNext(), is(false))`?
Could you also add a test for a range query where the start key is equal to the end key? Such a unit test ensures correct behaviour for this special case.    
nit: I would rename the test to `shouldReturnEmptyIteratorForRangeQueryWithInvalidKeyRange`. Correct me, if I am wrong, but I think the empty iterator and the invalid key range are the points here, not the negative starting key. I would even change the range from (-1, 1) to (5, 3). It took me a bit to understand why (-1, 1) is an invalid range. 
These comments apply also to the unit tests below.",code_debt,low_quality_code
spark,1313,comment,49744305,ping,non_debt,-
skywalking,5760,review,515447889,When and why this exception happens?,non_debt,-
cloudstack,2211,comment,353476171,"Environment: xenserver-72 (x2), Advanced Networking with Mgmt server 7
Total time taken: 27499 seconds
Marvin logs: https://github.com/blueorangutan/acs-prs/releases/download/trillian/pr2211-t1860-xenserver-72.zip
Smoke tests completed. 65 look OK, 1 have error(s)
Only failed tests results shown below:
Test | Result | Time (s) | Test File
--- | --- | --- | ---
test_01_scale_vm | `Error` | 20.30 | test_scale_vm.py
This error may be ignored as the XenServer 7.2 version based test-hosts did not have appropriate license for scaling VMs and failed with following error: (/cc @PaulAngus  - can this be fixed?)",non_debt,-
tinkerpop,572,review,106551449,`LABELED_PATH`,non_debt,-
dubbo,2053,summary,0,bugfix init reference dead lock,non_debt,-
drill,1298,review,202718440,Yes. I have added hasNonNullValue() in createIsNullPredicate(). It solves the issue. All other tests OK.,non_debt,-
beam,1841,review,98267841,thanks for catching that.,non_debt,-
flink,12206,review,426342495,Perhaps we should rename the old `HivePartitionComputer` to `HiveRowPartitionComputer`?,non_debt,-
druid,1503,description,0,- remove ZK status path nodes for workers after they are removed.,non_debt,-
dubbo,2520,summary,0, typo: leastIndexs->leastIndexes,documentation_debt,low_quality_documentation
geode,2794,review,231716097,"After some offline conversation, the ultimate goal would be to move everything to `SingleGfshCommand`.",non_debt,-
camel-quarkus,1526,comment,672675824,"The test in
org.apache.camel.quarkus.core.CoreTest#testLookupRoutes
has a TODO as the 2nd route from RouteBuilderConfigurer is not discovered.
It uses @Produces annotation from JEE but Camel cannot discover it. Not sure how we can make this possible.
The regular RouteBuilder classes are discovered via jandex index and added during recorder magic.
I would assume a @Produces annotation from CDI/JEE would also work. But since the bean is not injected somewhere then arc may not trigger it. So maybe we need some jandex magic to discover all methods that returns a RouteBuilderConfigurer and are annotated with @Produces should then record the method, or whatever needs to be done.",test_debt,low_coverage
geode,3650,review,291302823,"Without going too deep into code, but are we sure this is will always result in localhost? What would this value be in a multi-homed environment? Does this not matter for this test?",non_debt,-
incubator-pinot,2922,comment,400827365,"@kishoreg Gotcha! We can bring in a factor. The max_qps can be calculated from min_qps multipled by this factor. The ideal situation is that all replicas are up and each broker'd use the min_qps quota. If some replicas go down, the qps rate should not exceed max_qps.",non_debt,-
kafka,1537,comment,227613567,@ewencp for review,non_debt,-
brooklyn-server,53,comment,197925605,+1 will try to review in time for 090,non_debt,-
spark,22888,comment,434208856,"and this also have no effect on timestamp values.
tested.",non_debt,-
activemq-artemis,31,comment,112568121,"[ActiveMQ-Artemis-PR-Build #436](https://builds.apache.org/job/ActiveMQ-Artemis-PR-Build/436/) SUCCESS
This pull request looks good",non_debt,-
carbondata,3352,comment,520113028,"Build Success with Spark 2.1.0, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.1/295/",non_debt,-
apisix-dashboard,979,review,543388564,hard code is not a good way,code_debt,low_quality_code
pulsar,7798,summary,0,[Issue 7787][pulsar-client-cpp] Throw std::exception types,non_debt,-
tinkerpop,1125,comment,500026710,VOTE +1,non_debt,-
usergrid,180,summary,0,[USERGRID-415] Pushing changes to make duplicate properties more readabl...,code_debt,duplicated_code
incubator-doris,3025,review,515603245,"So this method should be a pure interface, you don't need to implement in `ColumnVectorBatch`.",non_debt,-
flink,3049,comment,271823497,@uce @rmetzger I use this script build local distribution successfully. But the apache release have not try.,non_debt,-
jena,894,review,550824479,"Or maybe
I think it should eval the same way, but moving the common condition `baseUri == null` out?",non_debt,-
tinkerpop,1114,description,0,"https://issues.apache.org/jira/browse/TINKERPOP-2217
Fix potentially harmful timing issue: _writeInProgress could be observed by BeginSendingMessages to indicate that the loop in SendMessagesFromQueueAsync is still ""in flight"" while in reality, it has already exited.",non_debt,-
nifi,4824,review,576641177,Minor: unnecessary space after `put(`,code_debt,low_quality_code
iceberg,933,review,411434730,I think it makes sense to reuse the definition.,code_debt,low_quality_code
cloudstack,763,comment,136279211,"[cloudstack-pull-rats #449](https://builds.apache.org/job/cloudstack-pull-rats/449/) SUCCESS
This pull request looks good",non_debt,-
accumulo,1651,comment,733152771,"@cradal There are several comments from older iterations of this, that are possibly no longer relevant. For those conversations, if you have already incorporated the suggestions, you can mark them as ""Resolved"", so that way the UI doesn't show them as prominently. That will make it easier to track current, ongoing discussions related to the latest iteration of this PR.",non_debt,-
lucene-solr,892,summary,0,"LUCENE-8972: Add ICUTransformCharFilter, to support pre-tokenizer ICU text transformation",non_debt,-
carbondata,1464,review,149005119,"no need of this match , directly use case inside map",non_debt,-
spark,15064,comment,247511475,the title is not fixed yet,non_debt,-
spark,27165,comment,572924850,"For fixing examples and documentation, I will do it separately.",documentation_debt,low_quality_documentation
trafficserver,763,comment,231514250,"I resolved the conflicts, I think: http://paste.fedoraproject.org/389174/80381301/",non_debt,-
netbeans,2165,review,435792471,I have opened this constants up when explicitly supporting JUnit 4.,non_debt,-
ozone,1010,summary,0,HDDS-2564. Handle InterruptedException in ContainerStateMachine,non_debt,-
pulsar,1646,description,0,"The output was changed for debugging in #1613 
Revert the output back to original output.
Pulsar admin CLI output should work as before.",non_debt,-
flink,10667,comment,568953276,"Hi @JingsongLi do you have future comments on this? 
Travis is passed in my repo: https://travis-ci.org/wuchong/flink/builds/629333389",non_debt,-
drill,996,review,146815141,Could you please factor out this logic in a separate method?,code_debt,low_quality_code
fineract,888,summary,0,FINERACT-972: Ensure Tomcat is started after compiling integration tests,non_debt,-
arrow,73,review,62449714,RETURN_NOT_OK?,non_debt,-
spark,18317,review,131493469,Updated the comment accordingly.,non_debt,-
calcite,688,comment,410285418,"Yes, the condition tried to identify it is the same kind of 'is' or not...unfortunately I  wasn't taking into account 'is false'
On 3 August 2018 16:55:18 CEST, Vladimir Sitnikov <notifications@github.com> wrote:",non_debt,-
trafficserver,1002,summary,0,TS-4838: CONNECT requests get forgotten across threads.,non_debt,-
carbondata,2141,comment,379563248,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/4868/",non_debt,-
shardingsphere,6041,summary,0,Update release doc,non_debt,-
airflow,14492,review,584304954,33884891-14492 review-584304954,non_debt,-
camel,1100,review,73126080,Can you use the same license header as the other XML files in Apache Camel. They need to be similar as we have a RAT check that check the code for license alignments.,non_debt,-
spark,13967,comment,229733872,"I added a `null` map testcase and remove redundant implementation.
For the removal from `sql/functions.scala`, there is no problem to remove that. But, could you check that again?",code_debt,complex_code
beam,9731,summary,0,[BEAM-8343] Added nessesary methods to BeamSqlTable to enable support for predicate/project push-down,non_debt,-
airflow,4390,comment,450291500,"# [Codecov](https://codecov.io/gh/apache/airflow/pull/4390?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/4390?src=pr&el=continue).",non_debt,-
dubbo,1741,description,0,"Add unit test for monitor module
Close #1688 
Add test for dubbo-monitor-api
Add test for dubbo-monitor-default
CI PASS https://travis-ci.org/diamondblack/incubator-dubbo/builds/375168584",non_debt,-
spark,26461,comment,552643459,Thanks. I will update the description.,non_debt,-
spark,31059,comment,756507187,"If updating chill means updating to kryo 5 and that means changing some imports in Spark, that's probably OK. Yes, kryo ends up in the user app namespace and that's a change that could be visible, but I don't think we expect users to rely on kryo directly. Still that might be a point up for discussion.",non_debt,-
geode,1697,review,178663133,"yes, in the docker compose yml file, we specify the port in the container to open, which is then mapped to a random, open port externally. Docker (and the library we are using) handle the mapping between the ports from there.",non_debt,-
superset,11778,summary,0,fix: Download as image not working on Dashboard view,non_debt,-
trafodion,604,comment,234079094,"The code seems fine, however, it seems like it would be good to at least log the transid with the exceptions so we can better track down/correlate issues after the fact.",code_debt,low_quality_code
kafka,1206,comment,207585048,Note for reviewers: I've added a few test cases which are probably risks for future transient failures. Unfortunately it's difficult to test this without using system time since the coordinator depends on the purgatory implementation which doesn't work with MockTime. Any ideas to fix this would be appreciated.,non_debt,-
spark,18,comment,36728453,"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13004/",non_debt,-
spark,20433,comment,374113076,Change the PR title to `[SPARK-23264][SQL] Make INTERVAL keyword optional in INTERVAL clauses`?,non_debt,-
flink,6676,review,216376586,"`""An exception occurred while processing a metric message.""`?",non_debt,-
pulsar,557,comment,314549416,"@joefk @saandrews @rdhabalia Please take a look ;) 
Once we get this one in, the build will generate the first version in the `asf-site` branch in the same repo. At that point we can ask INFRA to enable the gitpubsub bridge to publish the site. We can still do all sorts of changes fixes before (and after) that.",non_debt,-
flink,5663,comment,371969548,Currently in flink connector we are depending only on aws-sdk-kinesis and not on aws-java-sdk-bundle and also don't depend on kinesisvideo. So by default the dependency on kinesisvideo is not included in the connector which means we don't have to exclude any dependencies. I also verified that there is no unwanted netty dependencies by running mvn dependency:tree. The only instance of netty is this: `[INFO] |  +- org.apache.flink:flink-shaded-netty:jar:4.0.27.Final-2.0:provided` in accordance to the value in flink-parent pom.,build_debt,over-declared_dependencies
kafka,4001,comment,333581688,Am I missing something or you're using raw types in several places now? Raw types only exist for migration compatibility purposes and should never be used in new code IMO.,code_debt,low_quality_code
incubator-heron,600,summary,0,detailed documentation of hdfs uploader,non_debt,-
spark,18912,comment,321716909,Merging in master / 2.2.,non_debt,-
arrow,4373,summary,0,ARROW-5396: [JS] Support files and streams with no record batches,non_debt,-
hbase,1465,review,406331300,line len > 100 chars,non_debt,-
geode-native,27,review,102746696,"What is this really trying to do? Maybe fix it?
C++11 style casting changes at least.",non_debt,-
druid,4162,review,111186559,It's both. Clarified.,non_debt,-
incubator-doris,5033,review,538948912,99919302-5033 review-538948912,non_debt,-
nifi,3425,comment,482711295,Maybe I explained badly. What I meant is to change the `onEnabled` method to look something like this (semi-pseudo code):,non_debt,-
trafficserver,1073,review,84208555,`const char *`.,non_debt,-
geode,3983,description,0,"Authored-by: Mark Hanson <mhanson@pivotal.io>
Draft pull request to see test results.",non_debt,-
couchdb,1765,description,0,"**NOT** ready for merge.
Opening for discussion **only**.
- Somewhat related to https://github.com/apache/couchdb/issues/1515
Every developer might have a slightly different setup. Currently in order to do this customization developers are forced to modify `dev/run` and some other files. This PR is proposing to introduce a `hooks_dir` for `dev/run` customizations. The hooks are simple python modules which export following set of functions:
- `before_setup(ctx)`
- `after_setup(ctx)`
- `before_boot_nodes(ctx)`
- `after_boot_nodes(ctx)`
- `before_startup(ctx)`
- `after_startup(ctx)`",non_debt,-
iceberg,1321,summary,0,Avro: Fix pruning columns when a logical-map array's value type is nested,non_debt,-
zookeeper,774,comment,454206605,Merged to master branch. Thanks @nkalmar !,non_debt,-
spark,21169,comment,386474214,ping @michal-databricks,non_debt,-
druid,1576,review,56544892,fixed,non_debt,-
lucene-solr,345,comment,379675351,Can we also check that it returns null on non-matching documents?,non_debt,-
spark,31119,review,559304940,"Ok, I know. To be transparent to users, how about add a new thread local property `SparkContext.SPARK_RESERVED_JOB_GROUP_ID` or `SPARK_THRIFTSERVER_JOB_GROUP_ID` to separate it.",non_debt,-
druid,10366,review,493174369,It works for me.,non_debt,-
hive,964,summary,0,HIVE-23095 ndv 70,non_debt,-
beam,13622,summary,0,[BEAM-11530] Annotated setter parameters handled wrong in schema creation,non_debt,-
incubator-mxnet,9808,description,0,"This uses a webcam or video capture device to run SSD in real time.
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here",non_debt,-
airflow,4535,summary,0,[AIRFLOW-865] - Configure FTP connection mode,non_debt,-
airflow,1488,comment,222606343,Ah. The hive-hook shouldn't be there. I think I must have included it from someone elses commit when doing a rebase. Will tidy it up and do a force push later today.,code_debt,low_quality_code
spark,10989,review,51461171,"It's a huge topic, let's talk about this offline.",non_debt,-
trafficserver,1839,comment,300626875,clang-analyzer build *successful*! https://ci.trafficserver.apache.org/job/clang-analyzer-github/750/,non_debt,-
spark,13877,description,0,"This PR groups `spark.naiveBayes`, `summary(NB)`, `predict(NB)`, and `write.ml(NB)` into a single Rd.
Manually checked generated HTML doc. See attached screenshots.",non_debt,-
spark,27767,comment,593810727,"cc @cloud-fan , please take a look, thanks!",non_debt,-
spark,24548,comment,490271765,ok to test,non_debt,-
druid,10891,summary,0,Fix maxBytesInMemory for heap overhead of all sinks and hydrants check,non_debt,-
hudi,2309,review,549436300,"+1 - Since the differences come from thee RECORD and ARRAY/MAP are containers, testing against the RECORD is sufficient is my thinking as well.",non_debt,-
skywalking,6183,review,556204702,45721011-6183 review-556204702,non_debt,-
flink,11071,review,378317546,Could we add some simple test for this bug?,non_debt,-
zookeeper,307,review,127633626,"Not necessary as such, but will be useful in debugging if some change to `getData()` method might cause this test to fail.",non_debt,-
airflow,8962,comment,643526181,"Can you please rebase the PR on the master, there were some CI issues we fixed",non_debt,-
spark,3173,comment,74416823,"Hi, this looks great! Is there a reason why sort based join is not in spark core, only in spark SQL?",non_debt,-
incubator-pinot,4397,review,307084550,please add some javadoc,documentation_debt,outdated_documentation
spark,7711,comment,125508146,"LGTM from my side, but I will wait for @JoshRosen to LGTM it since he is most knowledgeable about this.",non_debt,-
superset,8106,description,0,"Choose one
https://github.com/apache/incubator-superset/pull/7507 added `cache_value` as a keyword arg, but `cache.set` doesn't take `cache_value`, it takes `value` https://github.com/sh4nks/flask-caching/blob/master/flask_caching/backends/rediscache.py#L102
CI
@graceguo-supercat @michellethomas @serenajiang @conglei",non_debt,-
systemds,643,review,143645110,"I really don't understand these comparisons, but they are guaranteed to fail because ret is null.",non_debt,-
spark,28661,comment,635693433,"Well, I'd say it differently. A Python person may not know what a JVM stack trace means. Taking it away doesn't itself do much except shorten a big dump of output, which doesn't really simplify much. Taking away important information that perhaps someone _else_ can make sense of isn't making usage (debugging) harder. if this is only removing ""unhelpful"" stack traces from the console, I can see it.",code_debt,low_quality_code
beam,14026,summary,0,[BEAM-11377] Fix retry & cleanup issues.,non_debt,-
samza,583,review,210977975,Prefer delete over remove.,non_debt,-
nifi,914,review,75858615,"Might be able to create a utility method in the abstract class like `byte[] getRow(String row, String encoding)` since it looks PutHBaseCell and PutHBaseJson both need the same logic",code_debt,low_quality_code
calcite,1917,review,410016516,What is the difference if childRel is join?,non_debt,-
tvm,4847,comment,584462639,CSourceModule with an empty string looks to me as well. @kumasento could you do that instead of creating a dummy llvm module? Thanks.,code_debt,low_quality_code
spark,5851,review,29545347,"would be great to add implementations of all the ClientInterface with ""override""",non_debt,-
kafka,3423,review,123887166,as above,non_debt,-
spark,24043,review,292472811,We should at least add a test case for this new option,test_debt,lack_of_tests
carbondata,2494,comment,404423063,"Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/5823/",non_debt,-
incubator-weex,1114,comment,381873245,Nice job!,non_debt,-
spark,4677,review,25138912,"""be cases, in which"" --> (no comma)",non_debt,-
nifi,2391,description,0,"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
beam,13492,comment,811490297,hi @sruthiskumar - just pinging to make sure you haven't forgotten : ),non_debt,-
airflow,5041,comment,482868293,Somehow I still see python2.7 usage in the CI.,non_debt,-
spark,16047,comment,263429488,cc @hvanhovell,non_debt,-
carbondata,1805,review,162311702,added one more select query.,non_debt,-
ambari,414,summary,0,[AMBARI-23035] HDFS Balancer via Ambari fails when FIPS mode is activated on the OS,non_debt,-
activemq-artemis,31,comment,112713726,"this service registry is used by WildFly to pass instance of user-defined classes (such as transformers, interceptors) to Artemis.
WildFly uses a modular class loader so it's its responsibility to instantiate these classes from the correct module. The ServiceRegistry is really just an object repository since Artemis configuration does not allow to pass instances but only class names",non_debt,-
incubator-mxnet,14475,comment,474913650,"@mxnet-label-bot add[Test, pr-awaiting-merge]",non_debt,-
kafka,4461,description,0,2211243-4461 description-0,non_debt,-
dubbo,413,summary,0,Collection,non_debt,-
spark,25905,comment,534375300,This PR's description?,non_debt,-
kafka,10383,description,0,"As the title. When using zk client, the query topicDescribe are output in order of topic names. Similarly, when using adminClient, they are also output in order of topic names.",non_debt,-
spark,25626,review,320607427,"We should normalize the attribute name. Let's say a table has a column `id`, and users write `UPDATE ... ID=1`, then the `AttributeReference` will be named `ID` instead of `id`.
The same thing should also apply to filters. You can take a look at `DataSourceStrategy.normalizeFilters` and see where we call it.",non_debt,-
cloudstack,3828,comment,593394720,@blueorangutan package,non_debt,-
flink,10146,review,351864748,we should probably mention here that it `CPU_CORES` has to be an integer in case of yarn and check it as well in `YarnResourceManager`.,non_debt,-
spark,15272,description,0,"Jira : https://issues.apache.org/jira/browse/SPARK-17698
`ExtractEquiJoinKeys` is incorrectly using filter predicates as the join condition for joins. `canEvaluate` [0] tries to see if the an `Expression` can be evaluated using output of a given `Plan`. In case of filter predicates (eg. `a.id='1'`), the `Expression` passed for the right hand side (ie. '1' ) is a `Literal` which does not have any attribute references. Thus `expr.references` is an empty set which theoretically is a subset of any set. This leads to `canEvaluate` returning `true` and `a.id='1'` is treated as a join predicate. While this does not lead to incorrect results but in case of bucketed + sorted tables, we might miss out on avoiding un-necessary shuffle + sort. See example below:
[0] : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L91
eg.
BEFORE: This is doing shuffle + sort over table scan outputs which is not needed as both tables are bucketed and sorted on the same columns and have same number of buckets. This should be a single stage job.
AFTER :
- Added a new test case for this scenario : `SPARK-17698 Join predicates should not contain filter clauses`
- Ran all the tests in `BucketedReadSuite`",code_debt,complex_code
druid,1983,comment,160717532,@pjain1 you must be running using an old version of the mysql connector code. The current code does not have any `SHOW VARIABLES` statements.,non_debt,-
airflow,2816,description,0,"Dear Airflow maintainers,
    - https://issues.apache.org/jira/browse/AIRFLOW-1848
Dataflow Python operator takes in a filename without `.py` extension, which was incorrectly documented previously.
N/A, just a doc change.
    2. Subject is limited to 50 characters
    3. Subject does not end with a period
    4. Subject uses the imperative mood (""add"", not ""adding"")
    5. Body wraps at 72 characters
    6. Body explains ""what"" and ""why"", not ""how""",documentation_debt,low_quality_documentation
beam,11344,comment,610881306,Run Load Tests Java ParDo Dataflow Batch,non_debt,-
arrow,167,description,0,"When third party env vars *_HOME are not present, use cmake's
ExternalProject to fetch and build them.  When those vars are present,
we just use them.",non_debt,-
kafka,3152,comment,304934017,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4538/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
flink,12218,summary,0,[FLINK-16160][table] Fix proctime()/rowtime() doesn't work for TableE‚Ä¶,non_debt,-
airflow,697,comment,192552103,Shouldn't the ssh connection type also be added to www/views.py -> ConnectionModelView.form_choices? Would be awesome if these would somehow be configurable through plugins as well.,non_debt,-
gobblin,435,review,44064138,ditto,non_debt,-
tinkerpop,1412,comment,812726281,"I made some updates to the docs. It's getting late here so I'll merge this sometime tomorrow.
VOTE +1",non_debt,-
beam,12240,comment,658370577,Have you run beamimport to test this PR internally?,non_debt,-
trafficserver,369,description,0,This allows you to drop the query string when doing the consistent hash in parent selection. Default behavior is the same. Add `qstring=ignore` to the parent.config line to enable.,non_debt,-
activemq-artemis,2741,comment,524821666,"I did not realize your -1, and I confused this wait with a similar change on FDs.
I'm reverting it.
@brusdev  can you address the issue with a new PR? a restart of the broker should remove any previous messages that were not deleted.",non_debt,-
openwhisk,2081,review,108746758,We could skip the push at the end of the build if we build on Dockerhub directly but attaching our repo via webhook. (Doesn't work ootb with our build processes though),non_debt,-
carbondata,777,comment,293853193,@kunal642 In AlterTableCommands.scala move all the lock acquiring logic inside try block,non_debt,-
airflow,11270,comment,703367324,@potiuk @mik-laj,non_debt,-
incubator-doris,764,review,268427310,I clarify the interface of AlphaRowsetReader and provide two private function _union_block and _merge_block in response to next_block.,non_debt,-
beam,1278,review,86889887,"Fair. Added that. Still not perfect, but at least it will be perfect in pipelines without bundle failures, which is, hopefully, most pipelines.",design_debt,non-optimal_design
nifi,4599,review,539711164,Otherwise LGTM +1,non_debt,-
shardingsphere,3879,summary,0,fix proxy hang up while throwing a customize SQLException,non_debt,-
storm,1154,comment,251518436,This has been fixed elsewhere.,non_debt,-
spark,14712,review,75569715,"I'll modify unit tests based on your comments, thanks!",non_debt,-
incubator-pinot,5823,summary,0,[TE] enchance anomaly api to propagate feedback,non_debt,-
ambari,3146,summary,0,Bump jetty.version from 9.3.19.v20170502 to 9.4.24.v20191120 in /contrib/views,non_debt,-
skywalking,4337,comment,584513941,"Is the agent compiling test failure from one to another because of network issue? The CI passed, so I assume the compiling should be fine.",non_debt,-
calcite,1278,comment,506493736,"Not nothing. Human diligence prevents it.
Even if there is a regression test someone could remove it. But we assume good faith, so we assume that doesn't happen.",non_debt,-
flink,8359,review,282768056,"I think it again, we do not need check size of aggregates in this method. and we can use `isTableAggFunctionCall` when need. such as `isTableAggFunctionCall(expr)`, see blow comments. :)",non_debt,-
tvm,4923,summary,0,[LLVM] Fix build breaks with StringRef changes,non_debt,-
flink,13056,review,465675256,,non_debt,-
spark,15901,comment,261837764,"thanks, merging to master/2.1!",non_debt,-
carbondata,1923,comment,362808650,LGTM,non_debt,-
nifi,1103,review,81982607,"""have you ensured""",non_debt,-
kafka,6032,comment,448645801,"Hi @guozhangwang, thanks for reviewing. I will check again later. And for the unit test, I will do it in a follow-up one PR.",non_debt,-
spark,9134,review,42121597,Do we need to define another exit status to distinguish with the class not found exception?,non_debt,-
superset,3733,description,0,"This [PR](https://github.com/apache/incubator-superset/pull/3722) added a check for `isXAxisString = ['dist_bar', 'box_plot'].includes(vizType) >= 0` but for a different viz type (like line) includes will evaluate to false and `false >= 0` will evaluate to true (so the formatter was not getting set).
Changing to use indexOf.",non_debt,-
arrow,9358,comment,772311969,Benchmark updated to include integers with `wide` and `narrow` value range.,non_debt,-
streams,47,review,14477763,"Needs to be ""Point"".  See http://geojson.org/geojson-spec.html#point",non_debt,-
spark,32011,review,604739239,"It seems there are many requests for that, but GA still does not support it: https://github.community/t/support-for-yaml-anchors/16128",non_debt,-
flink,1069,summary,0,[FLINK-1725]- New Partitioner for better load balancing for skewed data,non_debt,-
beam,11957,comment,644318238,"Because of the problems mentioned above, I did not change the implementation of `zetaSqlTypeToBeamFieldType`, instead I added another overload of it which does not depend on Beam `fieldType`. It will allow compliance test driver to call it (cl/316152825).
@apilloud Please check if this PR still LGTY.",non_debt,-
spark,9422,comment,153500624,"For stddev/variance, it's clear that using ImerativeAggregate will be faster. Then we don't have case that need the MutableProject to be atomic, should we revert this PR? cc @marmbrus @yhuai",non_debt,-
hbase,1849,summary,0,Revert inadvertent merge commit and re-apply HBASE-24280 ADDENDUM to branch-2,non_debt,-
incubator-pinot,3832,summary,0,[TE] Extend anomalies endpoint to fetch by metric/dataset and true an‚Ä¶,non_debt,-
beam,1981,comment,279164853,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7325/
--none--",non_debt,-
cloudstack,3500,comment,514738510,@rhtyd a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests,non_debt,-
tomee,763,review,585708625,"Thx @rmannibucau  for the follow up!
Yes - testing with `contains` is a simplification and neglects some weirdo cases... and yes: it would require char, position + offsets to properly test and replace the closing and opening brackets. 
Thus, I like the idea of using `PropertiesLookup#lookup` and returning the key there. The parsing ""magic"" (char, position, offsets) is already done in the substitutor, so I guess fallbacking there is a good option. 
I will take a look at it.",non_debt,-
spark,895,review,13469626,This still says log4j.,non_debt,-
flink,9977,review,338932359,"Add a UT test for these coders. For example, we can add a test in `pyflink/fn_execution/tests`.",test_debt,lack_of_tests
spark,17746,comment,297098093,LGTM. Merged into master and branch 2.2,non_debt,-
kafka,401,description,0,‚Ä¶ info from the wiki,non_debt,-
ignite,4639,review,215541102,"Can we still perhaps do a warning and `url = new URL(String.format(DOWNLOAD_URL_PATTERN, ver.replace(""-incubating"", """"), LAST_KNOWN_VERSION));` here?",non_debt,-
kafka,8065,review,377929200,"`empty partitions` -> should we say `empty topics` ?
Just to clarify that some partitions from _different_ topics are empty?",non_debt,-
orc,380,summary,0,ORC-490 Refactor SARG applier,non_debt,-
flink,15051,description,0,"The current `KafkaRecordDesrializer` has the following problems:
- Missing an `open()` method with context for serialization and deserialization.
The purpose of the change is to fix the above issues. 
- Renamed `KafkaRecordDeserializer` to `KafkaRecordDeserializationSchema` to follow the naming convention.
- Added the method `getUserCodeClassLoader()` to `SourceReaderContext` so the `SourceReader` implementation can construct the `SerializationDeserializationContext`.
- Added methods `valueOnly(...)` and `open(..)` in the `KafkaRecordDeserializationSchema` interface to enable the reuse of the `DeserializationSchema` and `KafkaDeserializationSchema`.
- Added the method `setValueOnlyDeserializer(...)` in `KafkaSourceBuilder` class to make it easy to set value-only deserializer.
- Added unit tests in `TestingDeserializationContext` and `KafkaRecordDeserializationSchemaTest`.
- Added tests in `KafkaRecordDeserializationSchemaTest` to verify the changes made in KafkaRecordDeserializationSchema",code_debt,low_quality_code
beam,6142,review,207951355,"@DariuszAniszewski Sorry for the unclear explanation of this fix. This line basically fix #6074. The tar file will be broken if we build it outside of `../sdks/python`, which causes worker crash looping during startup and timeout in Jenkins job. You can get more details if you check the tar file from staging location.",non_debt,-
arrow,6005,review,358343712,Indeed looks incorrect,non_debt,-
cloudstack,1611,comment,319500833,@rhtyd a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests,non_debt,-
beam,5229,description,0,There are a number of CVEs released fixed in the recent Tika 1.18 release - we should upgrade.,non_debt,-
druid,3928,review,106563776,"formatting, new lines would be easier to read",code_debt,low_quality_code
camel,3381,comment,561470632,"Thanks for keep working on this.
Thanks so what I think is really taking people with some surprise is that it registers Camel routes into the **same single camel context** from any bundles. The point of Apache Karaf was to be like an app server where each bundle is isolated. So this goes against this practice. This should be documented much much more clearly. And also it lacks features with the ease of use how to configure camel context itself (you end up with its defaults) and how would people do dependency injections for beans etc. 
Also it should be renamed to `camel-osgi-activator`, and moved to components (as its not a core piece, eg not used by other osgi like osgi blueprint which is the main osgi support in Camel).",documentation_debt,low_quality_documentation
incubator-mxnet,14295,comment,468806748,It is good to go once the CI tests pass,non_debt,-
flink,11047,review,379996777,Can we introduce a `TableSinkFactoryContextImpl` class to reduce so many anonymous classes?,code_debt,low_quality_code
flink,14955,review,578192965,maybe check whether the result is ResourceProfile.ZERO?,non_debt,-
daffodil,289,review,346101694,"Need to explain this algorithm more thoroughly around what is done with sequences specifically.
Also unit tests focusing specifically on this algorithm, to strongly characterize proper behavior and insure full coverage.",test_debt,low_coverage
trafficserver,5524,description,0,"This is an alternative to #5473.
I added an extra bit of code (which, unfortunately, would break the actual code if executed) to convince clang analyzer to shut up. I fiddled with this far more than I should and was unable to find any sort of assert
that would suppress the false positive. I _think_ the problem is the analyzer knows `x->_next == n` because of the previous code path, but doesn't realize `x->_next` gets updated when `n` is removed (via standard linked list removal). It's also annoying that the analyzer complains about the _assignment_ of a stale pointer as if it were a dereference. While dangerous, the assignment of `n = next(x)` is not actually a use after free. If I replace that with the content of the `next` function, however, the use after free moves to the next loop iteration, which would be correct if there was an actual use after free.
I prefer this because if the code is changed solely to make clang analyzer shut up, that should be obvious to a reader of the code, rather than puzzling over obtusely different implementation styles. It also means if we ever discuss this with the LLVM group, we can easily find the places in the code where the clang analyzer messes up.",non_debt,-
trafficserver,1744,description,0,Simple fixup.,non_debt,-
pulsar,9751,summary,0,[Issue 9725][Transaction] - Fix deleteTransactionMarker memory leak,design_debt,non-optimal_design
flink,15200,review,603203743,"yes, that would also be fine.",non_debt,-
airflow,143,description,0,"@mistercrunch 
This should fix https://github.com/airbnb/airflow/issues/142
This operator is still not as robust in its parsing as the key sensor, but prefixes are more tricky.",non_debt,-
carbondata,3663,comment,597691644,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/2430/",non_debt,-
kafka,158,review,38986690,"just to circle back here - I asked @benstopford to remove the test and doc and do it in a new PR, since they are a bit out of context here.",documentation_debt,outdated_documentation
incubator-heron,1584,comment,265843582,@billonahill Thanks Bill. The code LGTM.,non_debt,-
incubator-mxnet,13549,review,246983078,indent.,code_debt,low_quality_code
spark,16582,comment,272952834,"The code changes looks good to me, but my experience in code working with SSL is still small so someone with more experience should also double-check.",non_debt,-
beam,729,comment,267910051,This PR is covered in https://github.com/apache/incubator-beam/pull/1400.,non_debt,-
trafficcontrol,4821,description,0,"There are a couple of CDN-in-a-Box services that take much longer to build than they ought to because of how long it takes to send the build context to the daemon - but don't actually need a context of that size. This reduces the services' contexts to only what is necessary for them to build.
- CDN in a Box
Build CDN-in-a-Box",code_debt,slow_algorithm
tvm,1085,description,0,@eqy @merrymercy,non_debt,-
spark,20282,comment,358401766,"The auto-merge in 70917a5 somehow reverted part of this PR, causing the newly added test to fail.",non_debt,-
shardingsphere,6187,review,448777753,"The parameter `identifier` cannot correctly describe the meaning of orchestration instance unique identification,  my suggestion is `instanceId`, and i think it's better to define it as `String` type.",non_debt,-
storm,1472,review,68023943,"I feel this line misses behavior ""executing command"".
I'd like to have this line changed like 
`Archives resources to jar and uploads jar to Nimbus, and executes following arguments on ""local"". Useful for non JVM languages.`",non_debt,-
spark,15009,review,106230108,nit: move to previous line,code_debt,low_quality_code
tvm,7406,summary,0,[BYOC][Verilator] Refactor Verilator runtime,non_debt,-
beam,12646,comment,681006444,"Thank you, Pablo and apologies.  I was trying to figure out how this could be prevented in the future.  Is there anything this PR specifically that led to breaking whitespace precommit?  I was looking into the output of the pre-commit tests to see if there was anything related to the content of the PR specifically.",non_debt,-
activemq-artemis,1466,description,0,"Incorrect ordering of replication packets may happen because of
useExecutor parameter in the sendReplicatePacket method.
ReplicationStartSyncMessage packets are sent as first, but they are sent
with useExecutor=true. Although ReplicationSyncFileMessage packets are
sent after ReplicationStartSyncMessage packets, they are sent with
useExecutor=false. So sending of ReplicationStartSyncMessage packets is
scheduled to executor and there is no guarantee when the task will be
executed, whereas ReplicationStartSyncMessage packets are sent
immediately.
The solution is to wait for an ack for ReplicationStartSyncMessages.",non_debt,-
flink,12188,review,426150425,"I think the tool class should be stateless, or we make a common data structure there with pre-registered tables there only for testing.",non_debt,-
bookkeeper,2281,summary,0,[bookie-ledger-recovery] Fix bookie recovery stuck even with enough ack-quorum response,non_debt,-
flink,7123,review,236347738,missing space **...setup (data ...**,code_debt,low_quality_code
incubator-heron,1728,comment,281558138,"@objmagic - Thanks for reviewing. I'm agree with you. At first, I think putting `ByteAmountUnit` inside of `ByteAmount` is a better approach and using enum to instead static `MB` and `GB` inside of `ByteAmount` seems nice too. But It might trigger a large refactor in `ByteAmount`. So, I decided to put `ByteAmountUnit` outside of `ByteAmount` temporarily.",design_debt,non-optimal_design
spark,848,review,12921552,"What's difference between `spark.yarn.user.classpath.first` and `spark.files.userClassPathFirst`? For me, it seems to be the same thing with two different configuration.",non_debt,-
netbeans,1197,comment,488337457,@matthiasblaesing are you satisfied with the changes now?,non_debt,-
kafka,6837,summary,0,MINOR: fix integration tests,non_debt,-
gobblin,2015,review,129176332,"The configuration options are not symmetric here, which can lead to confusion and does not allow all override possibilities. The code is ignoring the config store whitelist tag when a job-level whitelist is specified, but it is adding the job-level blacklist to the tag-based blacklist.",design_debt,non-optimal_design
hadoop,802,review,289864413,"thanks, I'll resolve the conversations as I resolve these in my branch and don't have any questions",non_debt,-
spark,186,comment,39637660,"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13795/",non_debt,-
beam,874,comment,243063941,Looks good @jbonofre!,non_debt,-
storm,1595,review,72548331,"@ptgoetz No worries. I didn't take your comments in that way. It was just my criticism of **current metrics API**.
Storm community discussed about metrics several times, and the only one that I can get is the needs of new metrics feature. 
This is a latest discussion which also contains requirements of metrics feature. I don't think we should address all of them, but addressing most of them would be great.
http://mail-archives.us.apache.org/mod_mbox/storm-dev/201605.mbox/%3CCAF5108jB=4aL0Z1qMMnEe7U4Yx_9TZjD11BJYhKtkTtVJoTdhQ@mail.gmail.com%3E
Adding fixes to current metrics seems not competitive to recent metrics from others stream frameworks, and even JStorm, which is the thing we would eventually evaluate (I already did a first pass and looks promising) and port if it's better. This patch only has a value with current metrics, and I don't want to break backward compatibility because of will-be-deprecated-and-dropped feature.",non_debt,-
accumulo,1497,summary,0,Fixes #1420 - Made fields in AESCryptoService inner class constructor‚Ä¶,non_debt,-
kafka,6475,description,0,"Fix flaky test cases in `WorkerTest` by mocking the `ExecutorService` in `Worker`.  Previously, when using a real thread pool executor, the task may or may not have been run by the executor until the end of the test.
Related JIRA issues:
Ran all tests (`./gradlew test`).
Ran unit tests in `connect/runtime` repeatedly.",test_debt,flaky_test
incubator-pinot,5708,review,456656111,"This may sound like an over-optimization but adding type specific GroupKeyGenerator has the potential to add overhead for runtime dispatch which Java doesn't handle well.
What is the advantage of having a type specific next method like the following for Double
v/s the existing generic method
Both are doing toString()",non_debt,-
spark,31818,comment,804210048,Thanks all for the review.,non_debt,-
flink,5977,review,188205000,"I'm slightly leaning towards Stephan's suggestion, which I also agree is the better solution for this case.
It might be ok to have this as a ""hidden"" API for now anyways, since 1) it is marked `@PublicEvolving`, and 2) the API was added in quite a short timeframe.
If we want this fix in 1.5, I wouldn't suggest ""fully"" exposing it.",non_debt,-
bookkeeper,888,comment,352890209,"We assume that, for a given version, the notice and license files will not change, which is a safe assumption to make.
I'm dubious as to whether it will be possible to pull in NOTICE files automatically. We would have to pull in every NOTICE file, which isn't really necessary. And then someone would have to check the contents of the pulled in NOTICE file to ensure everything is ok.
If we make the pulling of licenses automatic, then they will only ever be checked at release time. At release time, all dependencies need to be checked, and when there's so much to check, people are likely to just give it a quick glance, and +1 it, without actually checking each dependency.
I would prefer that the work in manually checking dependencies occurs as part of the development process, each time we update a dependency. At this time, there will be a smaller subset of the dependencies changing, so it can be reviewed more carefully. The submitter will be able to take their time with it, and the reviewer will be able to give each dependency their full attention. Once a license/notice has been updated for a version of the dependency, it shouldn't need to be looked at again (as licenses/notices don't change within a single version).",design_debt,non-optimal_design
thrift,1318,comment,402135908,Any thoughts on moving this one forward?,non_debt,-
carbondata,2804,comment,434216599,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/1148/",non_debt,-
spark,4229,comment,127370989,@tdas please have a look.,non_debt,-
drill,283,review,46315590,add the parent allocator in the message to help with debugging?,non_debt,-
kafka,7877,review,362646654,Done,non_debt,-
trafficserver,1856,comment,300662944,FreeBSD11 build *failed*! https://ci.trafficserver.apache.org/job/freebsd-github/2207/,non_debt,-
drill,2046,summary,0,DRILL-7680: Place UDFs before plugins in contrib,non_debt,-
cloudstack,4490,description,0,"Fixes https://github.com/apache/cloudstack/issues/4481
TODO",requirement_debt,requirement_partially_implemented
cloudstack,2058,review,139283005,"Fix indentations if you want to remove try-catch, also add a test?",code_debt,low_quality_code
carbondata,3538,comment,573322241,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1602/",non_debt,-
airflow,3572,review,201043776,will enable source maps. good point!!,non_debt,-
kafka,6645,review,282840393,Add test for out-of-order data to make sure we set the correct timestamp,non_debt,-
flink,3374,comment,285864664,"Hi @StephanEwen 
Thank you for the review. I have rebased and done those improvements in your suggestion.",non_debt,-
trafficserver,2314,review,130437336,As it is ... it's not in the (new) first commit and I'll leave it out of this PR until jemalloc shows it cannot do it.,non_debt,-
netbeans,2324,review,511329375,Using `Logger` would be more standard. See [Logging in NetBeans](http://bits.netbeans.org/dev/javadoc/org-openide-util/org/openide/util/doc-files/logging.html) document.,code_debt,low_quality_code
flink,8809,review,296590815,It's better to be done in another pr.,non_debt,-
shardingsphere,1162,description,0,49876476-1162 description-0,non_debt,-
incubator-brooklyn,83,comment,49760955,"A few failing tests, and a couple of (very minor) comments, but other than that, looks good (once tests are passing again :-))",documentation_debt,low_quality_documentation
beam,13799,comment,768305516,@aromanenko-dev I didn't mean to ignore your comment about jira prefixes on commit messages. I just saw it late. Apologies.  Hope I can get a pass this time!,non_debt,-
nifi-minifi-cpp,146,review,144844644,"We need to call out the specific items and not just include the boilerplate message by the source LICENSE.
In the case of those exclusions we must note that ""the project includes <project/source> which is available under <license>"" and the associated copyright.  As a reference, scope out the other items in this LICENSE to see how they are handled. 
We should not have the caveats listed by the source but interpret them and include the appropriate clauses for those items which we do include.",non_debt,-
cloudstack,1287,summary,0,SecurityGroupRulesCmd code cleanup,non_debt,-
tvm,6797,review,517257511,"An alternative way, that I think it is equivalent, with less branches and return statements in the same function. Feel free to keep your version if you want :)",non_debt,-
spark,20278,comment,358159762,retest this please,non_debt,-
spark,18036,comment,302795517,"Nice catch, I'm not sure what @kishorvpatil 's original reason for blocking it from search like that was but this is good",non_debt,-
pulsar,8133,comment,698903199,/pulsarbot run-failure-checks,non_debt,-
incubator-weex,1446,description,0,78186814-1446 description-0,non_debt,-
kafka,2438,comment,275340158,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1226/
Test PASSed (JDK 8 and Scala 2.11).",non_debt,-
spark,16150,comment,265005796,cc @felixcheung,non_debt,-
ignite,8048,review,480182091,Bad formatting.,code_debt,low_quality_code
spark,25616,description,0,"Replaces some incorrect usage of `new Configuration()` as it will load default configs defined in Hadoop
Unexpected config could be accessed instead of the expected config, see SPARK-28203 for example
No.
Existed tests.",non_debt,-
carbondata,1225,comment,320290275,"SDV Build Success with Spark 2.1, Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/106/",non_debt,-
zeppelin,2120,comment,288543338,"Looking at the discussion and the different issues I see what you mean @1ambda. We are actually having two issues:
1. Local changes are overwritten when the notebook gets updates from the server (e.g. someone renames a note)
2. Local changes are not propagated on a regular basis (and you risk bigger conflicts when editing simultaneously)
The first point is addressed by #2176 #2177 more than this PR, because even when saving on losing focus you might risk losing local changes if you didn't lose the focus and someone renames, correct?
The second point is addressed in a way here but as mentioned in one of the other discussions we should really move towards real time updates where every keystroke gets transmitted (or at least while typing maybe after a few seconds) and you can see the cursor of the other users in your paragraph (similar to Google Docs).
Does it make sense?",non_debt,-
spark,17765,review,113583392,"yes, this seems fine",non_debt,-
incubator-doris,2050,comment,555584339,LGTM,non_debt,-
pulsar,2100,comment,402925739,retest this please,non_debt,-
trafficcontrol,365,comment,286793219,"This was merged, closing",non_debt,-
flink,4426,description,0,"Remove unnecessary include-elasticsearch5 profile since its activation condition (usage of jdk8) is always true.
Run any mvn command in `flink` or `flink-connectors` and check that the reactor includes ES5.",code_debt,dead_code
beam,11610,review,425548953,50904245-11610 review-425548953,non_debt,-
spark,25119,review,304750383,Done.,non_debt,-
nutch,184,comment,302673149,"Hi @lewismc Thanks for your effort to test my patch and I am very glad that everything worked perfectly and the PR is more likely to be merged :D I've added parameterized logging where possible and pushed the changes. However, [here](https://github.com/apache/nutch/pull/184/files#diff-ed9e59402a5d1e6a0f39361b2429e580R97) and [here](https://github.com/apache/nutch/pull/184/files#diff-ed9e59402a5d1e6a0f39361b2429e580R91), the log-messages need to be created to throw `RuntimeException` no matter log is enabled or not. So I used `String.format` here.",non_debt,-
beam,7956,comment,479834196,"I like the proposed name by @chamikaramj but I'd prefer @iemejia idea to keep a consistency with naming. 
So, my **+1** to `withOutputParallelization()` and `withOutputParallelization(boolean)` where default value is `true`.",non_debt,-
beam,11203,review,399469457,"Some documentation questions:
Is this meant to completely replace the artifacts supplied in replacements?
What if a user doesn't pass in all the original artifacts?
If something can't be ""resolved"" to something simpler, does it still appear in the output?",non_debt,-
spark,25628,review,319461063,Seems like it doesn't support special characters in the column name now. Can we keep the support?,requirement_debt,requirement_partially_implemented
spark,18974,description,0,"Update Arrow version to 0.6.0
Here is a [release note](http://arrow.apache.org/release/0.6.0.html).
Existing tests",non_debt,-
brooklyn-server,835,review,142912453,"TL;DR: I agree this is fine. Below are some notes from digging around in the synchronization code.
Looking at the other synchronization blocks, `emitInternal` will synchronize on `EntityManagementSupport` instance (when it calls `getSubscriptionContext()`. It will then call `publish` which will synchronize on `LocalSubscriptionManager` instance (in `getSubscriptionsForEntitySensor`).
However, I think we can trust both the `EntityManagementSupport` and the `LocalSubscriptionManager` to not call out to alien code while holding a lock on itself. Therefore we should be safe in that respect.
The code in `AbstractEntity` and `AbstractGroupImpl` looks a bit scary, where it first gets the lock on this `values` object and then on either `abstractGroup.members` or `abstractEntity.children` (but it's kind-of understandable why it does that and how that pattern avoids it; it would just be nicer if instead we didn't call out to alient code while holding the lock on `values` - but that's a bigger discussion than for this PR):",code_debt,low_quality_code
ambari,1151,description,0," 21526 passing (30s)
  48 pending",non_debt,-
spark,448,review,11813558,"Yeah, you are going to end up getting the same thing.  I'd say we drop this one and leave the other.  Right now it probably doesn't matter, but the other one is lazy and gives the optimizer a chance to possibly improve things before actually executing.",code_debt,low_quality_code
spark,3659,comment,66415145,I'm merging this into `master` and `branch-1.2`.  Thanks!,non_debt,-
nifi,3850,description,0,"Thank you for submitting a contribution to Apache NiFi.
Adds a 'replace first' and 'replace all' strategy to the ReplaceText processor.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
airflow,1956,comment,268910016,@criccomini,non_debt,-
tinkerpop,98,summary,0,Made correction to fix TINKERPOP3-855.,non_debt,-
incubator-mxnet,13503,comment,443465456,"@mxnet-label-bot add [MKLDNN, pr-awaiting-review]",non_debt,-
incubator-mxnet,5886,review,112565730,"suggest changing ""a L2 norm"" to ""the L2 norm""",non_debt,-
incubator-pinot,5200,review,408486325,"The default dataset delay logic will apply when we automatically onboard all the alerts for SLA detection. The yaml translation logic needs to take care of it. 
Here, we expect that the sla is defined by the user and we will strictly adhere to it.",non_debt,-
tvm,6449,comment,692921717,@masahi It looks like certain recent change causes this error. I'm investigating.,non_debt,-
storm,1958,comment,282003202,+1,non_debt,-
spark,21882,review,205935245,"Before this change, `Expected Array(Array(1)), but got Array() Wrong query result`",non_debt,-
flink,13033,comment,721025672,"Motivation: to have possibility to run this code on ververica platform when version 1.12 will be available.
I can add dependency in my job. 
But this will also probably need some shading as I will have two different version of confluent libraries.",non_debt,-
apisix,3224,review,559854280,"@membphis 
32 seconds",non_debt,-
flink,6178,review,196555570,maybe more descriptive name than `get`.,code_debt,low_quality_code
kafka,6977,review,298727205,This function seems not used?,code_debt,dead_code
ambari,183,review,163852609,there is an isNotBlank before calling that method,non_debt,-
pulsar,7786,comment,674414727,"I try to run the same unit test command on local
without any failed test case.
But online test failed after retry 3 times and each time failed with different test cases:",non_debt,-
tvm,4179,review,337872859,I think we can just use topi.identity op here. No need to write the shape func?,non_debt,-
tvm,7153,review,581353108,"TVMGetLastError may already contain a backtrace, so this would double it. (This was a preexisting issue).",non_debt,-
flink,4851,review,145917513,"ad 1. - that's why I have used `StateAssignmentOperation.operatorSubtaskStateFrom`, to share this logic. I didn't want to use all of the `StateAssignmentOperation`, because it's constructor is really annoying to fulfil.
ad 2. - that's a valid concern, however writing ITCases for this might be also an overkill. And wouldn't it be to necessary to test it against every state backend, to make sure that there are no introduced quirks during (de)serialisation?",code_debt,low_quality_code
cloudstack,29,summary,0,"CLOUDSTACK-7758: Fixed although api calls are failing, event tab shows them as successful",non_debt,-
nifi,1849,summary,0,[NIFI-3943] align combo option item toolips to hover closer to the icon,non_debt,-
airflow,5585,review,303217602,Fix pushed.,non_debt,-
incubator-mxnet,2380,comment,225808811,"Each of the commits is independent for each operator, so I prefer keeping them atomic",non_debt,-
spark,10162,comment,162268697,"it seems that there is an issue for python, I'll fix the signature soon!
I'd appreciate your feedback on the signature and the logic @davies @shivaram @rxin @sun-rui 
Thanks,
Narine",non_debt,-
zeppelin,954,comment,223481561,@ralphge Thanks for the contribution. I've tested and worked well. And CI failure is a temporary issue. Could you please re-trigger CI? It's OK to close and re-open this PR.,non_debt,-
beam,2974,comment,300015697,LGTM; please self-merge to the release branch.,non_debt,-
storm,1101,comment,183528417,+1 again,non_debt,-
beam,8162,comment,477866946,"Reading the code, I wondered if it might be pervasive in most/all runners. Is it a release blocker though? This bug has existed presumably for many releases.",non_debt,-
beam,12065,comment,650215296,Run Python2_PVR_Flink PreCommit,non_debt,-
kafka,1627,comment,235345722,The build failure is puzzling me. The line that it's complaining about seems correct and it builds locally fine. Am I missing something?,non_debt,-
spark,16706,review,98121245,"nit: remove ""of bytes"".",code_debt,low_quality_code
arrow,9610,review,588816147,The `r` and `nightly` groups are defined with wildcards that will match the name I used for this test.,non_debt,-
flink,9766,review,329334111,`__all__ = ['RowCoder']`,non_debt,-
tvm,7021,comment,797529527,"@ANSHUMAN87 Hi, thanks for trying this! Could you be more specific about your setting? The adaptive evaluator works in the occasion when the repeat/number of the measure_option is a big number (like 500), and according to our experiment results in the paper, the search efficiency outperforms than the base AutoTVM in the Transformer encoder tuning case.",code_debt,slow_algorithm
trafficcontrol,5570,comment,785250722,"Ah, I think I know the issue. Python ORT is setting the version to 2.0, which is likely causing an exception since 2.0 is ""not supported"".",non_debt,-
geode,3840,review,307057404,GeodeAwaitalities' default wait time is 5 min.,non_debt,-
spark,3269,review,20599886,"Ah yes, good catch.",non_debt,-
spark,29001,comment,653952590,Retest this please,non_debt,-
airflow,5343,review,288918215,"This overall feels like it makes the code cleaner, but I do wonder if it's useful to keep this config setting (so that people can tweak the behaviour of the Airflow without _having_ to go in to the UI and change behaviours.
If we do decide to delete these two config values we should add a note to UPDATING.md about it.",code_debt,low_quality_code
reef,536,review,41340326,Done,non_debt,-
helix,159,review,177534964,"it is backcompatible, the code reads """"helix.callbackhandler.isAsyncBatchModeEnabled"""" first, if it is not set, it reads ""isAsyncBatchModeEnabled"" instead (see the lines 199-202 following)",non_debt,-
ozone,597,review,383576726,"We would need this ""totalCount"" as part of DatanodeMetadata response to support server side pagination in the future.",non_debt,-
tvm,7078,description,0,"Fix using `num_workers` in omp
@hlu1 please kindly review",non_debt,-
nifi,24,review,24779501,needs a file-level license,non_debt,-
reef,311,comment,124298703,"[Reef-pull-request-ubuntu #143](https://builds.apache.org/job/Reef-pull-request-ubuntu/143/) SUCCESS
This pull request looks good",non_debt,-
trafodion,1347,comment,351943682,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2258/,non_debt,-
spark,10744,review,50049377,Updated the title,non_debt,-
tinkerpop,616,comment,304925239,@dalaro when i build this branch i get some errors (they pop up intermittently in integration tests and not in all environments) that i long ago thought i took care of. is it possible that the branch in your fork needs to be rebased on the latest stuff from tp32?,non_debt,-
spark,8791,comment,142457862,core parts LGTM,non_debt,-
airflow,7119,summary,0,[AIRFLOW-5840] Add operator extra link to external task sensor,non_debt,-
pulsar,8761,comment,737106073,62117812-8761 comment-737106073,non_debt,-
beam,1731,comment,270465630,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6405/
--none--",non_debt,-
tvm,4560,description,0,"dtype.h -> runtime/data_type.h
Changes:
- Rename all old reference of tvm::Type to DataType
- ExprNode.type -> ExprNode.dtype
- Expr.type() -> Expr.dtype()
- Change Expr related functions to expr_operator.
  - DataType::min() -> min_value(DataType)
  - DataType::max() -> max_value(DataType)
- Move type constructor Int, UInt, Float, Handle, Bool into DataType.
  - Int(bits) -> DataType::Int(bits)
  - UInt(bits) -> DataType::UInt(bits)",non_debt,-
accumulo,1132,comment,488354749,"@milleruntime I pushed my commit to remove SimulationMode from the tests. Does everything else look good? 
I also have the clientPropsPath test ready to go. Should I create a new pull request for that one or just push it onto this branch?",non_debt,-
incubator-mxnet,3811,comment,260932873,"Hi @piiswrong  I have modify the code and delete the input_data_shape according the @zhreshold 's advice.
Pls review it again!",non_debt,-
servicecomb-java-chassis,652,summary,0,[SCB 456]Remove warning,non_debt,-
beam,13983,comment,779459205,Run Java PreCommit,non_debt,-
incubator-pinot,6418,review,556011781,"controllers, brokers, and servers now support multi-ingress for different protocol. this opens up a three-phase migration path by first enabling TLS-secured servers, then upgrading client connections to TLS, and finally shutting off unsecured server ingress.",non_debt,-
brooklyn-server,842,review,141037439,"yep, `ClassLoaderUtilsTest` depends on this and it passes.  pretty cool.",non_debt,-
spark,16772,comment,276819250,Let me try and take a look tomorrow - I'm in transit today.,non_debt,-
apisix,2776,description,0,fix #2767,non_debt,-
beam,9254,description,0,"This removes default implementation of getRowCount for BeamTables. It will also change its name to getTableStatistics because it can represent rate as well.
**Please** add a meaningful description for your change here
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/)
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/)
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.",non_debt,-
spark,16892,comment,280117833,Ok I added back the other test but improved the commenting there.,documentation_debt,low_quality_documentation
reef,107,comment,78608631,"[Reef-pull-request-windows #156](https://builds.apache.org/job/Reef-pull-request-windows/156/) FAILURE
Looks like there's a problem with this pull request",non_debt,-
arrow,5689,summary,0,ARROW-6927: [C++] Add gRPC version check,non_debt,-
lucene-solr,382,summary,0,WIP: SOLR-12361,non_debt,-
fluo,514,description,0,10805187-514 description-0,non_debt,-
superset,1180,comment,249099337,"staging was broken because of this, merging",non_debt,-
gobblin,2863,review,366518825,Remove this block?,non_debt,-
spark,861,comment,44154923,Merged build started.,non_debt,-
carbondata,2296,description,0,[CARBONDATA-2369] updated the document about AVRO to carbon schema converter,non_debt,-
spark,31436,comment,772256896,"Merged to master, branch-3.1, branch-3.0 and branch-2.4.",non_debt,-
cxf,100,comment,160921994,"Jenkins keeps failing, but failures don't seem to be related in any way with the changes contained here.
Given the discussion at swagger-api/swagger-core#1558 I am going to merge this PR right now.
We can always remove it should they decide to do something about it.",non_debt,-
incubator-doris,2431,review,357906474,99919302-2431 review-357906474,non_debt,-
storm,2790,comment,412260769,"@uddhavarote That's not how it works. The stream you emit to doesn't matter. As long as the tuple you emit from the KafkaBolt is anchored to the input tuple, the tuple will be replayed from the spout and go to the entire topology again if processing fails after the KafkaBolt.
Basically the choice you will have is to use`OutputCollector.emit(input, new Values(...))`, in which case the tuple gets replayed from the spout if processing fails in your bolt, or `(OutputCollector.emit(new Values(...))`, in which case the tuple does not get replayed and is lost if processing fails in your bolt.
That said, your application probably needs to deal with potential duplicate writes to Kafka anyway, so it might not be a big deal. I'm fine with adding the output collector and tuple to the callback. Please raise an issue at https://issues.apache.org/jira and feel free to submit a PR here that makes the change.",non_debt,-
airflow,13728,comment,761947640,Temprary failures - it's good to go.,defect_debt,uncorrected_known_defects
systemds,740,review,172401453,use `new MatrixBlock(((ScalarObject) newOutput).getDoubleValue())` instead.,code_debt,low_quality_code
incubator-mxnet,8793,comment,346685420,"I don't think it's the right place to do it, but right now it fixes the build for the release, we should do it in the right way aftwards.",non_debt,-
spark,15301,review,81286160,"How about throwing `NoSuchDatabaseException`? We also have the other NoSuchXYZ exceptions. Then, when writing the test cases, we can check these specific exception types instead of the general `AnalysisException`",non_debt,-
beam,8248,description,0,"**Please** add a meaningful description for your change here
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) <br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.",non_debt,-
spark,15951,review,89019196,can you include the name of the table / relation ? the computation can refer to multiple tables which might have same `partitionColumn`,non_debt,-
couchdb,3172,description,0,"During the porting of rewrite_js.js tests into the elixir test suite I've found an erroneous behavior in the JS-rewrite functionality. 
When a rewritten request contains a BODY (POST/PUT) and the rewrite rule rewrites the query string, this query string is ignored in the final request. The following case illustrates this issue.
1 - Create a design document with a rule that rewrites the query string, like this one:
any request should be rewritten to `_changes?filter=_docs_ids`
2 - Create a couple of docs
3 - Perform a POST request to be rewritten
The expected result for this case is to have only the change for the doc with id 2 in the response, but the full change list is returned. The `filter=_doc_ids` is ignored after the rewritten.
This PR changes this behavior and the query string is preserved, so the result for the previous case is the excepted one returning only the change for the doc with id 2.
     Does it provides any behaviour that the end users
     could notice? -->
The previous test case has been added to rewrite_js.js tests
This issue #1612 and the companion PR #1620 are related to this issue
     repositories please put links to those issues or pull requests here.  -->",non_debt,-
spark,20923,comment,383571843,"@vanzin : The followup to this is #21066; I could move the compile time changes there but if you are going to have POMs playing with dependencies, seems best to have it all in one place...the other one just setting up the compile and tests
@jerryshao what do you suggest? It was your proposal to split things into pom and source for ease of reviewal, after all?",architecture_debt,violation_of_modularity
spark,19177,comment,328358846,it looks like as if this `appveyor.yml` is not enough to trigger appveyor test? it doesn't seem to be kicking off a run,non_debt,-
trafficserver,4470,comment,432435286,[approve ci autest],non_debt,-
kafka,8680,review,434445428,Done.,non_debt,-
activemq-artemis,1820,review,164163557,"So if this is related to virtual topics why complicate things for the Core JMS client and introduce such problems it has JMS 2.0 spec as such shared durable subscribers, which is the spec equiv of such feature.",non_debt,-
carbondata,3800,summary,0,[CARBONDATA-3877] Reduce read tablestatus overhead during inserting into partition table,code_debt,low_quality_code
beam,10758,comment,582703470,Run CommunityMetrics PreCommit,non_debt,-
nifi,613,comment,231845009,Closing this as incremental fetch capability is needed. Will produce a new PR against NIFI-2126 with both processors inside,non_debt,-
camel-quarkus,1799,comment,693283408,"I think we should first discuss whether Quarkus JAXP extension would not be a better place for this. Apparently, Quarkus JAXP extension does not exist yet. It looks like they put the JAXP stuff into their JAXB extension. I think we should propose splitting their JAXB into JAXB and JAXP and then put the `SAXMessages` registration in the new Quarkus JAXP extension. WDYT, @jamesnetherton ?",non_debt,-
carbondata,546,summary,0,[CARBONDATA-655][CARBONDATA-604]Make no kettle dataload flow as default,non_debt,-
spark,12820,description,0,"In order to support nested predicate subquery, this PR introduce an internal join type ExistenceJoin, which will emit all the rows from left, plus an additional column, which presents there are any rows matched from right or not (it's not null-aware right now). This additional column could be used to replace the subquery in Filter.
In theory, all the predicate subquery could use this join type, but it's slower than LeftSemi and LeftAnti, so it's only used for nested subquery (subquery inside OR).
For example, the following SQL:
This PR also fix a bug in predicate subquery push down through join (they should not).
Nested null-aware subquery is still not supported. For example,   `a > 3 OR b NOT IN (select bb from t)`
After this, we could run TPCDS query Q10, Q35, Q45
Added unit tests.",non_debt,-
spark,363,comment,39996864,Merged build finished. All automated tests passed.,non_debt,-
superset,10274,review,452420889,CSS class/IDs are generally case insensitive. Can we use `dash-case` instead?,non_debt,-
incubator-mxnet,15371,review,302817980,Get rid of dead code or un-comment to add coverage.,code_debt,dead_code
carbondata,4067,comment,752143730,"Build Success with Spark 2.3.4, Please check CI http://121.244.95.60:12444/job/ApacheCarbonPRBuilder2.3/5261/",non_debt,-
trafodion,1565,comment,408606664,New Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/2896/,non_debt,-
spark,1856,comment,53352004,Thanks!  I've merged this into master and 1.1,non_debt,-
incubator-brooklyn,131,review,16893613,No point in duplicating IfFunctionBuilderApplying's code just for one differing line. Better instantiate IfFunctionBuilderApplying with Null object for the input argument (i.e. empty IfFunction).,code_debt,duplicated_code
helix,570,review,342878123,Added,non_debt,-
storm,2567,comment,367118419,+1,non_debt,-
carbondata,3962,comment,701346144,"Build Success with Spark 2.4.5, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.5/2538/",non_debt,-
camel-website,261,description,0,"- Since, menu.js file was deleted, the preview was not working. 
  I made the required changes and checked it. Now my preview is working.",non_debt,-
tvm,2773,review,283191720,Doc formatting.,documentation_debt,low_quality_documentation
attic-apex-core,552,review,125503864,"@sanjaypujare please change these properties to use apex. prefix. For those that existed prior to 3.6 release, they should be deprecated like it was done for attributes, for others they should be just changed.",non_debt,-
beam,3385,comment,309494094,Retest this please,non_debt,-
nifi,4138,summary,0,NIFI-7248: Atlas reporting task handles PutHive3Streaming,non_debt,-
carbondata,4110,comment,808462123,"Build Failed  with Spark 2.4.5, Please check CI http://121.244.95.60:12602/job/ApacheCarbon_PR_Builder_2.4.5/3347/",non_debt,-
nifi-minifi-cpp,805,review,438146885,"We can get rid of this. The reason I kept readme was to keep a short description in-tree as I think this may provide some value to the future reader.
Do you think saving ~72K is worth removing this potential value? (honest question)",non_debt,-
kylin,1520,comment,749278320,"After this patch, the query job description is shown as below:",non_debt,-
spark,24793,review,303463548,ditto,non_debt,-
nifi-minifi-cpp,275,comment,373117634,"@achristianson I see conflicts. they may be easy to resolve, so i can take a look if need be.",non_debt,-
brooklyn-server,551,comment,278714166,"I think it only escapes illegal characters, not reserved ones, so `q1=my&value&q2=my-value` won't be changed - it assumes you know what you are doing. Let's see...
This passes:",non_debt,-
cloudstack,1953,review,102184780,"Not part of your changes but... variable `devIds` should have type `List<Long>` instead of `List<String>`.
All that conversion from `int` to `String` and then converting from `String` to `long` seems unnecessary.
Should simply be able to do:
Note: my code above includes fixes to two other comments I made further down in the code.",code_debt,complex_code
kafka,7568,comment,545474990,"Thanks, @C0urante. I've added your suggested tests plus a few more.",non_debt,-
spark,28935,review,446595267,`null` is a `value` and Hive exposes `void` as a type.,non_debt,-
spark,19885,review,154827513,"thanks all, I would update the PR soon.",non_debt,-
kylin,633,review,288895567,It's better to add lock here to avoid W/R conflict,non_debt,-
hadoop,2051,comment,640165168,"Re-executing against the patched versions to perform further tests. 
The console is at https://builds.apache.org/job/hadoop-multibranch/job/PR-2051/7/console in case of problems.",non_debt,-
kafka,5270,review,199701535,`ProducerBatcn.finalState` can also be updated to `FinalState.ABORTED` through  `Sender.run() --> RecordAccumulator.abortIncompleteBatches() or abortUndrainedBatches() -->  ... -> ProducerBatch.abort()` .,non_debt,-
flink,8290,review,281073584,We should add lock here to avoid mulitple thread entering when `isReleased` is false.,non_debt,-
kafka,3420,review,123811838,"One thing from the ClientRequest that we don't get from the builder is the correlationId. This is occasionally useful when debugging. If you think it's useful, we might consider adding it to the log lines in `doSend` as well.",non_debt,-
spark,18421,comment,319185787,"@gatorsmile , I believe I addressed all your comments and I'm seeing that tests are passing now. Thanks for pointers to -1 and <=0 changes. Tests were failing before because I missed these changes. Let me know if anything else needs more work or if this is now good to merge.",non_debt,-
superset,9321,comment,607398469,@dpgaspar can you give this a look?,non_debt,-
spark,24952,comment,505132786,ok to test,non_debt,-
incubator-doris,3651,review,429155817,"If the aggregate function is computed for the original column, it should be not key whether  the original column is a key column or a value column",non_debt,-
openwhisk,915,comment,233446020,"Sure, I can do that.",non_debt,-
spark,24300,description,0,"This PR aims to clean up package name mismatches.
Pass the Jenkins.",code_debt,low_quality_code
spark,28391,comment,638625572,@HeartSaVioR @xuanyuanking Thanks for reviewing.,non_debt,-
nifi-minifi-cpp,188,review,153901727,Ah seems that way from the cmake.,non_debt,-
druid,2296,comment,173051557,@drcrallen I don't think actual object support is there for java.util granularity,non_debt,-
spark,17945,comment,300855402,Actually one more thing to do is to change the vignettes to use 1 core as well since they get rebuilt / checked during the CRAN check.,non_debt,-
calcite,1995,comment,640931839,"It is disappointing that the Calcite's MV is trying to match physical plans. Further more, Choosing MV or not based on cost is totally meaningless.",non_debt,-
incubator-mxnet,13088,comment,449914577,There seem to be some bugs in test_operator_gpu.test_poisson_generator which lead to failure of CI.,non_debt,-
flink,1412,comment,206349631,"@chiwanpark: I think you can register multiple email addresses with GitHub, so that they can associate all your commits (from different addresses) to you.",non_debt,-
spark,5955,comment,99642187,Also @pwendell,non_debt,-
dubbo,3182,review,247350659,"double check all import sequence. I believe this will break CI. We have import rule configured, 'org.apache.dubbo' should go first.",non_debt,-
couchdb,1808,summary,0,Update before_doc_update/2 to before_doc_update/3,non_debt,-
hbase,2491,description,0,"Following the [announcement](0) to EOL branch-1.3, update the precommit
script to not consider this branch any longer, and refresh mentions of
this branch in the doc.
[0]: https://lists.apache.org/thread.html/r9552e9085aaac2a43f8b26b866d34825a84a9be7f19118ac560d14de%40%3Cuser.hbase.apache.org%3E",non_debt,-
zeppelin,2601,review,141550143,"From above PR test description, the comma separator is inside the quote, this csv case is ok.
When the string value is **fo""o \t b""ar**, then it goes to tsv case. 
The ""\t"" in **fo""o \t b""ar** can be correctly read by Excel. But if the string value is **fo""o	b""ar**, in which contains ""\t"" typed through **keyboard**, this string value will be misunderstanded by Excel. So the tsv case still need to be deal with.
I have pushed a new commit, which has delete the redundant excape.",non_debt,-
storm,1112,comment,184703067,+1,non_debt,-
superset,12739,description,0,"""Environment"" was misspelled on line 348, I have corrected this typo.",documentation_debt,low_quality_documentation
commons-lang,261,comment,289800588,@yasserzamani I thinks because you added more conditional branches in the latest commit and that is why less LOC are covered,test_debt,low_coverage
spark,9919,summary,0,[SPARK-11933][SQL] Rename mapGroup -> mapGroups and flatMapGroup -> flatMapGroups.,non_debt,-
tvm,4628,review,386174484,Good catch! Have added test cases.,non_debt,-
trafficserver,4992,summary,0,Doc: centering images is not in view,non_debt,-
openwhisk,1330,summary,0,Avoid printing object hashes in log messages related to containers,non_debt,-
kafka,1815,comment,316914906,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6241/
Test PASSed (JDK 7 and Scala 2.11).",non_debt,-
beam,5049,comment,379410687,"Note that this change is not Python 3 compatible since `long` was removed in Python 3.  Can you add something similar to the following?
https://github.com/apache/beam/blob/8d854d4ce0365a8e201b388618d7732f000c65b9/sdks/python/apache_beam/transforms/combiners.py#L39",code_debt,low_quality_code
airflow,12829,description,0,"Closes #12757 
When a task does not have a dag at the time it's being set as a relative of another task, the Graph View becomes empty.
For example:
Note op2 does not have a dag when `op1 >> op2` is called. It's added to op1's DAG after this line, but not added to any `TaskGroup`. This causes Graph View to break.
This PR fixes the issue by adding `op2` to the root `TaskGroup` if op1's DAG when `op1 >> op2` is called.",non_debt,-
trafodion,225,comment,164217785,"jenkins, add user",non_debt,-
nifi,4720,summary,0,NIFI-1440 Allow Remote Process Group Target URI to change after creation,non_debt,-
flink,2392,description,0,"The RpcGateway.getAddress method allows to retrieve the fully qualified address of the
associated RpcEndpoint.",non_debt,-
kafka,986,review,60941228,Why are we using `assert` instead of `JUnit` assertions?,non_debt,-
flink,1690,comment,188002588,"@rmetzger the test failures were because we had `auto.commit.enable=false` in the standard `KafkaTestEnvironmentImpl` standard props and it wasn't respecting that previously, updated those defaults",non_debt,-
drill,1836,comment,521174404,"@arina-ielchiieva 
Some time spent debugging the test showed that the last schema contains all fields. The field is added in ProjectRecordBatch#setupNewSchemaFromInput. 
In the original version of the test, field A was not added due to plan optimization - condition `1=0` was replaced by` limit 0`
I can still provide a solution with combining schema if required.",non_debt,-
drill,1279,review,195346887,How many times this would be called? I am just wondering why we have this null check.,non_debt,-
drill,1228,review,184192443,done. using 5 in both places now.,non_debt,-
spark,14783,comment,243683097,"should we have a test against DataFrame with binary column?
or, this `test_that(""dapplyCollect() on dataframe with list columns""` should say `bytes column` or `binary column`?",non_debt,-
calcite,782,review,209381477,"``
// below might change the type of the call (e.g. from nullable to non-nullable)
// however simplify(..) is allowed to return node with different type
// if the type should match, then `simplifyPreservingType` should be used",documentation_debt,low_quality_documentation
kafka,841,comment,177319309,@ijuma Do you mean adding it to the comments or update the ticket description? I did not test Java 1.7 CRC32 performance. I only tested the one we used to use and the CRC32 performance in Java 1.8.,test_debt,lack_of_tests
superset,6663,comment,453598576,"The issue I had with RAT is it wouldn't deal with py and js/jsx files in a way that our linters liked, and there was lots of files to touchup... rodent is a very small app (haven't pushed it to pypi yet), but could become an alternative to RAT.
Also it wasn't aware of my .gitignore, but I see you took care of this here with a rat-ignore...",code_debt,low_quality_code
spark,26738,review,361889612,"Before this PR, 
After this PR,",non_debt,-
superset,6943,comment,467251658,Can I merge https://github.com/apache/incubator-superset/pull/6808 first?,non_debt,-
spark,1379,comment,71531266,@dbtsai I did batching for artificial neural networks and the performance improved ~5x https://github.com/apache/spark/pull/1290#issuecomment-70313952,code_debt,slow_algorithm
lucene-solr,2336,description,0,"# Description
SOLR-13608 introduced support into Solr for an ""incremental"" backup file structure, which allows storing multiple backup points for the same collection at a given location.  With the ability to store multiple backups at the same place, users will need to be able to list and cleanup these backups.
# Solution
This PR introduces two new APIs: one for listing the backups at a given location (along with associated metadata), and one to delete or cleanup these backups.  The APIs are offered in both v1 and v2 flavors.
# Tests
Manual testing, along with new automated tests in `PurgeGraphTest` (reference checking for detecting index files to delete), `V2CollectionBackupsAPIMappingTest` (v1<->v2 mapping), and `AbstractIncrementalBackupTest` (integration test for list, delete functionality).
# Checklist",non_debt,-
spark,5511,review,28394051,Origin code ignore the `newName`. Is this intended?,code_debt,low_quality_code
spark,31296,review,564382393,"Err..I don't think you get the discussed points above. Let me clarify it...
@HyukjinKwon suggested to use ""TRANSFORM"" for the purpose of piping through external process, instead of adding ""pipe"" to Dataset API. The idea is basically to add DSL. But the problem is, ""TRANSFORM"" is not an expression and cannot be used in a DSL approach. So in the end in order to use ""TRANSFORM"" for piping through external process for streaming Dataset, you will need a top-level API too...But the point of DSL is to avoid a top-level API. So...",non_debt,-
madlib,303,description,0,42763345-303 description-0,non_debt,-
zeppelin,2991,comment,396809218,"@Savalek can you update the title of this PR?
something like ""change Description ""auto-restart interpreter on cron execution""""",non_debt,-
accumulo,394,comment,369808863,"Does this revert the behavior that ACCUMULO-4574 was trying to address - if the table is online, return and do not wait for additional actions? The original behavior would block if there was a fate operation on the table and online was called - the original fix was to not block and return because the table was online and no additional actions were required to be in online.
If the table is online - why do we need to wait?",non_debt,-
cloudstack,4175,comment,665553410,@DaanHoogland The problem is that for Redfish testing (and IPMI) you need a physical machine do test this on.,non_debt,-
spark,7833,comment,127803471,retest this please,non_debt,-
hudi,1044,description,0,"Currently, there are only two reporters MetricsGraphiteReporter and InMemoryMetricsReporter. InMemoryMetricsReporter is used for testing. So actually we only have one metrics reporter.  propose to provide a CSV metrics reporter.",non_debt,-
spark,24018,comment,472602348,"Ur, @srowen  It's replaced with a `private` version as I mentioned before.
For example, if you see HIVE-14259, it was like the following.
The new version on the lastest master branch looks like the following.",non_debt,-
spark,4533,comment,73925690,"Sounds correct. The subsequent tries do try in parallel. So, I suppose that's pretty good evidence it's parallelized. Unless anyone else speaks up I think this sentence can be removed.",code_debt,complex_code
cloudstack,4640,review,572617424,I think capability details map should ok in the response.,non_debt,-
spark,28882,review,450833763,"to be safe, can we compare the URI? we can convert path string to URI with `CatalogUtils.stringToURI`.",non_debt,-
nifi,3748,review,328267613,Changed,non_debt,-
spark,8669,comment,138927483,"Other than that if it doesn't change existing behavior and lets symlinks stand a chance of working, seems good.",non_debt,-
superset,2809,summary,0,[explore] include ControlHeader as part of Control interface,non_debt,-
spark,25942,review,328803928,I have fixed 3 out of 4. I will open a ticket for the last one.,non_debt,-
trafficcontrol,4537,review,399606341,"I like the idea of supporting that as a first-class feature, but I think it wouldn't need to be in the MVP. I think we could get by with just naming a Topology ""http-live-default"", etc, and simply choosing that based on the requested DS type if it wasn't clear which Topology to choose. Then if that ends up being a royal pain, maybe we add first-class support for it?",non_debt,-
flink,2455,comment,249097529,"@tillrohrmann , I rebase the PR. Thanks.",non_debt,-
activemq-artemis,3455,comment,779848243,"@michaelandrepearce this is just code cleanup. Nothing that would bring any value to a release notes...
The commit itself would be enough record of the change here.
If someone is creating a JIRA, it would be a task, as the is not an improvement, not a feature, not a bug fx.. no value on the release notes.",code_debt,low_quality_code
beam,7737,review,266999874,It's the opposite. If try_claim() returns true the current reader owns that set of records and must return that. If try_claim() returns fails the current reader should return right away without returning any records that it failed to claim. The reader should not return any records that it didn't claim. See https://beam.apache.org/documentation/io/developing-io-python/,non_debt,-
arrow,415,description,0,‚Ä¶on Windows; Correct linking with IMPORTED_IMPLIB of 3rd party shared libs on WIndows.,non_debt,-
flink,10999,review,376764141,20587599-10999 review-376764141,non_debt,-
spark,30224,description,0,"This change is to support user provided nullable Avro schema for data with non-nullable catalyst schema in Avro writing. 
Without this change, when users try to use a nullable Avro schema to write data with a non-nullable catalyst schema, it will throw an `IncompatibleSchemaException` with a message like `Cannot convert Catalyst type StringType to Avro type [""null"",""string""]`. With this change it will assume that the data is non-nullable, log a warning message for the nullability difference and serialize the data to Avro format with the nullable Avro schema provided.
This change is needed because sometimes our users do not have full control over the nullability of the Avro schemas they use, and this change provides them with the flexibility.
Yes. Users are allowed to use nullable Avro schemas for data with non-nullable catalyst schemas in Avro writing after the change.
Added unit tests.",non_debt,-
cloudstack,3166,comment,496197734,We'll need to check but current systemvmtemplate only has python 2.7.,non_debt,-
helix,43,description,0,"Helix allow messages to be sent to a partition with specific state. e.g. Sending message to PARTITION_8 whose state is SLAVE.
However, sometimes user wish the message is sent to only ONE SLAVE rather than ALL SLAVEs, e.g. A backup message should be sent to only ONE SLAVE.
This diff adds a new field in Criteria which allows user to specify maxReceipts, by default maxReceipts is null means no uplimit. In previous case, user just set maxReceipts as 1 and message would be sent to only 1 SLAVE.",non_debt,-
dubbo,4137,description,0,"see #2956
XXXXX
XXXXX",non_debt,-
carbondata,3330,comment,520767310,"Build Success with Spark 2.1.0, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.1/324/",non_debt,-
hive,506,review,241291108,"This code is dumping location of all the external tables at the time of incremental dump, which may not be the exact set of tables that would be available at the time of lastReplid when specified by the user. Can we avoid the tables which may not be present at lastReplid?",non_debt,-
flink,12611,review,438868568,Won't this kill JVM on _any_ exception regardless of whether it was handled below or not?,non_debt,-
kafka,5885,review,236424422,Can we name this `transactionGenerator` for simplicity?,code_debt,low_quality_code
pulsar,344,review,110961234,Convert tabs to spaces,non_debt,-
spark,13805,comment,227563624,"Alright - yeah lets leave `explode` as is for now. LGTM. Merging this to master, branch-2.0",non_debt,-
helix,1043,review,433522629,This is an out parameter. So the order will be kept when k-v pairs are added to the LinkedHashMap in this function. Forcing the caller to pass in a LinkedHashMap will  guarantee the order in which pairs were inserted into the map when user uses the map later.,non_debt,-
cassandra,500,description,0,206424-500 description-0,non_debt,-
kafka,2317,comment,270655221,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/531/
Test FAILed (JDK 7 and Scala 2.10).",non_debt,-
cloudstack,1092,summary,0,Fix NuageVsp errors for build-master-slowbuild,non_debt,-
cloudstack,3215,comment,478916160,@blueorangutan test,non_debt,-
flink,12900,review,454415036,"Is this interface really necessary? Especially with `@PublicEvolving` annotation? How are users supposed to use it? If I understand it correctly you need it for internal operations. Moreover you need it because the `WrapperTypeInfo` is in `blink-runtime`, right?
Can't we move the `WrapperTypeInfo` to the `table-common` instead?  The class itself has no runtime dependencies. Only the factory methods need some runtime classes.",code_debt,complex_code
openwhisk,1063,comment,243067533,"Adressed issues in comments and added 3 more commits. They should be squashed to their respective ""parent"" commit.
1. ""Adding withClue statements to action tests, dropping unneeded tests"" -> ""Rewrite CLIActionTests in Scala, refactor withActivation helper""
2. ""Formatting nit in TestUtils"" -> ""Refactor runCmd, remove obsolete helperclasses""
3. ""Renaming some ruletests, adding comments"" -> ""Rewriting CLIRuleTests in Scala, adding a new testhelper""",code_debt,dead_code
spark,14482,review,73472950,"BTW, test added: https://github.com/apache/spark/pull/14482/files#diff-b7094baa12601424a5d19cb930e3402fR144",non_debt,-
spark,31107,review,560355418,"Yes we still keep the existing behavior for the above scenarios which this PR doesn't touch. Technically we could switch them to all use cascade uncache but that is a broader behavior change and should be discussed separately.
Sure I can add some comments.",non_debt,-
helix,24,summary,0,[HELIX-584] SimpleDateFormat should not be used as singleton due to its race conditions,non_debt,-
flink,4143,review,123715389,"In general, I would not use the ""kleene"" term, as we do not use it throughout the rest of the docs.
You can say sth like ""looping"" or simply oneOrMore.
In addition, it would also be more consistent to not use ""operator"" but ""pattern"". In the docs we have ""pattern sequences"" composed of ""patterns"" so we should stick to that.
So ""Kleene Operator"" -> ""looping pattern"" or ""oneOrMore pattern""",documentation_debt,low_quality_documentation
openwhisk,3453,review,176278384,fair.,non_debt,-
airflow,12094,review,517592396,"I hadn't planned on changing anything but the mode of the read, but I'll commit your suggestion.",non_debt,-
spark,7181,review,33755269,how is this less code? It is just a bad idea to create unnecessary state. Just move both tables into two fields in Hex object.,code_debt,complex_code
kafka,764,review,50764320,"@apovzner Personally I think the timestamp should be accurate. Modifying the timestamp sounds very hacky and creates extra complexity. Please also notice that the timestamp index built by the followers will be purely depending on the timestamp in outer message of compressed messages. The followers will not even decompress the messages. If we play the trick here, the time index on follower will also be affected. 
If we want to make things right, then producer should be able to get the necessary topic configuration info from broker, either from TopicMetadataRequest or some other requests. So the producer can set the timestamp correctly to avoid server side recompression. But like you said this is a bigger change and it is unnecessary to block on that change.
I think the current solution is reasonably clean as of the moment.
Once the producer is able to get the topic configuration from broker, we can simply migrate to use that. Since everything is purely internal, the migration is very simple and transparent to users.",design_debt,non-optimal_design
airflow,7391,comment,590026357,All fixed @kaxil,non_debt,-
spark,10600,review,50745215,Fixed!,non_debt,-
spark,23476,review,245567612,"SGTM. I think it's a better idea to forget about GitBox. Once a committer has done the setup, he can pull/push to the github repo, which is the single south of truth.",non_debt,-
trafficserver,7023,review,508026325,Changed,non_debt,-
kafka,3010,comment,300553861,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3710/
Test PASSed (JDK 7 and Scala 2.10).",non_debt,-
drill,1361,comment,404663922,"@vrozov 
Made the requested modifications.",non_debt,-
carbondata,2479,comment,407090883,"LGTM
please rebase",non_debt,-
geode-native,133,description,0,80904111-133 description-0,non_debt,-
spark,2516,review,18909253,s/map/set/,non_debt,-
beam,8417,summary,0,fix memory issue,non_debt,-
hive,7,summary,0,Update ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,non_debt,-
spark,2215,comment,54260952,ok to test,non_debt,-
carbondata,987,comment,306424343,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/carbondata-pr-spark-1.6/99/",non_debt,-
camel-website,167,summary,0,Fixed right/left margin issue,non_debt,-
skywalking,5128,description,0,"- Why submit this pull request?
- Related issues
___
- Bug description.
- How to fix?
___
- Describe the details and related test reports.
update ui submodule",non_debt,-
arrow,3022,comment,442088978,Seems that way. I opened https://issues.apache.org/jira/browse/ARROW-3892,non_debt,-
incubator-pinot,2228,review,158572949,I actually don't need the .valueOf(). Removing it.,code_debt,low_quality_code
airflow,3090,comment,370109428,"nit: i think line 68-70 can be removed (to avoid confusion), looks like abandoned legacy code:
thanks for fixing this!",code_debt,dead_code
spark,19943,review,160133388,"Ur, I meant only current ORC readers. I didn't check the detail of old Hive ORC reader.
- org.apache.orc.mapred.OrcInputFormat.createRecordReader => MR reader
- org.apache.orc.OrcFile.createReader() => RowBatch reader",non_debt,-
cloudstack,15,comment,54590261,"If JAVA_HOME is not set, setJavaHome()(cloud-usage.rc:49)  method is trying to find it from default java available using something like `JAVA_HOME=$(dirname $(dirname $(readlink -e $(which java))))` and exits if couldn't find anything.
wouldn't that work in this case?",non_debt,-
hudi,1115,comment,567470696,Will add more test cases to cover complex dag.,test_debt,low_coverage
arrow,7240,review,428676385,It's weird to have this as part of the cast options.,non_debt,-
trafficserver,5380,summary,0,Fixes spelling in src,documentation_debt,low_quality_documentation
zookeeper,1614,comment,785912733,"using the link on the failed GitHub CI job (the one with label ""Details""), you can select ""Rerun Jobs"". But that would re-run all GitHub based CI jobs for this PR, and that potentially can cause more flaky tests. So I think this is good enough.",test_debt,flaky_test
carbondata,3770,review,434409645,ok,non_debt,-
zookeeper,567,comment,405419213,Thanks for the review @anmolnar. Please take a look at unit tests when you get a chance. I have addressed the comments. I will also add documentation in a separate commit.,documentation_debt,low_quality_documentation
hadoop,714,review,278759705,"Don't log here, just throw IOException that wraps the ExecutionException.",non_debt,-
cloudstack,4220,review,459883908,"@GabrielBrascher I didn't make changes in line 314 because it has different behavior compared to line 165.
I have mentioned the same in issue #4221 also. list hosts api returns absolute value where as findhostsformigration returns percentage. If we decided what to return then I can make changes in these two places also",non_debt,-
carbondata,3538,comment,577157879,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1738/",non_debt,-
beam,12645,review,486156062,@abhiy13 short javadoc please.,documentation_debt,outdated_documentation
lucene-solr,104,review,85476962,Need to enable all the tests again,non_debt,-
carbondata,3485,comment,559325045,retest this please,non_debt,-
kafka,8218,review,389053599,"You mean exception handling? For the producer all exception handling is done within `StreamsProducer` (note that `threadProducer` above is a `StreamsProducer`, not a `KafkaProducer`)",non_debt,-
jena,281,comment,333477316,+1,non_debt,-
geode,5716,summary,0,"test, do not merge",non_debt,-
tvm,5144,review,397530214,letlist is everywhere. can you put this in some common file?,code_debt,low_quality_code
cloudstack,4789,comment,809525122,@Pearl1594 a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests,non_debt,-
gobblin,2075,description,0,"Dear Gobblin maintainers,
    - https://issues.apache.org/jira/browse/GOBBLIN-190
- New unit test is added which is Kafka09TopicProvisionTest which can be run using 
./gradlew -PgobblinFlavor=custom :gobblin-modules:gobblin-kafka-09:test
- Live Unit test is also possible which can be run like this 
    ./gradlew -PgobblinFlavor=custom :gobblin-modules:gobblin-kafka-09:test -Dlive.cluster.count=3 -Dlive.zookeeper=<ZKHOST>:2181 -Dlive.broker=<KAFKA_BROKER_HOST>:9092 -Dlive.newtopic=liveTopic-Test -Dlive.newtopic.partitionCount=7 -Dlive.newtopic.replicationCount=2
For the tests to run the kafka-09 flavor needs to be enabled.",non_debt,-
kafka,6500,description,0,"Since we've added Kafka Streams optimizations in `2.1` we need to move the optimization for source `KTable` nodes (use source topic as changelog) to the optimization framework.
Updated streams tests.",non_debt,-
carbondata,1127,description,0,62117818-1127 description-0,non_debt,-
superset,10562,review,468025755,"let's use EMAIL_REPORTS_WEBDRIVER or just converge those 2
easy solution would be:
or",non_debt,-
spark,31977,comment,808681080,"Hm, got it. I'm running `HistoryServerSuite` now.",non_debt,-
incubator-mxnet,17882,review,411752033,"Again, could you please explain why there are only 3 cases regarding the number of axes?",non_debt,-
kafka,5397,comment,406524827,"@ijuma Yes, just updating the log for now and leaving the JIRA open.",non_debt,-
spark,5537,description,0,"It's better to let the NodeManager get down rather than take a port retry when `spark.shuffle.service.port` has been conflicted during starting the Spark Yarn Shuffle Server, because the retry mechanism will make the inconsistency of shuffle port and also make client fail to find the port.",non_debt,-
beam,9807,description,0,"R: @kennknowles 
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/)
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/)
XLang | --- | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | --- | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.",non_debt,-
beam,4990,review,178641522,The [int from the future.builtins](http://python-future.org/compatible_idioms.html#long-integers) is a subclass of python 2's long.,non_debt,-
spark,20303,review,249798529,"I can answer that myself, the countdown stuff is useful to figure out if the computation has completed. Please add some doc here.",documentation_debt,outdated_documentation
dubbo,5018,description,0,"Upgrade fastjson version to 1.2.60 (https://github.com/alibaba/fastjson/releases/tag/1.2.60)  for security issue 
pom.xml
ci pass",non_debt,-
hudi,673,review,291836975,Lets also revert this change as AbstractCommandConfig will be in hoodie-utilities,non_debt,-
storm,190,comment,51485317,"Looks good tests pass, I'll merge this in. +1",non_debt,-
beam,13591,review,564053596,Done.,non_debt,-
spark,12836,comment,216750713,"Ok - if the behavior we get from `dapply(repartition(df, cols))` is the same as `groupByKey().flatMap` then I'm fine with going with the simpler implementation. 
But I think we should have a high level `gapply(df, cols, function(group))` API in SparkR that is clearly specified. The internal implementation we should go with whatever is simpler.",code_debt,complex_code
incubator-dolphinscheduler,2143,summary,0,Add mail server configuration description information,non_debt,-
carbondata,1076,description,0,This PR is try to implement range interval partition and now work on process.,non_debt,-
spark,23257,comment,445491601,@cloud-fan What do you think of the PR?,non_debt,-
trafficserver,6730,comment,622372182,@ema please run `tools/clang-format.sh` on your branch. Also TS API changes should be discussed on the dev mailing list IIRC.,non_debt,-
tajo,470,comment,94593765,Rebased.,non_debt,-
spark,25823,review,330702021,"This is a single class name, variable name should reflect that.",code_debt,low_quality_code
spark,8780,comment,145693030,@travishegner looks like it is best to just do it in the oracle dialect.,non_debt,-
accumulo,242,comment,303775064,"@milleruntime The only standby behavior we want to retain is that of the log receiver. The active monitor registers itself as the log receiver in ZK, but the standbys shouldn't. Everything else should be the same.",non_debt,-
cloudstack,3965,description,0,"When expunge a Running vm, vm will be stopped with forcestop=false which does not make sense. we should honor vm.destroy.forcestop in global setting, or always set forcestop=true.",non_debt,-
flink,14734,review,569501903,nit: one step further would be to store `CheckpointBrief` in `PendingCheckpoint` instead of collections. But that's probably out of scope of this PR,code_debt,low_quality_code
spark,610,review,12292824,"As per our offline discussion, let's just change this to `unused/class/path` or something.",code_debt,low_quality_code
attic-apex-core,272,description,0,@tweise please merge and also cherry pick to release-3.2 and release-3.3,non_debt,-
beam,9073,comment,512241257,Run JavaPortabilityApi PreCommit,non_debt,-
drill,1397,review,210773341,space,non_debt,-
beam,13635,comment,753642255,@iemejia The CI is green now. What do you think of the Deque encoder?,non_debt,-
ambari,161,comment,359318955,"@venkatasairamlanka  Please help review/merge this to trunk and branch-2.6.
Ref: https://reviews.apache.org/r/64894/",non_debt,-
iceberg,1185,review,475330648,"Actually, the default value of `maxCommittedCheckpointId` can be null instead of `INITIAL_CHECKPOINT_ID`.",non_debt,-
hive,550,review,343202348,yeah...this seems to be one place where this tablename git confusing ... :+1:,code_debt,low_quality_code
trafficserver,1912,comment,301616781,"@vmamidi Yeah, please merge master to your branch, and git push this again (with --force) to your PR branch.",non_debt,-
carbondata,4015,review,533409073,done,non_debt,-
samza,103,review,110061958,I prefer addressing non SEP-1 related suggestions in a separate PR. This is an awfully large change and should not be bloated with other important changes.,design_debt,non-optimal_design
nifi-minifi-cpp,789,review,432613916,Why is this `NOLINT` here?,non_debt,-
airflow,6306,review,334277188,What is the difference between this variable and the above?,non_debt,-
spark,4200,comment,72024745,"PR updated. Several findings: 
1. LocalLAPACK and LocalARPACK shares similar upper bound, ""requested array exceeds vm limit""   when n = 17000. For 15000, it will take more than 5 hours but doable.
2. k is actually ignored in LocalLAPACK mode. It always computes full svd.
3. computeGramianMatrix also has upper limit somewhere < 17000. Actually that's why 1 fails at 17000. I'll look into it. Yet I need more time to locate the root cause for that.
4. Under DistARPACK mode, for 17K \* 17K full svd, I got a lot of future times out and job failed. I'm trying k = 10 with 17K \* 17K (1 hour now), and seems all worker CPUs are always idle.
My intention is to expand the range of matrix computing for Spark... 
I'll probably try to optimize the DistARPACK mode.",non_debt,-
beam,11307,comment,608785737,Run Python PreCommit,non_debt,-
trafficcontrol,2636,review,208027022,My editor doesn't seem to agree with git about spaces.  should be fixed now.,non_debt,-
beam,3711,comment,329306987,"Can you please file a JIRA, and change the title of this PR per the policy?",non_debt,-
geode,1353,review,164254842,How can GemFireCacheImpl.getInstance depend on InternalDIstributedSystem.getAnyInstance if when you connect a DistributedSystem it does not call addSystem(newSystem)? If ALLOW_MULTIPLE_SYSTEMS is true then you just call InternalDistributedSystem.newInstance(config). Doesn't this prevent it from getting registered in existingSystems that getAnyInstance uses?,non_debt,-
spark,24177,comment,475557785,"@HyukjinKwon Thanks!
Actually I had filed, but forgot to tag the JIRA ID and the category.",non_debt,-
trafficserver,4889,comment,459386130,"I don‚Äôt understand why we can‚Äôt run the autest in a for loop with time on the CI to figure out what performance issues exist. Without data, any optimizations are purely conjecture.",non_debt,-
spark,18565,review,126306207,I will refine.,non_debt,-
superset,9435,summary,0,"[dashboards] New, tittle and slug OR filter",non_debt,-
cassandra,711,review,472502571,"nit: better to do `node1.nodetoolResult(""disableautocompaction"", ""netstats_test"").asserts().success();` rather than exec into the instance.",code_debt,low_quality_code
incubator-pinot,5853,review,476001577,Recommend inline these 2 methods for readability (as the existing code). Current way is not as readable,code_debt,low_quality_code
flink,13304,review,482747567,`E.col('a')` -> `t.a`,non_debt,-
trafficserver,326,description,0,I fix one spelling mistake,documentation_debt,low_quality_documentation
karaf,1111,comment,621176047,retest this please,non_debt,-
shardingsphere,9676,description,0,Ref #8966 .,non_debt,-
beam,11656,comment,628149790,Run Python PreCommit,non_debt,-
beam,6532,comment,431204721,Run Python Flink ValidatesRunner,non_debt,-
trafficcontrol,3524,comment,490512780,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/3637/
Test PASSed.",non_debt,-
spark,14712,comment,242887201,@cloud-fan I think the [comment](https://github.com/apache/spark/pull/14712#discussion_r75540560) by @gatorsmile can answer your question.,non_debt,-
nifi,4495,review,482571304,"Again, this could be expanded to allow direct population of the public key, and many public keys are not provided in `.pub` files -- perhaps the default extension filter could be `*.pub` but allow for a regex like `*.pub|*.pem`, etc.",non_debt,-
ignite,5782,summary,0,Ignite 2.7.1 p2,non_debt,-
cassandra,53,summary,0,Create Readme_mohit,non_debt,-
tvm,4548,comment,575960459,sounds good to me.,non_debt,-
airflow,7085,description,0,"Most of the executors run local task jobs by running `airflow tasks run ...`. This is achieved by passing  `['airflow', 'tasks', 'run', ...]` object to subprocess.check_call. 
This is very limiting when creating new executors that do not necessarily want to start a new process when starting a local task job, e.g. fork a process instead of create.
We could achieve a similar effect if we process back the argument list, but this is an ugly and hack solution, so I did refactor the code and now the executor passes the LocalTaskJobDeferredRun object that contains all the detailed information. A particular executor could create a command if it needs it.
This will facilitate the development of other executors:
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-29%3A+AWS+Fargate+Executor (@aelzeiny)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-28%3A+Add+AsyncExecutor+option (@dazza-codes)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-25+The+Knative+Executor (@dimberman 
https://github.com/apache/airflow/pull/6750 (@nuclearpinguin )
This also made the DebugExecutor code simpler. (@nuclearpinguin )
---
Issue link: [AIRFLOW-6334](https://issues.apache.org/jira/browse/AIRFLOW-6334)
---
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.",design_debt,non-optimal_design
kafka,4319,review,164937959,"not sure its anything to be fixed here, but I had been hoping to keep logical types as obvious pre- or post- processing steps (and where possible, dispatch to handlers via simple map lookups). I would need to think more about how to make that work in this case, but something to consider, even if just for a future refactoring.",non_debt,-
spark,14140,review,70451380,"I think you can supply your own ordering and partitioner separately? the ordering is defined implicitly, which is sort of awkward to override. But then you should be able to partition differently from that ordering.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala#L50",non_debt,-
iceberg,2167,review,571340787,"Nit: it would be nice to keep the expression on one line, wrapping after `checkArgument(`",code_debt,low_quality_code
kafka,281,review,41351627,Appropriately concerned. I started writing a test before I realized I couldn't get a non-String value in there :),non_debt,-
arrow,4895,description,0,51905353-4895 description-0,non_debt,-
druid,7758,review,287539672,but why did you add the space between `undefined` and `)`,code_debt,low_quality_code
carbondata,2930,comment,440113778,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/1454/",non_debt,-
spark,19901,review,155169114,"nit: `val tmpIsNull = ctx.freshName(""coalesceTmpIsNull"")`, to be consistent with https://github.com/apache/spark/pull/19901/files#diff-a966ee88604a834221e82916ec051d7dR190",code_debt,low_quality_code
pulsar,2245,comment,408267297,for reviewers: this change is based on #2243 . commit b1865aa is the change to review.,non_debt,-
kafka,6427,review,265291607,"@ijuma, Actually, I now remember why I did it. `MessageFormatter` supports Java arrays while `String.format` does not.",non_debt,-
superset,1792,comment,265527614,Oh check out `assets/javascripts/components/Button.jsx` it's basically a bootstrap button with a tooltip prop.,non_debt,-
storm,1406,comment,222930875,"@abhishekagarwal87 @knusbaum 
Not at all. It shouldn't hurt much so let's apply this as it is. We can address broader considerations from another issues.
One thing I'd like to see is the status of backpressure for each queue. We can show the percentage, or just show whether this queue meets condition to trigger backpressure or not.",design_debt,non-optimal_design
beam,6981,review,233161421,"Regarding the operator as a whole and as discussed in the past, constraints are mostly inherited and would need to be addressed by refactoring the base. I'm not sure about the timing for this though, if it should wait until legacy runner goes away (there is still significant work for portable runner to become adequate replacement for Java pipelines).",design_debt,non-optimal_design
openwhisk,484,review,65249201,"I was thinking for the good test, if somehow there could be leaking across connections.",test_debt,low_coverage
nifi,2522,comment,371474068,"@MikeThomsen - since you've been extensively working on the Mongo processors, I'd appreciate if you could give your feedback on this one. Thanks much!",non_debt,-
tvm,4260,comment,550260065,"@comaniac ,
Looks good to me (looked at winograd part).",non_debt,-
spark,15939,comment,261768701,Merged to master/2.1,non_debt,-
druid,1472,review,34838258,In such a case this would NPE I think,non_debt,-
beam,8287,comment,482385820,R: @lukecwik,non_debt,-
kafka,7285,review,329133984,Ack,non_debt,-
apisix,1746,review,444645383,I think this issue has been already resolved in Openresty,non_debt,-
spark,17169,comment,437594000,"It seems has improved:
https://github.com/apache/spark/blob/v2.4.0/mllib/src/main/scala/org/apache/spark/ml/feature/Bucketizer.scala#L91-L104",non_debt,-
phoenix,955,review,520122158,Are there any checks to avoid a case where both of these attributes are set? Can something screw up if that does happen?,non_debt,-
beam,4406,comment,357410930,Joining the publisher thread before disconnecting and the test timeout configuration were the real fixes.,non_debt,-
kafka,9522,comment,718109283,"@abbccdda On second thought, we should just target trunk here and pick into 2.7 like usual.",non_debt,-
tvm,6343,comment,696872202,"@tmoreau89 I think we are waiting for more commits and responses from @anilmartha. I've just resolved the comments that should not be the blockers. The blockers now can be summarized as follows:
- @anilmartha is working on serialization of xgraph and will add it in this PR itself (https://github.com/apache/incubator-tvm/pull/6343#discussion_r483508132).
- Organize Xilinx artifacts in a better way (https://github.com/apache/incubator-tvm/pull/6343#discussion_r479578125, https://github.com/apache/incubator-tvm/pull/6343#discussion_r484641148).
- Question about 2 subgaph support (https://github.com/apache/incubator-tvm/pull/6343#discussion_r479581117).
- Rename in test (https://github.com/apache/incubator-tvm/pull/6343#discussion_r479239611).",non_debt,-
incubator-brooklyn,910,review,39962111,"Worth having some javadoc (even though the method is small, it's behaviour is non-obvious). It wasn't clear until looking carefully why would increment the iterator sometimes (i.e. call `next()`) and not other times.",documentation_debt,low_quality_documentation
nifi,367,description,0,27911088-367 description-0,non_debt,-
spark,22154,review,234385204,"The suggested change is only for making this test suite cleaner, right? In that case I'd +1 with the suggestion of being able to clearly check we're catching the exception we know we're throwing.
Would you like to submit a PR for it?
The intent is never to actually reach the null-return, but always cause an exception to be thrown at `CodeGenerator.compile()` and abruptly return to the caller with the exception. To make the compiler happy you'll have to have some definite-returning statement to end the function, so a useless null-return would probably have to be there anyway (since the compiler can't tell you'll always be throwing an exception unless you do a throw inline)",code_debt,low_quality_code
flink,1995,description,0,"[FLINK-3782] ByteArrayOutputStream and ObjectOutputStream should close
ByteArrayOutputStream close method does nothing and has no use, so is usually never called.However I am using try with resources for both to take care of closing closeable resources automatically.",non_debt,-
tvm,1488,comment,410861399,Sorry for the inconvenience.,non_debt,-
incubator-mxnet,2220,review,64336817,doesn't really make sense to use ignore for single output since you can just drop this instance alltogether,non_debt,-
airflow,2338,comment,438822212,Thanks @alekstorm . Appreciate that.,non_debt,-
beam,10367,review,364531963,"Yes, here:
It would be nice if we could get rid of this case somehow, because by making this optional we have to deal with the possibility of `pvalue.pipeline` being `None` throughout the code base.  I went back and forth on whether to make the arg optional or simply ignore the error in the method above, but I think I decided that the method above was a common case and thus we needed the protection against None-values throughout the code.",code_debt,low_quality_code
carbondata,1996,comment,368314877,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3888/",non_debt,-
zeppelin,2698,description,0,"This PR modifies the display of the name of the note.
Now, instead of the full path, only the name of the note is displayed.
Improvement
[ZEPPELIN-3007](https://issues.apache.org/jira/browse/ZEPPELIN-3007)",non_debt,-
hudi,1827,summary,0,[HUDI-1089] Refactor hudi-client to support multi-engine,non_debt,-
nifi,3168,comment,448301385,@pvillard31,non_debt,-
cassandra,470,summary,0,CASSANDRA-15630 fix testSerializeError,non_debt,-
reef,234,review,32866959,Remove the commented out code.,documentation_debt,low_quality_documentation
nifi,4510,review,490298103,Nice catch. Originally I intended to use the return value,non_debt,-
openwhisk,355,review,62893996,I definitely missed that one extended the other. Thanks for catching this.,non_debt,-
kafka,7132,comment,517025498,Merged #7132 into trunk,non_debt,-
trafficserver,2654,description,0,"This is a set of management APIs that was used by the Web UI, for creating,
modifying and deleting configuration files and entries. This is another step
towards removing the requirements for writing to configuration files.",non_debt,-
kafka,2618,comment,290636956,@bbejeck It seems your newly added test did hang in a build: would you mind taking a look? https://builds.apache.org/job/kafka-trunk-jdk8/1390/console,non_debt,-
streams,11,summary,0,Streams 33,non_debt,-
trafficserver,3340,description,0,356066-3340 description-0,non_debt,-
spark,31682,comment,788171596,Can we fix the problem if we use different `Pickler` instances for sending the input data to python and sending the UDF results to JVM?,non_debt,-
kafka,6399,review,263970330,"Besides source and sink nodes, there are other places in the middle of the topology that would pass in serdes:
1. Materialized.
2. Grouped.
3. Joined.
We need to cover those cases as well.",non_debt,-
hudi,2403,summary,0,[HUDI-913] Update docs about KeyGenerator,non_debt,-
flink,3733,review,112818005,"hi, there is a `.select('count.sum)` after `groupBy('word).select('word as 'word, 'num.sum as 'count)`",non_debt,-
incubator-doris,3681,description,0,"Fix #3680 
Change the load label of audit plugin as:
`audit_yyyyMMdd_HHmmss_feIdentity`.
The `feIdentity` is got from the FE which run this plugin, currently just use FE's IP.",non_debt,-
carbondata,2738,comment,424042054,"Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/632/",non_debt,-
geode,1921,description,0,"         * list lucene index gfsh command will now display on of three states
	* NOT_INITIALIZED if the index is present in only in defined map
	* INITIALIZED if the index is present in the index map
	* INDEXING_IN_PROGRESS if the index creation is in progress.
Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.",non_debt,-
kafka,9007,review,454569003,"That's great point. At the moment, I think that we are not consistent about this. Some are package private and some are not. The advantage of keeping it public is that it allows to use the class in unit tests which resides in other packages.",code_debt,low_quality_code
incubator-heron,1341,comment,244676446,@nlu90 - Should we remove the `local` prefix in pom.xml?,non_debt,-
spark,717,comment,43008847,I think the current approach is fine. I'm going to merge this.,non_debt,-
ignite,5410,description,0,31006158-5410 description-0,non_debt,-
incubator-mxnet,16790,review,345358591,This sync is not necessary.,code_debt,complex_code
apisix-dashboard,1277,summary,0,feat: return back the data just created via POST method in manager API,non_debt,-
spark,6059,comment,101015161,"Sorry I wasn't being clear, but my point is that it doesn't fail all the time, and when it does fail it delays other unrelated patches being merged. This is why we ignored it for now. We will reenable it later before we actually ship the release but we need to find a way to actually fix the flakiness before we do that.",test_debt,flaky_test
tajo,796,description,0,I fixed about repair partition and verified that this patch had run as expected on testing cluster.,non_debt,-
superset,3521,review,141178274,"Yeah it's at the frontier of data/metadata, though to me annotations are closer to metadata.",non_debt,-
carbondata,2374,comment,397584434,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/6353/",non_debt,-
hadoop,661,review,272010158,Maybe change this to loadSchemaFromXml?,non_debt,-
spark,30004,comment,741941170,cc @Ngone51,non_debt,-
beam,2097,comment,282329147,On the jenkins slave pip2.7 is unavailable for same reason. Unfortunately I don't have enough privileges to see how the slave is configured.,non_debt,-
druid,1912,review,44027306,"Usually its corner cases like people putting quotes and/or commas in their values.
The ""safe"" thing to do (and what we do elsewhere) is to add each value individually with an invalid UTF8 character (like `0xFF`) after each string.",non_debt,-
storm,1481,comment,225246465,"Thinking further on it, STORM-1876 is needed when building from the source. So it should be fine if not included in the release candidate. 
@ptgoetz  - +1 for merging this now and going ahead with release.",non_debt,-
flink,9061,review,304277888,How about using `ObjectIdentifier` here?,non_debt,-
superset,10836,review,489920646,"correct. Dashboard is running in synchronized mode, there is no query id passed from query engine to dashboard. While in SQL lab, which is running in asynchronized mode, query id is saved into database, and celery Worker will update query status.",non_debt,-
tvm,3597,description,0,This PR is the implementation of pre-quantized fully connected op.,non_debt,-
carbondata,2623,comment,413818682,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/7939/",non_debt,-
spark,29950,review,503666896,Added one comment.,non_debt,-
incubator-pinot,5987,comment,688946724,"Can we parameterize `JAVA_VERSION` and default still to 8?
So out build script can set `JAVA_VERSION` to 11 if needed.",non_debt,-
drill,720,review,96553793,"Which repositories have the dependency of index-common, indexr-segment?  I tried to build drill after applying the patch, and hit mvn dependency error.",non_debt,-
spark,12376,comment,209828163,"Hi, @markhamstra ! Thank you for commenting. 
I agree with your viewpoint. So, this PR has a meaning to add just a function, `bround`; not a HQL language level meaning. 
In terms of semantics, this is the same implementation with Hive. The following is [Hive code from the Hive master branch](https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/RoundUtils.java).
By the way, for the last issue, I think in a different way.
In order to become less and less directly dependent on Hive, we need to provide this as a Spark SQL function.",non_debt,-
flink,4553,comment,322854404,+1 I like these improvements.,non_debt,-
carbondata,3272,review,292772374,"yes, there issue is reported by tester. He use this csv file, it's difficult to replace it. I need reproduce it and validate with this file.",non_debt,-
incubator-dolphinscheduler,4896,review,584050628,keep old-style maybe better,code_debt,low_quality_code
kafka,1095,review,60755927,This should be `byte` instead of `Byte`.,non_debt,-
spark,24983,review,308947640,"Use the scala-way.
`val chromos = Seq.fill(determinPopSize(conf, itemsMap.size) {
  Chromosome(conf, shuffle(itemsMap), conditions, topOutputSet)
}`",non_debt,-
ambari,2366,summary,0,[AMBARI-24632] APT/DPKG existence check doesn't work for system packages (dgrinenko),non_debt,-
airflow,730,review,85289615,We already have a test plugin in `tests/plugins/`. This is less complete than that one and should probably not be added here. We can consider moving the other one (in a separate PR) to serve as a sample.,architecture_debt,violation_of_modularity
trafficserver,3903,comment,402768113,"I think it's good to take opportunities like this to do more general code cleanup. I'm on vacation this week, I will look at splitting it in to two commits when I get  back.",code_debt,low_quality_code
apisix-dashboard,379,review,471185309,We can specify that the document `contains online demo and screenshots`,non_debt,-
spark,2922,comment,60553132,"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/22276/
Test FAILed.",non_debt,-
spark,5511,description,0,"Even if we wrap column names in backticks like ``a#$b.c``,  we still handle the ""."" inside column name specially. I think it's fragile to use a special char to split name parts, why not put name parts in `UnresolvedAttribute` directly?",code_debt,low_quality_code
carbondata,3131,comment,465625917,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2587/",non_debt,-
incubator-dolphinscheduler,528,summary,0, update worker get task from queue,non_debt,-
beam,123,review,58893436,This was a common thread of feedback from other PR's as well-- I'll add some constructs to register values if not default.,non_debt,-
shardingsphere,4215,summary,0,Add IdentifierValue,non_debt,-
dubbo,2282,description,0,"#1728
System property dubbo.service.delay invalid, and we fix this bug.
this bug caused by  appendProperties(this) called in doExport(), and we put appendProperties(this) in front of doExport().",non_debt,-
airflow,2756,comment,343599537,@bolkedebruin nice! super excited about this,non_debt,-
beam,406,comment,224704830,So that's weird... all tests pass both in the IDE and with maven verify from command line. I'd be happy to blame Jenkins but Travis fails the same way.. üòÆ,non_debt,-
lucene-solr,357,review,376935901,"I like the `boost` package - I'm already thinking about a `TypeToBoostTokenFilter` that would automatically boost tokens marked with a `SYNONYM` type for example, and there are probably other boosting filters we can come up with, so a package to collect them all makes sense to me.  I prefer to group packages by functionality rather than implementation.",non_debt,-
apisix,957,summary,0,add doc FAQ about a/b test,non_debt,-
spark,19813,review,156552741,"So, shall we keep it as it is or restore it back?",non_debt,-
spark,25785,comment,531497209,"I see, can that thread be a daemon? If System.exit is viable (i.e. immediately stopping daemon threads) then it should be. But if not, then yeah such a thread needs to be shut down cleanly somehow during the shutdown process. This could be a shutdown hook.",design_debt,non-optimal_design
airflow,2011,comment,274330617,33884891-2011 comment-274330617,non_debt,-
drill,1593,description,0,"Currently, the WebServer side needs to process the entire set of results and stream it back to the WebClient, which puts immense pressure on the WebServer when rendering the resultset. 
Since the WebUI does paginate results, we can load a larger set for pagination on the browser client and relieve pressure off the WebServer to host all the data (most of which will never be streamed to the browser).
e.g. Fetching all rows from a 1Billion records table is impractical and can be capped at (say) 1K. Currently, the user has to explicitly specify LIMIT in the submitted query. 
An option is provided in the field to allow for this entry, and can be set to selected by default for the Web UI.
The submitted query indicates that an auto-limiting wrapper was applied.
In addition, the resultset is now configurable to allow for the default number or rows displayed per page to be changed from 10 to anything that the user might want.
Configuration additions in `drill-module.conf` (changes should be made in `drill-override.conf` ):
Screenshot with default (unselected) set to limit of 23:
Screenshot with default rows per page changed to 12.
Screenshot of the profile indicates the auto-limit having been applied:",non_debt,-
trafficserver,3044,comment,365999726,"Hmmm‚Ä¶ Seems @SolidWallOfCode fixed the issue 2 days ago with this commit https://github.com/apache/trafficserver/commit/e459667aad1756bad55db88235e8aa6dd229da61
I guess this PR is now useless.
Closing it",non_debt,-
airflow,1562,comment,223208225,"Yes it conflicts, thanks for finding this Max. Under the new model (you can see in my PR that max linked https://github.com/apache/incubator-airflow/pull/1525 ) you would create a dependency class for future succeeded.",non_debt,-
kafka,7994,review,373361209,"nit: ""an none"" seems ungrammatical",code_debt,low_quality_code
ozone,2130,description,0,"Addendum patch to fix compilation error
https://issues.apache.org/jira/browse/HDDS-5072
Existing testsuites.",non_debt,-
apisix,3694,description,0,‚Ä¶lled,non_debt,-
hadoop,1884,summary,0,HDFS-15202 Boost short circuit cache,non_debt,-
hudi,1574,comment,633337641,"@garyli1019 : Thanks for your suggesstion, i'll add usage in ITTest and try my best to mak it run in docker. If still failed, an available script is a good choice.",non_debt,-
gobblin,199,comment,118981706,`task.markTaskCompletion` should only be called if all retries fail. So calling it in a finally clause is not appropriate. What we can do is to make sure it is called if any `Throwable` is thrown.,non_debt,-
flink,5593,comment,392814940,"hi @tillrohrmann can this PR been merged into master branch, so that we can close it?",non_debt,-
airflow,67,summary,0,Adding pre and post execute hooks to BaseOperator,non_debt,-
hudi,1253,review,384914757,Sounds good.,non_debt,-
nifi,2940,comment,411414481,"Performed a build and verified both the Maven and the Hub (using 1.7.0) variants and worked as anticipated.  I think this should make it a bit easier for folks to migrate between versions.
Thanks for taking care of this and I'll get the merge in.",non_debt,-
incubator-pinot,4777,review,344013968,"try to make comparisons more specific. Since you know there are only 2 parts and parts cannot be negative, avoid the ""less than"" comparison and instead use `parts.length != 2`",code_debt,low_quality_code
karaf,961,description,0,Backport of the fix of issue KARAF-5796 into the karaf-3.0.x branch.,non_debt,-
systemds,349,comment,274585312,"@niketanpansare I agree that this move towards Scala support is really a necessary step in the evolution of the project since we run on Spark, and it is a step that we need to take. I just wanted other Eclipse developers to be aware that this PR represents taking that step.
Also, thank you for the Scala IDE setup information. It is good to hear that the integration is clean.",non_debt,-
carbondata,2396,comment,399415726,"Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/5304/",non_debt,-
carbondata,1653,comment,351470763,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1940/",non_debt,-
kafka,937,comment,186170645,Review by @guozhangwang.,non_debt,-
trafficserver,1033,comment,248191505,Linux build _failed_! See https://ci.trafficserver.apache.org/job/Github-Linux/732/ for details.,non_debt,-
kafka,2950,comment,317639576,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6323/
Test PASSed (JDK 7 and Scala 2.11).",non_debt,-
incubator-doris,1439,description,0,"1 . variance same as var_pop
2. stddev same as stddev_pop",non_debt,-
spark,27395,comment,588347501,Failed test seems irrelevant: org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.(It is not a test it is a sbt.testing.SuiteSelector),non_debt,-
ozone,1503,review,506851310,Can we reuse an existed method OzoneFSUtils#addTrailingSlashIfNeeded(keyName) instead of?,code_debt,low_quality_code
iceberg,1890,review,538631234,Shall we call it `defaultCatalog`?,non_debt,-
attic-apex-core,33,description,0,41348333-33 description-0,non_debt,-
nifi-minifi-cpp,367,comment,402224300,"code good, verified L&N, verified builds on a few different systems without issue.  will merge",non_debt,-
trafficcontrol,3075,comment,443905502,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/2874/
Test PASSed.",non_debt,-
kafka,6185,review,249982093,"This is the actual fix: as there are two state transitions, we cannot simply check for six transitions, because if we check too early, and only five transitions finished (or maybe none), `stateListener.numChanges` might still be at 4 or 5.",non_debt,-
flink,3336,comment,282036668,"Thanks a lot for working on this @shixiaogang. ! üòÉ 
I just merged your PR, could you please close is.",non_debt,-
spark,13152,review,76170720,"add a simple classdoc here - something like ""A TopologyMapper that assumes all nodes are in the same rack""",non_debt,-
incubator-doris,2922,review,380016716,üëå,non_debt,-
camel-quarkus,1483,description,0,Fix #1468,non_debt,-
spark,28038,review,402528402,"Yes. Talking with @dbtsai he wanted to add a lock on the blocks inside of `doCleanupShuffle`, but given that the only price is duplicated messages to the executors I'm not sure its worth the overhead of keeping track of that many locks.",code_debt,low_quality_code
spark,31682,review,584742586,can we add an end-to-end test to show the correctness bug? I'm still not very sure why we compare rows with different schema. Sounds like something we should forbid at the analysis phase.,non_debt,-
tvm,7313,review,586937376,"Yeah, I've also had some discussions in our weekly sync while didn't figure out any better solutions.
There're several reasons:
1. Different ops have different requirements over specific inputs;
2. While the problems is in a subgraph generated in Relay Integration, the placeholder are all the same, we can not differentiate them from tag, name or any other way, even the order of inputs are not guaranteed.
Current approach is to merge all specific inputs checking to this function, at least they have a same entry here. For the other ops, you have to add their own check functions below.",non_debt,-
incubator-pinot,6690,review,596257840,You might want to change the logger in `BaseCombineOperator` to private?,non_debt,-
spark,21976,summary,0,[SPARK-24909][core] Always unregister pending partition on task completion.,non_debt,-
airflow,14219,review,584775664,"Ideally you should get the token by calling a python method directly -- by calling this endpoint you are ""retesting"" the login endpoint, which it would be better to avoid.",code_debt,low_quality_code
incubator-mxnet,18560,description,0,"Currently, 1.6.x branch is failing on multiple pipelines
1. centos-cpu & centos-gpu pipelines due to 
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline
2. Flaky test on unix-cpu mkldnn
3. Edge pipeline",test_debt,flaky_test
spark,4588,review,26225879,"Wait. As a sender, I don't expect a reply to be sent to my `receive` method. I expect the reply to be sent back to the point in the code that made the call - which is why I've mentioned before that `sendWithReply` should return a Future.",non_debt,-
carbondata,200,review,82923303,"hi sujith
i am thinking if user already trim the data with the option setting,then when user query with some space filter,it would no getting result.",non_debt,-
druid,6683,review,244036188,"`which is different with Announcer` means `NodeAnnouncer announces single node on Zookeeper and only watch this node`, while Announcer watches all child path, not only this node",non_debt,-
hbase,1653,description,0,‚Ä¶gionServer can fail with retries exhausted on an admin call.,non_debt,-
airflow,4963,comment,475922883,"# [Codecov](https://codecov.io/gh/apache/airflow/pull/4963?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/4963?src=pr&el=continue).",non_debt,-
hbase,2876,comment,763383475,"This problem was introduced by private repository, currently no need to apply this PR, so close this.",non_debt,-
spark,25782,comment,531650889,"jenkins, retest this, please",non_debt,-
spark,14656,review,74853965,"sorry I literally meant the SQLQueryTestSuite, which is file based :)",non_debt,-
thrift,1496,comment,371441144,"I finalized the changes, however there are a couple of errors in my builds which I think are irrelevant to my changes.
Here are the links to my build results (please let me know if you cannot access them):
Travis-CI: https://travis-ci.org/ozymaxx/thrift
AppVeyor: https://ci.appveyor.com/project/ozymaxx/thrift/build/1.0.0-dev.42
Here are the error descriptions:
On Travis CI `#19.7`:
On Travis CI `19.11`:
On AppVeyor `Environment: PROFILE=CYGWIN, PLATFORM=x64, CONFIGURATION=RelWithDebInfo, DISABLED_TESTS=(ZlibTest|OpenSSLManualInitTest|TNonblockingServerTest|StressTestNonBlocking)`:",non_debt,-
airflow,6281,review,339296209,"Actually you should not need to disable it here, because you documented the class (starting from line 29). It should just work if you remove the complete docstring here.",non_debt,-
spark,26159,comment,544371364,"HadoopMapReduceCommitProtocol is somehow a custom committer in Spark. Actually we use it with dynamicPartitionOverwrite in this way, at least for data source.",non_debt,-
trafficserver,347,summary,0,TS-4038: Redundant `isdigit(b)` in `LogFormat::parse_escape_string,code_debt,complex_code
spark,20387,comment,361368296,"Mutable nodes violate a basic assumption of catalyst, that trees are immutable. Here's a good quote from the SIGMOD paper (by @rxin, @yhuai, and @marmbrus et al.):
Mixing mutable nodes into supposedly immutable trees is a bad idea. Other nodes in the tree assume that children do not change.",non_debt,-
dubbo,4526,review,303318487,"it is a bad idea to have two *isSetter* methods, pls. consider to combine both into one single isSetter method",code_debt,low_quality_code
bookkeeper,1410,review,190751880,will do,non_debt,-
iceberg,1393,review,479567345,Should we do the operations above this point in the transaction as well? That seems reasonable to me. I'm not sure why we don't in other places.,non_debt,-
tinkerpop,1308,review,512632352,"nit: Capital ""A"" in ""Authorization"" please since it's a title. 
nit: There's a bit more formatting to do in the text like enclosing class names in backticks.
I think it would be worth adding some note here to providers to say that while Gremlin Server supports this authorization feature it is not a feature that TinkerPop requires of graph providers as part of the agreement between client and server. Graph providers may choose to implement their own methods for authorization in the manner they see fit.  I would say a similar ""IMPORTANT"" callout box should probably be added to the reference documentation to alert users to this notion. Finally, as you draw closer to a final body of work, this is a neat new feature that should have upgrade documentation. (and perhaps more user facing documentation?))
UPDATE: I read a bit further on and saw you linked from the user documentation to this page....that could suffice, but if I'm thinking of this feature right I sense that users will write these authorizors and i think it could be a popular feature which means more front facing documentation.",documentation_debt,low_quality_documentation
ignite,7851,review,430399878,"It is not correct javadoc ""list"", it should be written in HTML style like
So, it is a test though... who cares.",code_debt,low_quality_code
hudi,1274,review,370818776, I thought having all metadata constants in one place would make it simpler. This is used in reading archived commit. I can move the constant to ArchivedTimeline if you think thats a better place.,code_debt,low_quality_code
ignite,4434,review,233787764,Just include it into the next `if` clause.,non_debt,-
druid,1978,comment,173417227,:+1: after my comment around the README fix is addressed,non_debt,-
nifi,777,comment,239009886,"No worries! I think it is easiest to do development from a branch other than your master branch. Here is a sequence of commands that might work:
git checkout -b add_escape_utils
git fetch upstream master
git reset --hard upstream/master
git cherry-pick 2a4f1bd68a17d3fc765a9d6c8a4e48ef21c77bfe
git cherry-pick 37651b83bca350dd3c50da16e6b4eee0f2768a7f
git cherry-pick b3fbe7540ffa47905bbbfa172ca08da40387cc58
git cherry-pick 4e6a0d57958a3575b1bfd7647538aaf61b28c712
git cherry-pick fd63fe95eaadb31209bf245e393e60d0030dbb88
git checkout master
git reset --hard add_escape_utils
git push -f origin master
That would update this PR. For future contributions you may want to create a new branch off upstream/master and do your development there, it makes rebasing a lot easier. For example, in your local master, every time you do a ""git pull upstream master"" you will get a merge commit (like you see in this branch/PR).
If something gets messed up and (god forbid) you don't have your commits after any of this, I took the liberty of doing the above and pushing to my fork: https://github.com/mattyb149/nifi/tree/add_escape_utils
Looking forward to having this contribution in NiFi, thanks again!",non_debt,-
helix,1472,comment,710682396,"This PR is ready to be merged, approved by @kaisun2000 
There is connection leakage in CustomRestClientImpl and causes timeout waiting for connection.
Fix this issue by consuming entity and releasing the content stream and connection.",design_debt,non-optimal_design
druid,6007,comment,405648789,Filed https://github.com/apache/incubator-druid/issues/6009,non_debt,-
spark,8734,review,50617823,"Hmm, I just want a test case to show it actually order the bins by soft prediction. Although @jkbradley suggested we should use directly `binsToBestSplit`, but in order to do that, we also need to expose many details of `findBestSplits` too, e.g., `binSeqOp`, `getNodeToFeatures` and `partitionAggregates`...etc.",non_debt,-
beam,2504,summary,0,Triggers: handle missing case,non_debt,-
zookeeper,904,review,274872856,This method is only used in the test currently. Please add it to `shutdown()` methods of `ZooKeeperServer` and `QuorumPeer` classes.,code_debt,low_quality_code
incubator-mxnet,12918,review,227170725,"Can you use something like this instead?
Otherwise it throws an uncaught exception on CPU only machines.",code_debt,low_quality_code
flink,10017,review,349058826,To check whether a file is zip-format we must read its actual content. If user specifies a DFS url the check will introduce additional IO. Suffix checking is also hard because there are too many file formats are actually zip format. Maybe rewriting the doc string in detail to tell users what file format are actually supported is a better choice?,documentation_debt,low_quality_documentation
flink,13697,comment,718463957,"Thanks for your review, @kl0u , @gaoyunhaii !",non_debt,-
incubator-heron,2295,description,0,"New commands:
  heron examples list
  heron examples run <cluster> <example-id>
This patch is to provide easier commands to get the heron examples up and running.",design_debt,non-optimal_design
superset,7788,comment,506845639,"Welcome, @cguan7!",non_debt,-
ozone,91,description,0,"Modify the if-condition in ```hadoop-ozone-manager.sh```
https://issues.apache.org/jira/browse/HDDS-2361
Ran the command in ```hadoop-ozone/```:",non_debt,-
spark,24490,comment,488126326,Retest this please.,non_debt,-
rocketmq,1563,summary,0,[ISSUE #1564]Fix the ip filter logic in IPv6/IPv4 coexist environment,non_debt,-
kafka,2926,comment,297827982,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3223/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
incubator-pagespeed-ngx,1362,review,96928694,"Yes it is.  The enclosing namespace starts on line 665, just before the definition of `struct ps_main_conf_t` above, and ends on line 1287, just after the definition of `ps_determine_port()`.",non_debt,-
spark,26682,review,371318137,"it's for correctness.  to allow users to manipulate the ExecutorResourceRequests multiple times.  The original intent was all of these classes would be immutable, but that made things less user friendly so in the original PR this class got created and users are allowed to modify multiple times and they could do it from multiple threads.  It's something I missed in the original pr rework.",code_debt,low_quality_code
spark,12474,comment,211507927,"Thanks for the pull request. We can't just change a private to public like this, because it can make the API more difficult to maintain in the long run. Can you justify more why this is needed, and why you can't work around it from the user side?",design_debt,non-optimal_design
carbondata,240,review,84475433,Map is not supported yet so thrown unsupported exception for Map,requirement_debt,requirement_partially_implemented
kafka,6601,review,276552165,Mentioning this on the KIP slipped.,non_debt,-
incubator-pinot,1260,description,0,"currently, we never update Routing on instanceConfigChange callback because of the following code in HelixExternalViewBasedRouting.isRoutingTableRebuildRequired.
Pinot expected Helix to populate the version field but that does not seem to be the case. Will file a separate Helix ticket to fix this in Helix. 
For now, since we already read the versions in processInstanceConfig method, the fix was to set the version in the instanceConfig",non_debt,-
spark,19911,review,155335791,"@dongjoon-hyun to be clear, I think there are 2 problems:
1. The PostgresDialect indicates that `CASCADE` is enabled by default for Postgres. This isn't the case as the Postgres docs show. 
2. As you correctly mention (this is what in my previous comment), Spark doesn't use `CASCADE` at all, which, especially considering the method this PR edits, is a bit odd I think. I plan to open a different JIRA ticket for this, and add it. This will be more work, and is outside the scope of the current JIRA.",code_debt,low_quality_code
reef,1473,review,198332781,"I did -- so setting SkipMessage=null has the same effect as the SkipTestException since our version of XUnit does not support it. But I think the comment of ""// Use null to run tests"" could be a bit more explicit. This is more what I was thinking:",code_debt,low_quality_code
reef,1487,review,258710277,Note that we don't lock the `_schedule` here - is that Ok?,non_debt,-
hbase,2392,comment,694938784,"Some updates here, I found the place where we fail, it is inside startMiniCluster, where we will scan meta to see if cluster is up. Still need to dig why we can not get meta location from MasterRegistry.
And #2420 is for the missing stack trace. We just throw out the exception in scanner.next so it lost the actual stack trace which makes really hard to find out the place we fail.",non_debt,-
thrift,162,comment,49839795,"@bufferoverflow I'm sorry for not explaining, the intention was merely to initiate a discussion. I would love the option to generate only ARC-compatible code, with a CLI flag/option which would remove lots of boilerplate setters/getters. I will check JIRA and link a ticket if that's not already there in some form. Thanks",non_debt,-
tvm,7053,summary,0,"[wip][auto_scheduler] buffer support, correctness check",non_debt,-
spark,15329,comment,252432203,"Actually, when will a user want to specify non-nullable for any json field? I am not sure if we are actually addressing the right problem. I am wondering if we should just not allow non-nullable fields for json.",non_debt,-
nifi-minifi-cpp,888,review,483008523,"I think this error should be handled in onSchedule. 
This function (processbin) is called runtime, config issues shouldn't be handled here.",non_debt,-
spark,8253,review,37247740,"Done in #8255, will rebase",non_debt,-
arrow,5969,review,363018200,"Thanks for your comments.
I have added a note for this.",non_debt,-
hudi,1457,comment,611819504,What will happen if there is incompatible message in Kafka? Will pipeline stall? What will be the way to fix it without purging whole kafka topic?,non_debt,-
tinkerpop,377,comment,239186446,"This solution makes sense to me -- no new `SelfEdge` class and we now have a test case. Thanks.
VOTE +1.",non_debt,-
iceberg,843,review,396016799,Will add orc once ORC patches are committed which read/writer Iceberg generics,non_debt,-
spark,11236,comment,189069233,"LGTM,
thanks for writing these benchmarks. 
I think moving forward, I agree that ColumnVector is a natural data structure to decode into, but we should probably not add this logic directly into those classes just from a code maintenance point of view. I think exploring the parquet encodings makes sense but let's start by benchmarking those and see if they have the right performance characteristics.",design_debt,non-optimal_design
nifi,2828,review,199534888,If there were jiras around what needed to be done or worked out I'm sure some people would want to work on them,non_debt,-
cloudstack,4141,comment,669956049,@rhtyd it requires haproxy 1.8+ for http2 support. As I remember haproxy 1.8 is installed in debian10 systemvm template. No other changes is needed in systemvm template,non_debt,-
camel,2976,description,0,‚Ä¶xchange received (#2669),non_debt,-
drill,159,review,39996293,"this is not important, but you can just pass nameIdentifier and .toString() will be called implicitly",non_debt,-
gobblin,1283,review,80815984,Changed it to `GreedyRequestAllocator`,non_debt,-
zeppelin,3897,description,0,"Travis-CI:
 - remove all environment variables from the build matrix, to use the same cache for all jobs
 - activate the mysql service only when this service is needed
 - activate the xvfb service, when is necessary and possible
 - removed Bower Caching to remove too many complicated lines in Travis-ci
 - Giving the test names
 - Installing R only once with conda, previously it was installed twice with 'testing/install_R.sh' and 'testing/install_external_dependencies.sh
 - remove the R-Cache, because the installation with conda is quite fast
 - Delete 'test/install_R.sh' because it is no longer used
Other:
 - Ignore the tests in 'HeliumApplicationFactoryTest.java' to get the JUnit tests running in the IDE + remove exclude in 'travis.yml
 - Remove deprecation warning in maven-surefire-plugin
 - Helium works better with an absolute path, because a relative path in PATH, is not a good idea for local testing
 - Remove JVM language dependent asserts
Improvement
* https://issues.apache.org/jira/browse/ZEPPELIN-5024
* Travis-CI: https://travis-ci.org/github/Reamer/zeppelin/builds/723116251",code_debt,dead_code
kafka,5874,comment,435637476,Failing Java11 tests unrelated:,non_debt,-
systemds,1023,comment,728301245,"LGTM - thanks for the cleanup @Shafaq-Siddiqi. The scenario with 10 components was still failing, but after some debugging it turned out this was due to Kmeans not converging. During the merge I fixed the hard-coded maximum iterations for Kmeans, some formatting issues, and vectorized part of the cholesky computation. With those changes it ran fine.",code_debt,low_quality_code
spark,30212,review,601172541,Done,non_debt,-
beam,13984,comment,778422690,"The code that introduces a cleanup probably comes from:
The test creates a bunch of pipeline objects with references to them in the interactive environment. The test tries to find out whether a `cleanup(pipeline)` is invoked when explicitly tracking all user pipelines if some of the pipelines held in the environment is no longer in scope. Of all the pipeline objects created, only one would be cleaned up (because we intentionally put a string typed ""pipeline"" in the interactive environment so it would just ""disappear"" when tracking pipeline-typed objects), so cleanup should just be invoked once during the call. The changed test still tests this logic.
R: @TheNeuralBit 
PTAL, thx!",non_debt,-
flink,4684,review,139929465,Perhaps also mention the option to specify the target path?,non_debt,-
spark,31296,review,562868528,"This sounds a little risky to me, `A process is invoked even for empty partitions`.
This may cause a hang situation if the command is expecting input.
For example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.
If we are okay, could you add a test case of empty partition to make it sure that we handle those cases?",code_debt,low_quality_code
hbase,2483,review,521531128,Ya this is in RsRpcServices under scan() so should be ok. One thing I noticed is that - once  a scan is created and a next() call happens does the rpcScanRequestCount becomes 2 or is it 1?,non_debt,-
incubator-mxnet,12572,review,218248701,will add UserWarning and I checked the target file existence before starting downloading at line 280,non_debt,-
airflow,3443,comment,400939280,"I'm not convinced this is the right fix.
1) Calling `basic_config` is something we want to avoid (we used to have it all over the place and it caused problems, so we now only want to configure logging from the one place)
2) Normally we add a default config entries to airflow/config_templates/airflow.cfg. I think the fix here would be to add values for the things being complained about.",non_debt,-
spark,21127,comment,383582817,We already have SparkBuildInfo for this purpose.,non_debt,-
druid,2025,comment,166169384,"@fjy Ah, sorry. I've missed your comment.",non_debt,-
airflow,2095,summary,0,Dummy PR,non_debt,-
arrow,7818,review,459082935,"I think that it has a problem when we use shared utf8proc for the arrow static library.
How about using `INTERFACE_COMPILER_DEFINITIONS` property like https://github.com/apache/arrow/blob/master/cpp/cmake_modules/ThirdpartyToolchain.cmake#L2125-L2126 ?",non_debt,-
gobblin,2645,comment,497887473,@sv2000 can you merge? thanks,non_debt,-
cxf,713,description,0, Fixed mask for XML elements containing attributes,non_debt,-
geode-native,201,review,165239532,done.,non_debt,-
kafka,7421,comment,547852218,@hachikuji Always routing to controller could bring a consistent view.  I believe KAFKA-9096 was triggered by such an inconsistency caused by asynchronously updates. What do you think?,non_debt,-
systemds,172,description,0,45896813-172 description-0,non_debt,-
incubator-doris,3417,summary,0,[metrics] Make DorisMetrics to be a real singleton,non_debt,-
airflow,1352,comment,309399231,"I'm sorry for commenting on such an old (and closed) PR.
Is this feature completely implemented?  It seems that the SchedulerJob class uses a completely different method for loading DAGs than the webserver etc. and it will not schedule tasks from a packaged dag since it only considers raw python files.",requirement_debt,requirement_partially_implemented
hadoop,1790,review,386485353,"org.apache imports need to go into their own block just above any static imports, ordering imports
This is to try and keep cherry-picking under control.",non_debt,-
openwhisk,2795,review,225473479,We should also update the SclaDoc for `ContainerProxy`,non_debt,-
incubator-mxnet,14614,comment,486274123,CI is not passed yet~ Please take a look. Thank you. @rongzha1,non_debt,-
drill,1562,review,239278308,Done,non_debt,-
pulsar,5191,summary,0,[Issue #5176][pulsar-broker] Fix bug that fails to search namespace bundle due to NPE,non_debt,-
hbase,1765,review,429547270,"I believe the purpose of retryCounter.sleepUntilNextRetry() should be uninterrupted sleep because RetryCounter is mainly being used by retries with sleeps and retries with different backoff policies. In such scenario, RetryCounter being a library should not ideally throw InterruptedException even if sleep is interrupted because it is being retried by clients to achieve certain tasks.",non_debt,-
hudi,765,review,376978554,@haiminh87 I was thinking if we could make this configurable in sense that have a boolean like readUsingLatestSchema with a default value of true and can be overridden via TypedProperties instance.,non_debt,-
carbondata,1998,comment,368413110,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3673/",non_debt,-
iceberg,529,review,337837687,Sorry for the delay. I think the API with location parameter should be retained as one may want to maintain the table metadata by itself. While in the case of `HadoopCatalog` we should not call that API since it is assumed the metadata is delegated by the catalog. Make sense?,non_debt,-
helix,1208,review,464529958,"Typo.
Also, why do you ever need to set the task version after-the-fact?",documentation_debt,low_quality_documentation
arrow,2890,review,230019538,There's some other cruft here that ought to be removed. I'll make some additional changes and then merge this,code_debt,dead_code
spark,14729,comment,241453606,"@viirya Yeah, a normal temporary table would be resolved as a LogicalPlan. Analyze Table does not give us any benefit there. 
However, you are also allowed to do this:
For these I would like to be able to collect statistics.",non_debt,-
spark,6415,comment,106154559,LGTM,non_debt,-
ozone,94,review,339823750,I think from below comment we don't need this new method.,non_debt,-
cloudstack,3374,comment,505694632,Packaging result: ‚úñcentos6 ‚úñcentos7 ‚úñdebian. JID-31,non_debt,-
superset,8293,summary,0,"#8292: Added Portuguese translations for ""Edit dashboard"" dropdown",non_debt,-
parquet-mr,769,review,397723382,Why do we think that an `IOException` might occur? Would a file read/write suppose to happen in an implementation?,non_debt,-
carbondata,1639,comment,350636356,"Build Failed with Spark 2.2.0, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/631/",non_debt,-
beam,2288,comment,288510782,Run Spark RunnableOnService,non_debt,-
druid,3570,review,87488947,"TreeMap was used in the original GroupByEngine code being moved, my goal with this PR was to create the interface and move type-specific code but not change functionality generally/target performance optimizations",code_debt,slow_algorithm
beam,4253,comment,351757162,"What I mean is this:
In #4027 I added a bunch of exclusions to the `pom.xml` but they turned out to be redundant and the problem was something else IIRC.
There is a subtlety that I only just sorted out fully. The patterns in a `.gitignore` are resolved relative to the `.gitignore` but _also_ directories inherit the `.gitignore` from parent directories.
So when `.gitignore` contains `vendor/` this means that when `git` is working in `sdks/go` it will inherit the `vendor/` ignore pattern, and it will work. But when the working directory is the root, `sdks/go/vendor` is not ignored.
I think it is probably best to just have full patterns in `.gitignore`, too. It also supports the same `**/vendor/` syntax.",non_debt,-
couchdb,67,comment,19819667,"So, only whitelist, right?",non_debt,-
spark,10583,summary,0,[SPARK-12573][SPARK-12574][SQL] Move SQL Parser from Hive to Catalyst,non_debt,-
flink,7716,description,0,"The AkkaRcpActor should stop itself immediately after the onStop future has been completed.
Before we sent a Kill message which enqueues into the mailbox and does not overtake messages.
Now we call Context.stop(ActorRef) which directly stops the AkkaRpcActor.
- Added `AkkaRpcActorTest#testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor`.",non_debt,-
spark,29438,review,470961136,We need to follow this restriction of Hive? IMO its okay to support any char as a line separator normally.,non_debt,-
spark,4279,review,23827446,Update `estimateDim` to use `blockInfo`.,non_debt,-
kylin,1106,summary,0,Read/write separated deployment doc,non_debt,-
kafka,223,review,40055216,perhaps wrap in Collections.unmodifiableList?,non_debt,-
spark,1541,comment,49855865,"Instead of a ConcurrentHashMap, we should actually move it to a disk backed Map - the cleanup of this datastructure is painful - which it can become extremely large; particularly for iterative algo's.
Fortunately, most cases, we just need the last few entries - and so LRU scheme by most disk backed map's work beautifully.
We have been using mapdb for this in MapOutputTrackerWorker  - and it has worked beautifully.
@rxin might be particularly interested since he is looking into reduce memory footprint of spark
CC @mateiz - this is what I had mentioned about earlier.",design_debt,non-optimal_design
incubator-heron,1678,summary,0,Display error message for NetworkErrorCode,non_debt,-
kafka,3148,comment,305072495,"@guozhangwang A minor change that has ConsoleConsumer commit offsets manually just like what MirrorMaker does. In the original version, automatic offset committing does not work well with `--max-messages` since it sees all polled messages as consumed which does not honor `max-messages` config.",non_debt,-
nifi-minifi-cpp,163,review,148353398,Can you not use the static_pointer_cast since this is already a shared_ptr?,non_debt,-
spark,9826,review,45293897,ditto,non_debt,-
pulsar,3368,review,250433084,We don't use `@Author` tag in ASF projects.,non_debt,-
flink,2966,comment,267009842,"@StefanRRichter Thanks for you work! üëç 
I merged this, could you please close the Jira issue and this PR?",non_debt,-
arrow,3925,review,266510894,"Given the desire to also compile for netstandard, this seems reasonable.",non_debt,-
hadoop,2331,comment,702380473,"+1, merged to 3.3 and trunk",non_debt,-
servicecomb-java-chassis,450,review,159653095,"mode determined by SC?
i remember changed design that determined by microservice?",non_debt,-
druid,8197,comment,517065779,"@gianm, @clintropolis: Ready for re-review",non_debt,-
hudi,1215,comment,573509113,"You are welcome. Right, we can keep improving base on it. üòÑ",non_debt,-
samza,122,review,113090448,deleted.,non_debt,-
drill,1652,review,259595997,final?,non_debt,-
kylin,477,comment,467040007,"Merged with patch, close the PR.",non_debt,-
infrastructure-puppet,452,summary,0,sync -vm3 and -vm4 yaml. promote INFRA-14849 to production.,non_debt,-
kafka,5436,summary,0,KAFKA-7164: Follower should truncate after every leader epoch change,non_debt,-
spark,928,comment,44719301, Merged build triggered.,non_debt,-
beam,5443,comment,391422222,"Pushed a new version, sql passes 100%. Lots of other things fail. Does this look reasonable? How do we get this in? Do we need to hide this behind a flag like we did with warnings as errors?",non_debt,-
incubator-heron,1969,review,122385823,"There was an intentional optimization to reduce one  `System.nanoTime()` call per tuple execution. In the new commit this optimization is removed to simplify the reasoning. 
Now it is much easier to understand.",code_debt,complex_code
storm,1608,comment,240056822,"Done. Fixed things after receiving +1 are here:
1. nimbus.clj: just log exception instead of crashing nimbus when blob-rm-dependency-jars-in-topology
2. DependencyPropertiesParser.java: check --jars parameter string is empty, and treat it as empty list (also add new unit test testing this change)",non_debt,-
flink,938,comment,125751865,"Out of curiosity: why was it failing sometimes on Travis and not locally? And how did you discover this? From the program level logs?
Another thing that came to my mind: in the long run, do we need a more complex way of configuring the retry policy? In my understanding, the number of retries is fixed. I can see an issue for very long run programs, which fail once in a while, but operate normally most of the time -- then at some point they will fail because of the fixed number of retries.",design_debt,non-optimal_design
iceberg,2132,review,562258531,From DB2,non_debt,-
nifi-minifi-cpp,969,review,551945346,"well, I am leaning towards considering a heartbeat with `""""` agent class an error (although we do not check for such cases), it all depends on what the constraints of the c2 protocol are, do you think an `""""` agent class should have the same semantics as a ""missing"" agent class, or we could even abandon the ""missing"" agent class, and use the empty string to denote an agent with non-specified class?",non_debt,-
fineract,109,summary,0,FINERACT-155 issue resolved,non_debt,-
gobblin,429,review,44487777,This can be final.,non_debt,-
beam,5027,review,179518485,Can all the println statements be removed?,non_debt,-
parquet-mr,150,comment,90990009,"could you also update this doc with the new API? https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md
It could serve as a tutorial for users to use it",documentation_debt,outdated_documentation
flink,11727,review,412069268,added case to verify temporary view masks permanent view with same name,non_debt,-
airflow,3040,comment,365879404,This option is not available in Python 2.x:,non_debt,-
spark,10860,comment,178159771,Could we merge this please?,non_debt,-
flink,15434,review,604587598,Move this method besides `snapshotState`.,non_debt,-
daffodil,408,review,488125346,"I think it would be more clear to just do string matching. E.g.
The regex just adds extra complexity. Same with setProperty.",code_debt,complex_code
spark,1029,comment,45579331,"Jenkins, retest this please.",non_debt,-
incubator-dolphinscheduler,2106,description,0,WorkerServer refactor,non_debt,-
spark,30378,comment,728755482,"Although it's important to track upstream, Java 14 is already EOL.
Only Java 8 / Java 11 / Java 17 (September 2021) are LTS.",non_debt,-
gobblin,67,summary,0,"Allow Writers, Converters, and QualityCheckers to read the same config key from different branches",non_debt,-
calcite,1584,review,347069363,"BTW, this code `this.getRoot().getCluster().getMetadataQuerySupplier().get()` confused me a lot, can you please explain why we need a fresh new `RelMetadataQuery` instance here ? Couldn't we use the instance already existing there with `RelOptCluster#getMetadataQuery` ? (I have checked that almost each `isValid` case is in the `RelOptRuleCall` circle).
Even we have to got a fresh new instance here, why we just add a new interface `RelOptCluster#getMetadataQuerySupplier` just for debugging ? Can you refactor that out ?",code_debt,low_quality_code
cloudstack,569,comment,119913350,LGTM,non_debt,-
airflow,5065,review,273068593,"This line is in the if-check block of `if slas:` https://github.com/apache/airflow/blob/fc3b45a61ac51441c3b9e4d99a20473cd3664056/airflow/jobs.py#L673
It's impossible for `len(slas)` to be zero here (correct me if I missed anything here)",non_debt,-
druid,2679,comment,198086372,@jon-wei we'll also need a TOC entry,non_debt,-
pulsar,8046,description,0,"-->
Fixes #7815 
add functions-worker process jvm metrics, use cluster-name, type lables to distinguish function-worker and function.",non_debt,-
parquet-mr,612,review,255468638,I would replace this one with a junit rule: https://garygregory.wordpress.com/2010/01/20/junit-tip-use-rules-to-manage-temporary-files-and-folders/,non_debt,-
flink,6181,comment,613341188,"I had a look at this (old) ticket/pull request today.
The main goal at the time was to enable having control over defining a key so that the records with the same are all going into the same Kafka partition.
What I found is that as part of https://issues.apache.org/jira/browse/FLINK-11693 that you (@aljoscha) created the KafkaSerializationSchema which produces a ProducerRecord which has a key and a value (both byte[]).
It looks to me like this feature is the better implementation of the goal of this ticket.
@aljoscha Can you confirm that this change already does what this ticket intends to do also?",non_debt,-
superset,11293,summary,0,fix: use dashboard id for stable cache key,non_debt,-
spark,158,review,11264777,"Instead of doing this, just prevent users from creating a StorageLevel with offHeap = true and replication = 1. Add a check in the StorageLevel constructor and throw an exception if they make one. Otherwise nobody will understand why this code was added here.",code_debt,low_quality_code
spark,23943,review,264538235,"Since we don't need the old index, shall we remove the obsolete indexes?",code_debt,dead_code
carbondata,2691,comment,418343589,"@xubo245 What does 'CSV table' mean in the title?
Can you explain why the version upgrading for common-langs is needed? Does it improve something? (Just curious)",non_debt,-
carbondata,586,comment,278029426,"Build Failed  with Spark 1.6.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/852/",non_debt,-
cloudstack,3186,review,338934210,As we mainly use those tools in CloudStack for validating,non_debt,-
pulsar,3379,review,251563529,"if we failed to create transaction, we have to error out the `toFlushList`.",non_debt,-
carbondata,259,summary,0,[WIP]Fix constants and method names,non_debt,-
incubator-heron,1629,review,97120274,resolved via https://github.com/twitter/heron/pull/1629/commits/3be17b3309d59eb60b5326d4e37c0f9c8529d3b1,non_debt,-
superset,6519,comment,448402012,"i agree no matter gunicorn/nginx support very long header or not, carry 8k referrer request header is too much. I think add an upper limit is necessary.
About losing query state, @mistercrunch do you think use localStorage to save query state is a good idea? we can use js (already existed) generate an `impression` key, unique for every page load. With same `impression` key, we store query state, so we can support redo and undo like dashboard edit.",code_debt,low_quality_code
trafficserver,7667,comment,812410036,[approve ci],non_debt,-
storm,308,description,0,"First of all, I'm sorry for mistake.
It's for reverting broken #279 and reapply correct patch (using System Environment, not JVM Property).
I reverted #279, and rearrange #279's changesets into one commit.
You can find further information from #279.
If Storm project has a rule about reverting and this PR doesn't fit, I'm sure to wait @clockfly to revert, and re-create PR.
@ptgoetz @clockfly @harshach Please take a look and comment. Thanks in advance.",non_debt,-
pulsar,9246,comment,775491410,"sorry for the late reply. @sijie 
I just notice this change and am wondering the necessity of exposing the whole admin client from functions to users. Maybe we can have some discussion when you are available",code_debt,low_quality_code
thrift,1391,review,146107107,"dreiss's copyright statement says, ""provided the copyright notice and this notice are preserved."".  I don't think we should be removing it.  You can move it down to be below the Apache Thrift license, or @jfarrell needs to approve this change.",non_debt,-
zeppelin,1471,comment,257302276,@felixcheung correct - i missed it - updated doc/examples.,non_debt,-
activemq-artemis,3470,review,600726761,"username is mandatory and should normally always be available so this is just a ""fake"" key must not collide with any username. I don't see any advantage in having it null compared to a synthetic one.",non_debt,-
fluo,1001,comment,358024831,"Travis seems to have one test failing, but when I run it locally all tests pass. I'm gonna look into this tonight.",non_debt,-
camel,1084,summary,0,fix to bom,non_debt,-
spark,22857,comment,434800043,Thanks all for reviewing! The latest change looks good to me too. Merged into master.,non_debt,-
beam,11244,comment,610407671,LGTM. Thanks.,non_debt,-
hbase,1828,summary,0,HBASE-24446 Use EnvironmentEdgeManager to compute clock skew in Master,non_debt,-
kafka,4841,comment,382894525,"Yes, I saw that earlier.
One note is all of the `StreamsUpgradeTest `s are failing, but don't use the `VerifiableProducer` at all.  
The latest PR from @mjsax upgrading streams has all streams system tests passing.  I'm thinking maybe once that PR is merged I'll rebase and try again.  
WDYT?",non_debt,-
iceberg,830,review,429030345,"I tried to remove the `try...catch...` here and let `ApplyNameMapping` to return the schema with partial IDs assigned. That causes problems when we visit the parquet type.
Firstly, the `PruneColumns` visitor assumes the `typeWithIds` is fully assigned with IDs. Its `getId` has a precondition check. A file schema with partially assigned IDs cannot pass the condition check.
Secondly, the `HasId` logic also has the same assumption, which makes `SparkParquetReader` throws NPE when it calls `filedType.getId().intValue()`.",non_debt,-
thrift,988,review,59993662,"@nsuke: any comments from your side? 
this is from f43d0ca6e57c4c30ea742e5f80e086288e999ecb",non_debt,-
spark,16280,review,92527834,"oh i know why. The `InMemoryExternalCatalog` hasn't implemented all the interfaces(e.g. some partition related ones), so we did it intentionally.",requirement_debt,requirement_partially_implemented
flink,13135,review,475413540,"We are no longer referring to field names here. Honestly I am not sure if the point makes sense with the `KeySelector` only. The way I read this point is tells you can use field names, which we discourage nowadays.
I'd rather remove the point whatsoever.",code_debt,complex_code
trafficcontrol,250,review,99355200,Can you delete this function since it's commented out?,non_debt,-
spark,12299,comment,215675192,"I went ahead and pushed my last attempt for the record, but am not going to merge this. I'm going to instead propose just the changes that seem to have no downside.",non_debt,-
phoenix,671,review,363576854,Remove stdout prints?,non_debt,-
hawq,929,review,80166393,I like this modification here!,non_debt,-
drill,1666,review,261613484,Looks like comma should be before `COLUMNS`.,non_debt,-
hudi,1149,comment,657946414,@yihua is this ready for review?,non_debt,-
skywalking,2935,review,296469085,"Don't package the image every time. The better way to mount a volume including the target tar file, you could mapping this volume to any place you like.",non_debt,-
hudi,625,comment,480006882,"If this issue is fixed, It may be possible to use ```spark-submit --packages com.uber.hoodie:hoodie-spark-bundle:0.4.5,com.uber.hoodie:hoodie-utilities:0.4.5 ...``` as hoodie utilities brings the shaded versions of hive bundled.
I can check it later to see if it works.",non_debt,-
zeppelin,1929,comment,276875597,LGTM,non_debt,-
spark,31766,review,592084726,"ah, now I got it! What a hidden bug üòÇ",non_debt,-
druid,6430,review,267088298,`hasNext()` is correct since interval upper bound is open-ended.,non_debt,-
flink,10388,comment,561631987,"Thank for the PR @HuangZhenQiu , the PR overall looks good, can you also add the cases for Blink planner ? Even though the code path looks the same, we still need the tests to make sure the logic works correctly.",test_debt,lack_of_tests
spark,11356,comment,188613807,LGTM,non_debt,-
carbondata,3293,comment,503188255,"Build Success with Spark 2.3.2, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/11891/",non_debt,-
drill,1799,review,289393173,And here also.,non_debt,-
streams,5,summary,0,STREAMS-62 | Serializability in processors,non_debt,-
geode,4189,comment,544634796,"CI is failing due to flaky test, for which I created ticket GEODE-7319, and proposed solution.",test_debt,flaky_test
incubator-hop,136,summary,0,HOP-303,non_debt,-
carbondata,310,review,94093632,62117818-310 review-94093632,non_debt,-
spark,23208,comment,448728287,"@rxin, that's true, but it affects the API because it may change the structure of the builder or may be a reason to use a different pattern.",non_debt,-
arrow,9017,comment,751325986,"# [Codecov](https://codecov.io/gh/apache/arrow/pull/9017?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/9017?src=pr&el=continue).",non_debt,-
spark,29082,review,460624604,"If an exception doesn't have any error message, it can't appear in the summary.",non_debt,-
beam,13435,comment,747291549,"@rHermes yes general good practice is to separate PRs that deal with different subjects. But for cleaning PRs, it is the same subject. There a lot of leftovers in nexmark what I would like to avoid is tens of PRs that remove only a couple of fields.",code_debt,dead_code
spark,27249,review,367778667,can we add an analyzer rule to catch all the CREATE TABLE like commands and add the ownership properties?,non_debt,-
incubator-mxnet,19892,description,0,"Still getting errors due to container create calls timing out. This PR changes the logic to retry starting the container up to 3 times. If it fails after 3 times, it will raise the exception.",non_debt,-
spark,11300,comment,187079024,LGTM pending tests.,non_debt,-
spark,8744,review,39581040,`en` is unused.,code_debt,dead_code
airflow,3127,comment,374799488,@feng-tao @Fokko Thanks! :),non_debt,-
superset,7751,description,0,"Choose one
Show a generic error message and hide stacktrace if SHOW_STACKTRACE feature flag is disabled
Tested by enabling and disabling SHOW_STACKTRACE feature flag.",non_debt,-
cloudstack,1446,comment,198795359,"Jira is used for more than you might think.
It is used to build release notes as well as a metric in board reports for community activity.
There is no reason not to file a ticket, just mark it as improvement.",non_debt,-
spark,4525,review,24634640,"So, this feels a little racy, in that this thread might miss things added by the log checking thread. I'd suggest the following:
- Create a single-threaded executor for running the replay tasks
- Create a list of app infos to parse in the log checking thread, break it down into batches.
- Submit each batch to the executor
Basically, instead of having `logLazyReplay`, you'd have something like `replay(apps: Seq[LazyAppInfo])`. You don't need `lazyApplications` because that becomes part of the task being submitted to the executor, so you solve another source of contention in the code. And since it's a single-threaded executor, you know there's only a single thread touching `apps`, so it should all be thread-safe.
For testing, you can use Guava's `sameThreadExecutor()` as I mentioned, instead of the single-threaded executor.",code_debt,low_quality_code
ignite,5506,description,0,31006158-5506 description-0,non_debt,-
flink,4369,comment,317411393,"Hi @summerleafs!
I think we cannot use this approach to fix this. The most important thing is that this introduces a blocking operation (`.get()` on the future) in the call, which will make the while `scheduleEager()` call block. Since the ExecutionGraph runs in an actor-style context, methods must never block. Everything must be implemented with future completion functions.",non_debt,-
incubator-mxnet,11076,description,0,"this pull request is based on https://github.com/apache/incubator-mxnet/pull/10804
with the following further changes:
1. reduce ident changes
2. prefer cudnn depthwise convolution over mxnet implementation
still use the explicit #if #else #endif statement over
the new variable effective_num_group solution for backward code path compability
because the new variable effective_num_group may confuse readers with standard group convolution",design_debt,non-optimal_design
spark,25178,review,305176094,"you can see that if the `nnz` grows, the speed up decrese. That is because with a big `nnz`, the  searching complexity `log(nnz)` dominate the whole process. However, when `nnz` is a small number (most frequently), the conversion is relatively the main part.",non_debt,-
spark,1313,comment,49215361,"I am not sure if it is ok to wait ... This is something I never considered
from the beginning when I added process_local ... Maybe it is ok !
If it is not, then we might need to come up with something.
Unlike earlier, the noPrefs list now truely contains tasks which have no
preference (earlier task failure also ended up here) ... So maybe not
common anymore ? And so ok to wait ?
On 17-Jul-2014 12:26 am, ""Matei Zaharia"" notifications@github.com wrote:",non_debt,-
carbondata,1634,comment,350923807,"Build Success with Spark 2.2.0, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/648/",non_debt,-
hadoop,840,comment,499279608,The implementation is different from the description.  It would be easier to call it docker instead of docker-build to make the command less characters to type.,design_debt,non-optimal_design
arrow,3576,summary,0,ARROW-4076: [Python] Validate ParquetDataset schema after filtering,non_debt,-
flink,10767,summary,0,[hotfix][docs] Align the documentation of checkpoint directory to the actual implementation,non_debt,-
spark,13078,summary,0,[MINOR] Fix Typos,documentation_debt,low_quality_documentation
beam,3618,comment,317188356,"Hi @iemejia, It seems that the last failure of jenkins ( https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/13334/console ) does not relate to this PR",non_debt,-
arrow,7430,comment,644410813,Maybe adding `BOOST_SOURCE: BUNDLED` around [here](https://github.com/apache/arrow/blob/master/.github/workflows/cpp.yml#L179) would be sufficient? The job is already building Thrift.,non_debt,-
spark,22074,comment,412227375,"Unclear, but this successfully removes lintr (and associated failures), it seems? seems OK to merge if so.",non_debt,-
kafka,5797,review,224996320,Can we add a comment explaining that the expected result is 1 or 2 depending on whether both ipv4 and ipv6 are enabled or not?,non_debt,-
flink,3502,comment,285672241," Green Travis build https://travis-ci.org/DmytroShkvyra/flink/builds/209713170
This PR is renewed https://github.com/apache/flink/pull/2870",non_debt,-
rocketmq,63,comment,280247176,"@lizhanhui , # 4 is not the nessary conditions, since even it is not enble, the tpInfo's method is still used.
        if (this.sendLatencyFaultEnable) {
            try {
                int index = tpInfo.getSendWhichQueue().getAndIncrement();
                for (int i = 0; i < tpInfo.getMessageQueueList().size(); i++) {
                    int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size();
                    if (pos < 0)
                        pos = 0;
                    MessageQueue mq = tpInfo.getMessageQueueList().get(pos);
                    if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) {
                        if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName))
                            return mq;
                    }
                }
                final String notBestBroker = latencyFaultTolerance.pickOneAtLeast();
                int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker);
                if (writeQueueNums > 0) {
                    final MessageQueue mq = tpInfo.selectOneMessageQueue();
                    if (notBestBroker != null) {
                        mq.setBrokerName(notBestBroker);
                        mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums);
                    }
                    return mq;
                } else {
                    latencyFaultTolerance.remove(notBestBroker);
                }
            } catch (Exception e) {
                log.error(""Error occurred when selecting message queue"", e);
            }
            return tpInfo.selectOneMessageQueue();
        }
        return tpInfo.selectOneMessageQueue(lastBrokerName);//thrown from here
Besides, if topic route info is null which propably means user is send through my seletor method, resend should still respect user's seletor, so the same chosen broker is not enough, the same chosen queue is needed too, which may be another issue.
I guess it is not a very minal effort, since the existing interface does not record any chosen queue info.",non_debt,-
kylin,961,review,352444230,It is a `Singleton`. Should be `final` instead.,code_debt,low_quality_code
flink,5224,review,160946952,"True. But since I didn't introduce it, I'll keep it like it is (avoiding further downstream merge conflicts).",non_debt,-
spark,9297,review,46203981,"Also, is the only purpose of `sqlListener` to prevent multiple listeners from being created at the same time? If so, I think it would be better to use an AtomicBoolean so that we don't create another strong reference to a SQLListener, which might have a lot of internal state that could lead to memory leaks.",design_debt,non-optimal_design
pulsar,5767,review,352337822,"Is it better to create a new api called getPermissionsOnPartitionedTopic?
I understand if I support searching partitioned topic in internalGetPermissionsOnTopic. The output structure of getPermissionsOnTopic will be changed to
Map<String, Map<String, Set<AuthAction>> which will break API compatibility?",non_debt,-
spark,28841,review,463960973,"Could you add tests for multiple file cases? Probably, I think you might be able to use `(new File(""/tmp/file.csv"")).setLastModified(xxx)` to control timestamp.",test_debt,lack_of_tests
spark,16944,review,103047268,useless import?,code_debt,low_quality_code
beam,5764,summary,0,[BEAM-4065] Performance Tests Results Analysis and Regression Detection (do not merge - test),non_debt,-
spark,8429,comment,134717879,LGTM,non_debt,-
camel,467,comment,89960647,"I just merge the patch into Apache Camel master branch with some minor changes.
UnitOfWork can help us to do some clean up work after the exchange is processed by the route. Your test case doesn't show that part, I think we can leave the question there until we need to fix this kind of issue.",non_debt,-
pulsar,1248,description,0,"Two main goals of this PR:-
- Start Pulsar services (broker, proxy, websocket, discovery) in TLS only mode, such that they only listen on TLS ports. 
- Once ServiceUrlTls is set enableTls becomes redundant information - hence getting rid of the flag in relevant components.",non_debt,-
beam,8035,summary,0,[BEAM-6810] Disable CalcRemoveRule to fix trivial projections,non_debt,-
pulsar,8358,comment,715932777,/pulsarbot run-failure-checks,non_debt,-
carbondata,1693,comment,360073659,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3085/",non_debt,-
servicecomb-java-chassis,562,review,171433099,"why need to split tags?
servo/spectator's already have tags information.
if lost these informations caused by our mechanism, then we should change our mechanism.
this is why i DO NOT agree work on current mechanism.",non_debt,-
ignite,2422,summary,0,IGNITE-5963: Add additional check to Thread.sleep to make test correc‚Ä¶,non_debt,-
flink,9363,review,312004763,"use `names.mkString("", "")`",non_debt,-
geode,3345,summary,0,"GEODE-5971 Refactors ShowMetricsInterceptor, DeployFunctionCommand and",non_debt,-
carbondata,3498,comment,562581467,retest this please,non_debt,-
storm,429,comment,75878755,"@ptgoetz sorry for being the newbie here, but does this mean a new jar will be built of 0.9.3 that will include this? or will we have to wait for 0.10.0? 
we could really use this fix",non_debt,-
beam,7773,comment,461943922,Run Portable_Python PreCommit,non_debt,-
spark,5792,review,31554863,"I think I mentioned this earlier, but any reason not to use guava's `ByteStreams.copy()`?",non_debt,-
nifi,3672,review,318211409,I don't think it can happen here but we should be confident that none of the integration tests could accidentally delete a user's _real_ keys in the cloud provider's (or Vault's) secrets store.,non_debt,-
carbondata,3584,comment,596357267,"Build Failed  with Spark 2.4.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.4/685/",non_debt,-
airflow,3908,comment,423772330,This is the right Jira: https://issues.apache.org/jira/browse/AIRFLOW-3074,non_debt,-
flink,11679,comment,611357905,"@pnowojski  thanks for reviewing again. Tests on Travis have passed, so I merged the two commits.",non_debt,-
tvm,1168,review,189436619,can we support while?,non_debt,-
spark,86,comment,37382222,Merged build finished.,non_debt,-
systemds,877,summary,0,[MINOR][DOC] Name refactor SystemML to SystemDS in Documentation,non_debt,-
gobblin,57,comment,84181228,"I think this can be done by configuring jets3t properties (which Hadoop S3 FS uses internally). Look for the various httpclient.proxy\* keys:
http://jets3t.s3.amazonaws.com/toolkit/configuration.html#jets3t
I'll research this and comment back.",non_debt,-
hudi,2127,review,499304054,we can reuse some code in this file by pulling the common structure into a helper function ?,code_debt,low_quality_code
arrow,8715,comment,751980283,Closing in favour of #8988,non_debt,-
hawq,548,comment,204355747,+1,non_debt,-
calcite,1895,summary,0,[CALCITE-3891] Remove use of Pair.zip in RelTraitSet,non_debt,-
accumulo,1450,review,355575551,"Could pass a KeyExtent here, which would provide the information needed to implement `getEndRow()`, `getTableId()`, and  `hasTableId()`.  These are know by the initialization code.",non_debt,-
guacamole-client,122,review,161413980,"If this is an upper limit, I would suggest something like `radius-max-retries` to make this more clear.",code_debt,low_quality_code
hive,1982,review,577500461,"If I understand well this class is used to restrict casting timestamp/date to boolean, double, byte, float, integer, long, short values. I am not sure why we should deal with these checks at this point but I if we really need this then I guess it makes sense to extend it so that we apply the same checks for all types under `hive.strict.checks.type.safety` property. Should we create another JIRA for this?",non_debt,-
spark,12618,comment,217952164,Looks OK,non_debt,-
madlib,486,review,390627982,"Sounds good to me!
Also backing up this decision, I just noticed this:
Pretty clear the previous behavior was a bug!  Glad we've fixed it.",non_debt,-
arrow,7565,description,0,51905353-7565 description-0,non_debt,-
spark,20433,review,174016181,"ok, I'll do later.",non_debt,-
spark,7471,review,35220658,"I'll find a better name. IMO `Dummy` means nothing, so if names are important we could probably find a better one here. This isn't a dummy input dstream at all, it's giving access to its rate limit.",non_debt,-
spark,9682,summary,0,[SPARK-11719] [ML] Remove duplicate DecisionTreeExample under examples/ml,code_debt,duplicated_code
activemq,184,description,0,‚Ä¶irst,non_debt,-
spark,13775,review,89334966,I turn it on again in new commit in order to test if all tests can pass. I will turn it off by default later.,non_debt,-
spark,13581,comment,225032387,LGTM except a minor comment,non_debt,-
cloudstack,4574,comment,763102959,"@DaanHoogland I would limit it to ""4.15.0.0 to 4.15.1.0"", because adding it also to 4.14.1 will conflict with the the insert's in table `cloud.guest_os` during upgrade to 4.15.0.
I noticed that there is an `com.cloud.upgrade.dao` package still needed, but I hope, someone other will implement this ü§û üòÑ",requirement_debt,requirement_partially_implemented
skywalking,5827,review,521138108,"@wu-sheng I modified some content, can you help me see if this is feasible?",non_debt,-
apisix,2339,comment,706452466,@liuhengloveyou,non_debt,-
flink,10454,review,355092999,also useless,code_debt,complex_code
brooklyn-server,144,comment,222313156,"Worth adding test case(s).
Also note the related discussion the mailing list. An alternative suggestion is that we pick up _all_ files with the given name on the classpath (rather than all those in a given directory). I personally prefer the approach of all files in the directory. That would allow us to more easily incrementally add things (e.g. have separate files for upgrading between versions).
---
I wonder about a nicer package name than `org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames`. I imagine many people will just put this in their `./conf/` directory, so we don't want them to have to create a really deep nested directory.
---
Another thing we could add (in the future?) is if there are conflicting changes - e.g. A is renamed to B in the first file, and B is renamed to C in the second file. Currently, the result would depend on the other the files were processed: i.e. it could be ""B"" or ""C"". It would be good to be more predictable.
I'm fine with that being deferred for now.",test_debt,lack_of_tests
spark,16603,comment,273438586,@samkum Just this fix or combined with #16387?,non_debt,-
spark,14132,review,70927162,remove this?,non_debt,-
superset,1251,comment,251796755,"need to fix these linting errors:
/home/travis/build/airbnb/caravel/caravel/assets/javascripts/explorev2/components/ChartContainer.jsx
/home/travis/build/airbnb/caravel/caravel/assets/javascripts/explorev2/components/charts/TimeSeriesLineChart.jsx
/home/travis/build/airbnb/caravel/caravel/assets/javascripts/explorev2/components/ExploreViewContainer.jsx",non_debt,-
beam,5726,review,197573457,"Again, this is how the projects are named in master. We can change those separately if desired, but this is generally the convention used for java artifacts that depend on specific Scala binaries.",non_debt,-
beam,10367,review,364979191,Is the 'Caching' part necessary here (even if it always is right now)?,non_debt,-
activemq-artemis,3053,summary,0,ARTEMIS-2697 Avoid using raw types for Persister<T>,non_debt,-
netbeans,1453,review,316347120,That is also true for FakeDocumentModelProvider.,non_debt,-
kafka,4234,comment,345501432,"FAILURE
 No test results found.
--none--",non_debt,-
beam,10732,review,374009189,"But your idea sounds good. I think we could even remove the underscore, set `True` as the default value of `native` and expose that transform to users (not in this PR though) WDYT?",non_debt,-
camel,3934,comment,670323413,"Hi @bedlaj @omarsmak @DenisIstomin 
Thanks so much for your feedback. I think I fixed all your suggestions. Please, let me know if you have other suggestions.",non_debt,-
gobblin,2101,review,139617135,üëç,non_debt,-
spark,23955,comment,469182241,Nice! Could you add the BM results?,non_debt,-
druid,4143,comment,291628474,üëç,non_debt,-
spark,1609,review,15541257,"The reason for change to var from def is perhaps subtle.
Consider the case of :
add for mapIdToIndex with mapId 0
add for mapIdToIndex with mapId 1
add for mapIdToIndex with mapId 0 (on re-execution)
add for mapIdToIndex with mapId 1 (on re-execution)
Now both 0 and 1 will end up with the same index assigned (since it was based on mapIdToIndex.size).",non_debt,-
spark,186,comment,39081836, Merged build triggered. Build is starting -or- tests failed to complete.,non_debt,-
guacamole-client,546,review,511687968,Sure thing.,non_debt,-
incubator-brooklyn,545,comment,77748553,"thanks @neykov for your comments. I've addressed them but I think there are still problems 
Failed tests:
  ApplicationResourceTest.testDeployApplicationYaml:236 expected [simple-app-yaml] but found [Application (p6lRzMdf)]
  ApplicationResourceTest.testReferenceCatalogEntity:249 response is Client response status: 500 expected [true] but found [false]
not sure what is causing them to fail. can you @neykov help on that?",non_debt,-
kafka,8105,review,379594069,When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.,design_debt,non-optimal_design
incubator-pinot,3121,comment,416713328,"# [Codecov](https://codecov.io/gh/linkedin/pinot/pull/3121?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/linkedin/pinot/pull/3121?src=pr&el=continue).",non_debt,-
cxf,21,comment,218432577,"Build finished. 0 tests run, 0 skipped, 0 failed.",non_debt,-
flink,8600,review,289982051,Same as above.,non_debt,-
beam,5242,comment,388440426,Run Seed job,non_debt,-
bookkeeper,701,review,154556572,"see above comments, need more work on 'depends_on'",non_debt,-
spark,6973,review,33601514,can you kill this blank line,non_debt,-
arrow,8122,review,492850490,are `row_groups` also integers? 0-based?,non_debt,-
flink,3600,summary,0,[FLINK-6117]Make setting of 'zookeeper.sasl.disable' work correctly,non_debt,-
flink,517,summary,0,[FLINK-1544] [streaming] POJO types added to AggregationFunctionTest,non_debt,-
beam,7603,comment,457255545,#7611 is out to cherry-pick the Python pre-commit fix to the release branch.,non_debt,-
zeppelin,3409,description,0,"This issue is based out of comment https://github.com/apache/zeppelin/pull/3370#issuecomment-511281165, where Injellij shows unknown error.
[Improvement]
* CI should be green
* Intellij IDE should not show any error",non_debt,-
pulsar,5714,review,352358178,"""Failed to unsubscribe subscription %s of topic %s""",non_debt,-
apisix-dashboard,1576,review,597786636,"fixed, thanks",non_debt,-
flink,12574,review,438528714,"I suggest to list some of the options, otherwise, users may still don't know how to use it.",non_debt,-
couchdb,3192,comment,704052315,"Yes, more work is required to fully remove the JS support.  But, as an initial step, we could just stop running javascript target during the build process.
I'll move all these commits to main and 3.x
Thanks.",non_debt,-
spark,15963,review,88906249,Nit: the second brace and its match are redundant,code_debt,complex_code
spark,28197,comment,616372354,"For timestamp and date type, we already add `Instant` and `LocalDate` from java8 time API. In that case, we are very likely to add `Duration` and `Period` later for CalendarIntervalType later too, then it seems more reasonable to add `CalendarInterval` encoder support first as the encoder has half-implemented already.",non_debt,-
spark,28271,comment,618105766,"Hi, All.
This seems to break all Jenkins job in `branch-3.0`. Could you make a follow-up?
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-3.0-test-maven-hadoop-2.7-hive-2.3-jdk-11/278/",non_debt,-
carbondata,3014,comment,451990373,"Build Success with Spark 2.3.2, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/10457/",non_debt,-
trafodion,701,comment,246909356,Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/1133/,non_debt,-
trafficcontrol,5716,description,0,"This fixes a bug in the plugin_verifier so that it does notreport a malformed config when a plugin uses a long option
parameter in a @pparam field.
- Traffic Ops ORT
See the traffic_ops_ort/plugin_verifier/README.md and execute the
plugin_verifier against a copy of the remap.config which has entries
for the cachkey.so plugin using long options in its @pparam fields.
You should not see any errors.
- master (b31db7)
None
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->",non_debt,-
couchdb,131,summary,0,Kill off old templates,non_debt,-
attic-apex-malhar,676,comment,350005283,@PramodSSImmaneni I don't see any difference with master?,non_debt,-
spark,17088,comment,284561791,Note alternatively we could change it to not fail on fetch failure. This would seem better to me since there is no reason to throw away all the work you have done but I'm sure that is a much bigger change.,non_debt,-
spark,23284,review,240865741,oh yes I got messed up :P,non_debt,-
spark,6445,comment,106097629,Can one of the admins verify this patch?,non_debt,-
drill,1214,review,183982099,Done.,non_debt,-
arrow,9397,review,571604769,"Fixed Length Byte Array makes sense to me (as the interpretation of the bytes for decimal is different than either i32 or i64). 
Reasons I could imagine using i32 or i64 to write decimals into Parquet would be 
1. ecosystem compatibility (aka that the pandas parquet reader assumed decimals were stored using those types) 
2.possibly so better / more performant encodings could be used.
But I am just SWAG'ing it here",requirement_debt,non-functional_requirements_not_fully_satisfied
spark,22729,summary,0,[SPARK-25737][CORE] Remove JavaSparkContextVarargsWorkaround,non_debt,-
storm,3322,comment,672260764,close/reopen to rebuild.,non_debt,-
carbondata,2097,comment,377139528,"Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/3421/",non_debt,-
carbondata,3986,review,510065227,It was formatted,non_debt,-
incubator-brooklyn,61,review,14808432,"The suggestion to put concrete examples of probability is great. I was considering removing the comment altogether after your comment above about the birthday paradox, but this would be very useful to add.",non_debt,-
airflow,2438,summary,0,[AIRFLOW-1255] Fix SparkSubmitHook logging deadlock,non_debt,-
phoenix,419,comment,452925088,"I didn't mean ""The second aspect of re-running only the necessary tasks"". What I mean is -- assume the first MR job scheduled 100 mappers, 90 of them succeeded and 10 them failed, so the first job successfully updated 90 regions but the whole job failed. The second job (the retry job, assume it succeed) still schedule 100 mappers but only 10 mapper should actually update the stats on the 10 regions which failed in the first job and the other 90 mapper should skip stats update and succeed.",non_debt,-
spark,22006,review,207876249,nit: 2 space indentation,code_debt,low_quality_code
spark,13651,review,66885516,"@hvanhovell  Thanks for the catch. I will try to test hive behavior more, like how `select 5 div 3.0` behaves in Hive.",non_debt,-
couchdb,1011,summary,0,Allow to use $or with a mango JSON index,non_debt,-
arrow,2995,summary,0,"ARROW-2720: [C++] Defer setting of -std=c++11 compiler option to CMAKE_CXX_STANDARD, use CMake option for -fPIC",non_debt,-
airflow,15112,review,605095651,33884891-15112 review-605095651,non_debt,-
kafka,7917,comment,572580060,"@ijuma Thanks for the review, merging to trunk.",non_debt,-
couchdb,1972,review,267628995,"Same thing, where is the `#state{}` record saved on disk?",non_debt,-
beam,539,comment,228978298,Anyone got an idea why Jenkins suddenly fails on JavaDoc issues in classes that aren't related to this PR ?,non_debt,-
beam,219,review,60802609,What does it return?,non_debt,-
couchdb,1370,review,198471640,"Thank you I found it [here](https://github.com/apache/couchdb/blob/COUCHDB-3326-clustered-purge-pr5-implementation/src/couch/src/couch_db.erl#L521)
This means that return values from couch_db are inconsistent ([POLA](https://en.wikipedia.org/wiki/Principle_of_least_astonishment)). Here is a list of functions which deal with different sequences:
- `get_update_seq` returns `integer()`
- `get_purge_seq` returns `{ok, integer()}`
- `get_oldest_purge_seq` returns `{ok, integer()}`
- `get_compacted_seq` returns `integer()`
- `get_committed_update_seq` returns `integer()`
I think that since we are updating purge feature we should take an opportunity to make it consistent, change return type and update all callers.",non_debt,-
airflow,2460,comment,435914220,"Interesting to see this, the author of the CLI implementation assumed that there can't be a next schedule if there has not been any execution yet  
which contradicts my implementation where when this is the case, I derive the first planned run date from the tasks and normalize it to calculate the actual first execution date: 
@ashb @Fokko and all others, I must admit that I'm not sure which point of view is the right one here since the CLI implementation has been accepted in the code base already, does that mean that it's right?",non_debt,-
hbase,754,review,379774633,"On one hand I agree with this and this is something I have implemented in past too but on the other hand, user would not want to change `hbase.regionserver.slowlog.ringbuffer.size` that frequently. The default value for the config is not too high, not too low either. Hence, the reason to have enable/disable config is to ensure we provide reasonable default value to ringbuffer size, something user doesn't have to spend much time with and let user enable this config without touching ringbuffer size.",non_debt,-
hadoop,2674,comment,781282914,"+1 LGTM , @cyrus-jackson  please to take care of the checkstyle too",code_debt,low_quality_code
kafka,5353,comment,407906273,"Hi @cmccabe , when you said enable batching in FindCoordinator, do you mean enable batching in FindCoordinatorRequest class? Or you still mean to modify and improve on what we currently have? Thanks!",non_debt,-
madlib,10,review,52082172,Just pushed the changes. Please take a look and comment.,non_debt,-
nifi-minifi-cpp,488,comment,465570793,reviewing,non_debt,-
hbase,2222,comment,674614477,"Thanks @virajjasani .
Let me take a look at the failed UT.
And then roll a RC2 with @joshelser 's patch in place.",non_debt,-
spark,23873,summary,0,[SPARK-26975][SQL] Support nested-column pruning over limit/sample/repartition,non_debt,-
spark,15609,comment,255675557,"actually, its not covered there... So, if any of my expression is calling if expression ( nested, with different data types..) .. its failing because of this issue.
 Because if expression datatype only considers true expression.datatype, and not considering false expression's datatype.
As date type is represented by int and timestamp type is long, thats why there are two scenarios:
2. While in the reverse case, long cannot be assigned to int without explicit type casting.. Hence this is teh issue.",non_debt,-
incubator-brooklyn,615,review,30307969,Probably worth a comment why guard `super.setup()`,non_debt,-
incubator-mxnet,10819,summary,0,[MXNET-367] update mkldnn to v0.14 and disable building test examples,test_debt,lack_of_tests
bookkeeper,368,review,130937339,Capitalize ZooKeeper (here and elsewhere),non_debt,-
tinkerpop,1297,summary,0,Fix graphbinary format description of Long,non_debt,-
arrow,1804,comment,377119109,"I'm sorta ambivalent on the package name -- I looked at crates.io and there are some other ASF projects with packages that just use the Foo in Apache Foo. If ""arrow"" is shorter and sweeter, that's no problem",code_debt,low_quality_code
kafka,767,comment,171426445,"@Ishiihara Sounds good. I'm just getting started with this code, but the current threading model seems reasonable to me. I see the Worker as just the task manager, which is primarily responsible for managing the task lifecycle. Currently this just means starting and stopping tasks, but it'll probably have to be extended to handle task failures. I assume it'll also need hooks for status tracking. Also, I like the tasks being single-threaded since it makes it easy to reason about their state.",non_debt,-
spark,26138,description,0,"What changes were proposed in this pull request?
Some of the columns of JDBC/ODBC tab  Session info in Web UI are hard to understand.
Add tool tip for Start time, finish time , Duration and Total Execution
Why are the changes needed?
To improve the understanding of the WebUI
Does this PR introduce any user-facing change?
No
How was this patch tested?
manual test",design_debt,non-optimal_design
druid,4754,review,152706384,"I think it should be computed on higher level, producing two different classes. The lambda below is capturing, i. e. it's an allocation on each iteration",code_debt,low_quality_code
camel-quarkus,626,comment,576870750,@oscerd do you mind rebase so we can test against quarkus 1.2.0 RC ?,non_debt,-
incubator-heron,1441,description,0,fixes #1411,non_debt,-
tvm,7642,review,600034455,"I think in some instances we need to recreate the schedule because the operations are reordered. I can update to not create a new schedule every time, and add a comment about incremental improvements.",non_debt,-
druid,1899,summary,0,Docs improved: more details about caching and memory for segments on historicals,documentation_debt,low_quality_documentation
kylin,702,comment,503875149,Can one of the admins verify this patch?,non_debt,-
spark,16605,review,96789868,"okay, fixed!",non_debt,-
storm,2428,comment,351426274,+1. Can you please squash. Thanks.,non_debt,-
arrow,1646,review,172281516,you are right. added back the test with assertion.,non_debt,-
flink,14610,summary,0,[FLINK-20909][table-planner-blink] Fix deduplicate mini-batch interval infer,non_debt,-
ignite,218,comment,487269281,"https://issues.apache.org/jira/browse/IGNITE-1681 is closed, closing the pr",non_debt,-
superset,514,review,74370933,we could delete this.,non_debt,-
flink,6213,summary,0,[FLINK-9672] Fail fatally if job submission fails on added JobGraph signal,non_debt,-
flink,11403,review,392897158,nit: indent,code_debt,low_quality_code
apisix,410,summary,0,grpc proxy support,non_debt,-
openwhisk,4403,description,0,"Co-authored-by: Sugandha Agrawal <sugandha.agrawal18@gmail.com>
This PR is related to https://github.com/apache/incubator-openwhisk/pull/4388. Seeing only a http response 400 Bad Request isn't enough to understand the root cause, but the detailed error and reason information which may exist in the error response will do.
This code change adds the error and reason info of the error response to the the HTTP status code.",non_debt,-
spark,1415,comment,49090832,@rxin IIRC at one point we changed this before and it caused a performance regression for our perf suite so we reverted it. At the time I think we were running on smaller data sets though. Maybe in this case were are willing to take a hit?,non_debt,-
parquet-mr,487,description,0,20675636-487 description-0,non_debt,-
kafka,3575,review,129764370,Nit: should we include bytes like we did `ms` for the other case?,code_debt,low_quality_code
drill,1450,comment,434638067,"@Ben-Zvi looks like Oleg changed the code. Is it ok now? Could you please take a look.
@oleg-zinovev please do not forget to tag reviewers when you have made the changes, otherwise nobody will know that you have addressed code review comments.",non_debt,-
spark,14553,comment,243855740,"@ScrapCodes, would you mind triggering a build of this PR?",non_debt,-
beam,10779,comment,583548103,Run Java PreCommit,non_debt,-
hudi,1756,description,0,"add more tests for MarkerFiles,RollbackUtils, RollbackActionExecutor for markers and filelisting.
also fix the bugs for RollbackActionExecutor with markers mode or filelisting mode:
in HoodieWriteClient.java should not deleteMarkerDir, the rollback with markerfiles mode will failed
in ListingBasedRollbackHelper.java ""(path.toString().endsWith(HoodieFileFormat.HOODIE_LOG.getFileExtension()))"" can not check file is logfile
in ""MarkerBasedRollbackStrategy.java"" baseCommitTime should use FSUtils.getCommitTime(baseFilePathForAppend.getName());
This pull request is a trivial rework / code cleanup without any test coverage.
This change added tests and can be verified as follows:",non_debt,-
spark,17662,comment,294663951,"I like it, I'm going to leave this a for a bit and see if anyone has any comments overnight :)",non_debt,-
spark,15870,summary,0,[SPARK-18425][Structured Streaming][Tests] Test `CompactibleFileStreamLog` directly,non_debt,-
spark,4173,comment,71614063,"Jenkins, test this please.",non_debt,-
superset,10267,comment,655918789,"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/10267?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/10267?src=pr&el=continue).",non_debt,-
carbondata,1889,comment,361674829,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3286/",non_debt,-
geode-native,481,description,0,Co-authored-by: Dave Barnes <dbarnes@pivotal.io>,non_debt,-
ozone,140,description,0,"https://issues.apache.org/jira/browse/HDDS-2411
and you need to set the title of the pull request which starts with
the corresponding JIRA issue number. (e.g. HDDS-XXXX. Fix a typo in YYY.)
-->",non_debt,-
carbondata,794,comment,294209058,"Build Success with Spark 1.6.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/1629/",non_debt,-
ignite,7607,review,409054064,Fixed.,non_debt,-
lucene-solr,1351,review,406046648,Can we merge this with the  `if` statement immediately above it? That way `scoreMode` can stay final,non_debt,-
spark,13959,comment,302569371,"Yes, I tried to identify this case.
For this PR (or such PRs), the author looks still responsive and active so I do not disagree with re-opening personally because this was the point in https://github.com/apache/spark/pull/18017. Probably, I should have left a comment about this in each PR for clarification though.",non_debt,-
spark,13262,comment,221354484,@yanboliang Thanks for the updates.  I responded above about a tiny fix.,non_debt,-
spark,22863,comment,433660183,Thanks @felixcheung,non_debt,-
spark,31595,review,581561966,"yea, that is what I said a no-op aggregate. I think it is okay to add it. Maybe I can add it in follow up.",non_debt,-
openwhisk,3388,comment,371279253,Can't get a PG to work.,non_debt,-
cassandra,730,review,485793488,"this is `Class.toString` which is harder to read for arrays, so did this so I could read the output without googling =D.
primitives and objects will normally have valid string names, but arrays will be like `J[` which require knowing what that means or googling it.",code_debt,low_quality_code
jmeter,627,review,495568930,"You don't need to provide those messages_XX.properties if you don't translate them.
Only messages.properties and messages_fr.properties are mandatory.
Provide the additional ones that you translate",non_debt,-
nifi,2805,comment,399167717,"Looks good, merging, thanks!",non_debt,-
incubator-heron,3541,description,0,"Fixes #3540 by embedding the Trykker fonts as directed on this [Google webfonts helper app.](https://google-webfonts-helper.herokuapp.com/fonts/trykker?subsets=latin,latin-ext)",non_debt,-
pulsar,3377,description,0,"Currently, we print auth data in info logs - roleToken, private key info etc.
Print ClientConfiguration without authentication field.
We will no longer print sensitive info.",non_debt,-
spark,23208,review,239581374,"Maybe it should also be part of the `TableProvider` contract that if the table can't be located, it throws an exception?",non_debt,-
incubator-heron,3479,comment,626232335,"One ""short term workaround""‚Ñ¢ for getting the `pex_pytest` to work was to change it so the pex binaries+tests defaulted to be non-zip safe so they are extracted. There were still failures later, which I think is from pex_library consumption. I'll see if I can patch that too, then hopefully come up with a neater solution for all of the issues",design_debt,non-optimal_design
cloudstack,954,comment,150064890,@karuturi @NuxRo @ustcweizhou could you please check [#962](https://github.com/apache/cloudstack/pull/962) for the details as #954 could then be closed and #962 could be merged if all LGTY ?,non_debt,-
superset,9837,description,0,"* Fixes issue with `pip` install failing when dependencies are duplicated across requirements files.
* Removes `restart: always` from tests worker as it crash-loops during ""normal"" operation. 
* Adds `Dockerfile-dev` which depends on `preset/superset:dev` which acts as a cache for local development",non_debt,-
flink,9717,review,327954939,I guess we do not need the tail </p> before in code formatting rule. Anything changed now?,non_debt,-
airflow,5659,comment,581119882,Needs rebase,non_debt,-
airflow,6073,comment,530132536,"# [Codecov](https://codecov.io/gh/apache/airflow/pull/6073?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/6073?src=pr&el=continue).",non_debt,-
flink,8967,comment,508887383,CI report for commit 18b9bf24d58870dd64c6f5e511785967ef2b9452: FAILURE [Travis](https://travis-ci.org/flink-ci/flink-ci/builds/554871984?utm_source=github_status&utm_medium=notification),non_debt,-
spark,28991,review,449456447,What does `//do not want to change TOperationState in hive 1.2` means?,non_debt,-
zookeeper,1292,comment,602099491,@maoling¬†- Latest JLine jar available is `3.0.0.M1` which has major implementation changes. Have upgraded to second latest jar `2.14.6` to address this issue with minimal code changes. Kindly review.,non_debt,-
flink,10105,review,344547802,ditto,non_debt,-
incubator-mxnet,9552,review,176825148,"Just tried. That function for FP32 convolution seems not implemented appropriately. The data member `layout` of struct `ConvolutionParam` is a `option<int>` type, but it was used with the assumption that `layout` contains a real value in that function. This will cause my test case to fail. Making that function properly implemented needs another PR. For now, I will just keep the shape inference for quantized_conv op. Already deleted the TODO comment.",requirement_debt,requirement_partially_implemented
hbase,1860,comment,646896275,Picking back...,non_debt,-
kafka,837,comment,177891645,"One other advantage of System.nanoTime is that it is monotonically increasing. With regards to the performance cost, here's a thorough and reliable analysis on the cost of `System.nanoTime`:
http://shipilev.net/blog/2014/nanotrusting-nanotime/
I was unable to find an analysis of similar quality for `System.currentTimeMilllis`. We should definitely quantify the performance difference (particularly on Linux) before we choose one way or another.
Also, it would be good to do KAFKA-2247 before we do this.",non_debt,-
arrow,36,comment,339548456,"Pass compile and tests, please review, Thanks!",non_debt,-
netbeans,1392,comment,525818262,Ping. PR has been ready for more than a month now.,non_debt,-
flink,11124,summary,0,[Hotfix]Add suffix L to WatermarkGeneratorCodeGenTest#testAscendingWatermark long type fields,non_debt,-
pulsar,2617,comment,423583902,run java8 tests,non_debt,-
camel,3171,review,324231977,Maybe this at debug. Too noisy.,code_debt,low_quality_code
flink,14052,review,526233564,"I think you're right, I'll change it.",non_debt,-
airflow,4083,comment,495929466,"This looks good to me.
But it needs rebase",non_debt,-
airflow,4646,comment,464366589,"@seelmann , no worry, thanks.",non_debt,-
accumulo,195,review,94623388,Thanks!,non_debt,-
superset,6058,comment,430363331,LGTM,non_debt,-
flink,15045,description,0,"fix some typo errors to make the context consistent:
some are ""streamOfWords"" but some are ""dataStreamOfWords""",documentation_debt,low_quality_documentation
flink,8430,review,284183023,`regionsToVisit.addAll(regionConsumers.get(regionToRestart))`,non_debt,-
dubbo,5799,description,0,"‚Ä¶umer]
XXXXX
XXXXX
XXXXX",non_debt,-
drill,530,review,73929975,"I added a session/system option in latest commit. In my opinion if a `LIMIT 0` query is taking longer than 10mins, we should try to fix it, otherwise it is not an interactive experience when using Drill with BI tools.",non_debt,-
incubator-pagespeed-ngx,1071,summary,0,log: initialize logging earlier,non_debt,-
skywalking,2470,summary,0,Fixed Failure of Enhanced Kafka Interceptor to Acquire SkyWalking DynamicField,non_debt,-
systemds,688,review,147017845,There are 4 different options. Which one do you recommend ?,non_debt,-
phoenix,913,review,541363956,Fixed -- lots were remnants from when we annotated indexes,non_debt,-
airflow,1830,review,90948950,"I find the else to be more readable, but happy to drop it and the indent...",code_debt,low_quality_code
hadoop,1601,comment,541059565,"1. updated the docs. The only place we don't do Head and dir marker is in create()
1. also added a test to verify that the empty set of probes skips all http requests
Now. can you create a Path with a trailing / ? I was about to say no, but remembered https://issues.apache.org/jira/browse/HADOOP-15430 .. one of the constructors of Path does let you get away with it, which is something which breaks S3Guard already",non_debt,-
kafka,2191,comment,268001461,"Thanks for the review and merge @xiguantiaozhan. It LGTM now. @becketqin, can you please take a look as well?",non_debt,-
beam,2246,comment,287187424,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8495/<h2>Failed Tests: <span class='status-failure'>1</span></h2><h3><a name='beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark' /><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8495/org.apache.beam$beam-runners-spark/testReport'>beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark</a>: <span class='status-failure'>1</span></h3><ul><li><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8495/org.apache.beam$beam-runners-spark/testReport/org.apache.beam.runners.spark.translation.streaming/ResumeFromCheckpointStreamingTest/testWithResume/'><strong>org.apache.beam.runners.spark.translation.streaming.ResumeFromCheckpointStreamingTest.testWithResume</strong></a></li></ul>
--none--",non_debt,-
camel,2070,summary,0,CAMEL-11959: Add new camel-yql component,non_debt,-
spark,6090,summary,0,[SPARK-7567] [SQL] Migrating Parquet data source to FSBasedRelation,non_debt,-
incubator-mxnet,13109,comment,438792906,"@mxnet-label-bot update [pr-awaiting-review, cmake]",non_debt,-
spark,7580,review,35294849,"Sorry, I mean we don't need the implicit casting, as it's not the same behavior in Hive.
Or are you trying to implicit casting?",non_debt,-
arrow,4706,comment,505897550,"checkout an old commit, `make -f Makefile.docker run-python`, come back to master, see the test fails because the old libraries are installed in /opt/conda/lib...",non_debt,-
helix,731,review,376166792,"That is untrue, because `shardingKey` could be either empty or ""/"". Adding delimiter first makes the logic easy to understand - after adding the delimiter, all logic applies to sharding keys with leading delimiters.",non_debt,-
hudi,2106,review,503737912,"please keep this javadoc to be just about the preCombine() method, without any context from this PR;s scenarios.",documentation_debt,low_quality_documentation
couchdb,1972,review,265677665,It's two levels to match match existing indentation for callbacks specs. (Two levels for the response to break it up from one level indentation for when the function head is too long).,non_debt,-
spark,14087,review,81610590,"This change seems unrelated and takes us out of sync with the batch version.  I don't think this means a JVM interface, but rather the `interface` in API.",non_debt,-
druid,5632,review,180936341,"This deserves a comment so someone doesn't ""simplify"" it back into the old code.",documentation_debt,low_quality_documentation
incubator-pagespeed-ngx,1028,review,43148618,comment is out of date now,documentation_debt,outdated_documentation
incubator-pinot,425,description,0,"Re-write of StarTree and StarTreeIndexNode.
Implemented new versions for star tree (StarTreeV2) and star tree node
size (1.1GB to 469MB). The feature is currently OFF by default, and
controlled via StarTreeBuilderConfig.
1. StarTreeV2 is a compact memory representation of StarTree with native
   serialization/de-serialization support. And can be loaded either in
   direct memory, or via memory mapped file. Used the Xerial LBuffer
   library to be able to support data sizes larger than 2GB.
2. Added custom serializer/de-serializer for StarTreeV2, as opposed to
   V1 that uses JAVA serialization/de-serialization.
3. Modified StarTreeIndexOperator to be able to work off of
   StarTreeInterf, which can be implemented either by StarTree or
   StarTreeV2.
4. Added a utility to convert a pinot segment with star tree v1 into a
   pinot segment with star tree v2.
5. Added unit tests to test:
   - Reading/Writing of StarTreeV2.
   - Query processing using StarTreeV2.",non_debt,-
pulsar,4062,review,277191942,isn't `initialize` already called when the factory is loaded? this is a double-initialize action.,non_debt,-
spark,14562,summary,0,[SPARK-16973][SQL] remove the buffer offsets in ImperativeAggregate,non_debt,-
thrift,916,summary,0,THRIFT-3637: Dart compact protocol,non_debt,-
spark,26533,comment,557886732,retest this please,non_debt,-
camel,1043,comment,227651822,Yeah sure go ahead and merge.,non_debt,-
drill,578,review,102303605,should the server connection exposes SASL protocol directly? isn't ServerAuthenticationHandler a better place to manage the saslServer instance?,non_debt,-
cloudstack,3003,comment,435941591,"makes sense, if the contents is covoured. I didn't give that any attention, @rafaelweingartner.",non_debt,-
spark,31829,summary,0,[WIP][SPARK-34736][K8S][TESTS] Kubernetes and Minikube version upgrade for integration tests,non_debt,-
beam,6830,summary,0,Allow streaming update for Python on Dataflow.,non_debt,-
incubator-pinot,2068,review,149484084,"todo before release: 
Document this cp as what's it's doing is a little bit obscure",code_debt,low_quality_code
ozone,1870,comment,781711803,Thanks @amaliujia for the review.,non_debt,-
bookkeeper,2347,description,0,"Load bookkeeper parameters from environment variables prefixed with BK_
Add the way to configure bookkeeper through environment variables to simplify deployment
Load bookkeeper parameters from environment variables with the prefix BK_ before load bk_server.conf
Master Issue: #2341",non_debt,-
phoenix,749,review,402659415,"Same here, is the logging diff required?",non_debt,-
spark,24116,comment,473683327,Merged to master again,non_debt,-
phoenix,855,description,0,Jenkinsfile for multibranch job,non_debt,-
kafka,2264,review,95306094,ok,non_debt,-
superset,9915,comment,634387543,"@john-bodley, I tried my best to add a test here, but wasn't able to get it to work due to an issue (maybe with FAB?). Here's my test code (also added in a commit to this PR):
And the error I saw:
It looks like when I'm trying to create an annotation inside the annotation layer, it's rendering a template instead of creating the new annotation. Maybe @dpgaspar has thoughts here?",non_debt,-
ignite,8940,review,603982014,31006158-8940 review-603982014,non_debt,-
avro,1070,review,578050603,"Excellent point.  I updated the test to use your round trip logic.  As far as empty strings, my original code would have eliminated them from the output, but since the test was written expecting them, I changed it to preserve the doc attribute when specified; empty or not.",non_debt,-
shardingsphere,8604,review,546475778,@lwtdev merge into one line.,non_debt,-
kafka,2824,comment,293434490,"LGTM. Thank you.
Merging into 0.10.2 and trunk.",non_debt,-
carbondata,1664,comment,450070857,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2006/",non_debt,-
spark,28098,summary,0,[SPARK-30973][SQL]ScriptTransformationExec should wait for the termination,non_debt,-
couchdb,3127,review,485072389,I was noodling over a way to dynamically enable the tracing logs I've been using while developing this. As metrics I think the cardinality would either be high enough it'd crush most metric collectors or coarse enough that it wouldn't be super helpful.,non_debt,-
skywalking,845,comment,376875153,@carlvine500 CI fails...,non_debt,-
spark,19041,review,163041732,"It's very rare for us to mark a class as final, especially a private one. Is there a reason for that?
It makes other things (like mocking the class in tests) more complicated, for example.",code_debt,low_quality_code
kafka,4206,comment,344148911,"FAILURE
 6695 tests run, 1 skipped, 2 failed.
--none--",non_debt,-
spark,18301,review,122376556,"Ah, I miss this one. Thanks @dongjoon-hyun",non_debt,-
spark,21200,review,185268813,17165658-21200 review-185268813,non_debt,-
carbondata,1524,comment,345426984,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1266/",non_debt,-
spark,7186,review,36114225,"I see it in UTF8StringSuite, make sense.",non_debt,-
hbase,426,review,310130785,"We need to be compatible with 2.x client I think. And also, we need to change a lot of tests if we want to change the behavior here. So I suggest that we do it in 4.0.0.",non_debt,-
kafka,6538,review,274196345,Should be ApiKeys.INIT_PRODUCER_ID,non_debt,-
samza,430,comment,367865351,@xinyuiscool Please take a look.,non_debt,-
madlib,389,comment,491443083,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/madlib-pr-build/897/",non_debt,-
arrow,467,review,109291184,"There's https://github.com/apache/arrow/blob/master/cpp/cmake_modules/FindPythonLibsNew.cmake, is one or the other made redundant by the other?",code_debt,complex_code
trafodion,1475,comment,372915337,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2482/,non_debt,-
flink,9057,review,302544895,Created.,non_debt,-
spark,19884,comment,350871930,"@zsxwing that's right, we will have to coordinate to make sure the Jenkins pyarrow is upgraded to version 0.8 as well.  I'm not sure the best way to coordinate all of this because this PR, jenkins upgrade, and Spark Netty upgrade all need to happen at the same time.
@holdenk @shaneknapp will one of you be able to work on the pyarrow upgrade for Jenkins sometime around next week?  (assuming Arrow 0.8 is released in the next day or so)",non_debt,-
ambari,39,comment,304184352,"hello, Install Ambari 2.5 install HDP 2.6 , spark_client install error ""parent directory /usr/hdp/current/spark-client/conf doesn't exist"", copy other node `/etc/spark` to current node solve the problem„ÄÇ
Detail Errors",non_debt,-
incubator-pinot,2062,review,148907373,is this a placeholder?,non_debt,-
arrow,898,review,130206222,I would prefer doing this change differently. Maybe by allowing allocator to return an empty buffer even if closed. This is because it makes bugs/issues much easier to understand than getting an NPE.,code_debt,low_quality_code
kafka,557,comment,185440297,+1,non_debt,-
superset,12873,review,568157907,Let's not rename this here,non_debt,-
drill,1953,review,373533767,"Thanks, fixed.",non_debt,-
skywalking,4987,comment,652475825,"local compile and package speed is so slow,because  need to download so much dependency jar file from maven depository",code_debt,slow_algorithm
storm,1480,comment,226404728,"@abhishekagarwal87: Ok, didn't spot that. Was only looking for ""guava"" relocations.
So, the dependency could be additionally included relocated into storm-redis. Or is it possible to use the already relocated package which is provided via storm-core? This would save some resources. I'm not very experienced with the `maven-shade-plugin` yet, unfortunately.
What is preferred?",design_debt,non-optimal_design
pulsar,43,review,82489542,I think it was part of out rollout plan.. we wanted to enable this feature in two phases: rollout broker first and later on enable at client-side.,non_debt,-
couchdb,1605,review,222812034,"Missing `.` after ""partitioned"".",non_debt,-
flink,13354,summary,0,[FLINK-19135] Strip ExecutionException in (Stream)ExecutionEnvironment.execute(),non_debt,-
incubator-heron,2761,review,172425361,use import to avoid full path,non_debt,-
tvm,1181,review,194825932,also add autofunction to below(see the filed in topi.nn),non_debt,-
tvm,4934,review,383560367,Updated,non_debt,-
druid,2144,comment,166701946,@drcrallen file issues for new test failures you encounter,non_debt,-
reef,1054,review,68419655,This looks 100% the same as in the local runtime. Can we reuse that code instead of duplicating it?,code_debt,duplicated_code
spark,18748,review,139141803,"How does that eliminate the need for `distinct`?
e.g. take a look at the below:",non_debt,-
spark,28163,description,0,"In the PR, I propose to optimise the `DateTimeUtils`.`rebaseJulianToGregorianMicros()` and `rebaseGregorianToJulianMicros()` functions, and make them faster by using pre-calculated rebasing tables. This approach allows to avoid expensive conversions via local timestamps. For example, the `America/Los_Angeles` time zone has just a few time points when difference between Proleptic Gregorian calendar and the hybrid calendar (Julian + Gregorian since 1582-10-15) is changed in the time interval 0001-01-01 .. 2100-01-01:
The difference in microseconds between Proleptic and hybrid calendars for any local timestamp in time intervals `[local timestamp(i), local timestamp(i+1))`, and for any microseconds in the time interval `[Gregorian micros(i), Gregorian micros(i+1))` is the same. In this way, we can rebase an input micros by following the steps:
1. Look at the table, and find the time interval where the micros falls to
2. Take the difference between 2 calendars for this time interval
3. Add the difference to the input micros. The result is rebased microseconds that has the same local timestamp representation.
Here are details of the implementation:
- Pre-calculated tables are stored to JSON files `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json` in the resource folder of `sql/catalyst`. The diffs and switch time points are stored as seconds, for example:
  The JSON files are generated by 2 tests in `RebaseDateTimeSuite` - `generate 'gregorian-julian-rebase-micros.json'` and `generate 'julian-gregorian-rebase-micros.json'`. Both tests are disabled by default. 
  The `switches` time points are ordered from old to recent timestamps. This condition is checked by the test `validate rebase records in JSON files` in `RebaseDateTimeSuite`. Also sizes of the `switches` and `diffs` arrays are the same (this is checked by the same test).
The hash maps store the switch time points and diffs in microseconds precision to avoid conversions from microseconds to seconds in the runtime.
- I moved the code related to days and microseconds rebasing to the separate object `RebaseDateTime` to do not pollute `DateTimeUtils`. Tests related to date-time rebasing are moved to `RebaseDateTimeSuite` for the same reason.
- I placed rebasing via local timestamp to separate methods that require zone id as the first parameter assuming that the caller has zone id already. This allows to void unnecessary retrieving the default time zone. The methods are marked as `private[sql]` because they are used in `RebaseDateTimeSuite` as reference implementation.
- Modified the `rebaseGregorianToJulianMicros()` and `rebaseJulianToGregorianMicros()` methods in `RebaseDateTime` to look up the rebase tables first of all. If hash maps don't contain rebasing info for the given time zone id, the methods falls back to the implementation via local timestamps. This allows to support time zones specified as zone offsets like '-08:00'.
To make timestamps rebasing faster:
- Saving timestamps to parquet files is ~ **x3.8 faster**
- Loading timestamps from parquet files is ~**x2.8 faster**.
- Loading timestamps by Vectorized reader ~**x4.6 faster**.
No
- Added the test `validate rebase records in JSON files` to `RebaseDateTimeSuite`. The test validates 2 json files from the resource folder - `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json`, and it checks per each time zone records that
  - the number of switch points is equal to the number of diffs between calendars. If the numbers are different, this will violate the assumption made in `RebaseDateTime.rebaseMicros`.
  - swith points are ordered from old to recent timestamps. This pre-condition is required for linear search in the `rebaseMicros` function.
- Added the test `optimization of micros rebasing - Gregorian to Julian` to `RebaseDateTimeSuite` which iterates over timestamps from 0001-01-01 to 2100-01-01 with the steps 1 ¬± 0.5 months, and checks that optimised function `RebaseDateTime`.`rebaseGregorianToJulianMicros()` returns the same result as non-optimised one. The check is performed for the UTC, PST, CET, Africa/Dakar, America/Los_Angeles, Antarctica/Vostok, Asia/Hong_Kong, Europe/Amsterdam time zones.
- Added the test `optimization of micros rebasing - Julian to Gregorian` to `RebaseDateTimeSuite` which does similar checks as the test above but for rebasing from the hybrid calendar (Julian + Gregorian) to Proleptic Gregorian calendar.
- Re-run `DateTimeRebaseBenchmark` at the America/Los_Angeles time zone (it is set explicitly in the PR #28127):",code_debt,slow_algorithm
kafka,9720,review,540486098,"We want to make `close()` idempotent and not throw an exception but we will log a warning, but only for close so that is why these logs are not in the `setState()` method.",non_debt,-
beam,1808,comment,275576811,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6845/<h2>Build result: FAILURE</span></h2>[...truncated 6598 lines...]	at hudson.remoting.UserRequest.perform(UserRequest.java:153)	at hudson.remoting.UserRequest.perform(UserRequest.java:50)	at hudson.remoting.Request$2.run(Request.java:332)	at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:68)	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.maven.plugin.MojoFailureException: You have 1 Checkstyle violation.	at org.apache.maven.plugin.checkstyle.CheckstyleViolationCheckMojo.execute(CheckstyleViolationCheckMojo.java:588)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)	... 31 more2017-01-27T02:45:40.933 [ERROR] 2017-01-27T02:45:40.933 [ERROR] Re-run Maven using the -X switch to enable full debug logging.2017-01-27T02:45:40.933 [ERROR] 2017-01-27T02:45:40.933 [ERROR] For more information about the errors and possible solutions, please read the following articles:2017-01-27T02:45:40.933 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException2017-01-27T02:45:40.933 [ERROR] 2017-01-27T02:45:40.934 [ERROR] After correcting the problems, you can resume the build with the command2017-01-27T02:45:40.934 [ERROR]   mvn <goals> -rf :beam-sdks-java-io-google-cloud-platformchannel stoppedSetting status of 9f062e3350c7fa7df393d99bbf38ad3b3d83e8b8 to FAILURE with url https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6845/ and message: 'Build finished. 'Using context: Jenkins: Maven clean install
--none--",non_debt,-
daffodil,291,review,348540375,"These ``_fileOS == null`` checks are kindof ugly. Thoughts on making this so this class just extends OutputStream instead of ByteArrayOutputStream, and then make it a wrapper for an output stream, which might change from ByteArrayOutputStream to FileOutputStream? I'm thinking something like:
Makes it so all the overrirde functions are basically just stream.whatever(), except for write which just calls the switch thing. Another benefit is once the switch happens, the old ByteArrayOutputStream can be garbage collected, whereas before it couldn't.",code_debt,low_quality_code
cloudstack,732,comment,134135278,"[cloudstack-pull-analysis #309](https://builds.apache.org/job/cloudstack-pull-analysis/309/) SUCCESS
This pull request looks good",non_debt,-
spark,14690,review,82707709,"Should this be a method of location? Btw, including the number of partitions would be nice too, since it gets truncated.",non_debt,-
spark,2432,review,18315435,"Yes that's true, but this will never be null right? Here we have a default app ID if the scheduler doesn't provide us with one.
Yes we'll have to update the relevant backward compatibility test in `JsonProtocolSuite` to make it take in a string rather than an option.",non_debt,-
spark,26656,review,359262166,The attributes reference by filter is also needed.,non_debt,-
spark,21442,comment,405148846,"@HyukjinKwon All the involved reviewers will get a ping. This is annoying to see many pings within one hour, right? My suggestion is to read the comments before triggering the test",non_debt,-
incubator-mxnet,14783,comment,492901200,"Merging now, thanks for your contribution.",non_debt,-
flink,4213,comment,312696319,@aljoscha you merge since you asked first.,non_debt,-
spark,17530,comment,291961397,"That is not what you change does, though.",non_debt,-
kafka,6020,comment,447558662,"Build failed with checkstyle errors.
@vvcephei I also expected that the test is fixed with this PR instead of disabled. Not sure if we can get the information about user privileges.
What I am wondering thought is: why does a root user not respect read-only flag?",non_debt,-
openwhisk,1438,comment,256636239,PG2 492 blue before this morning's rebase.  Let's see what Travis has to say.,non_debt,-
druid,2225,description,0,"This pull request adds support for [RocketMQ](http://github.com/alibaba/rocketmq), a Kafka-like messaging system as data ingesting source.",non_debt,-
spark,27267,review,369526132,Done,non_debt,-
spark,14788,comment,256916341,I will be back after testing/looking into other databases tomorrow.,non_debt,-
nifi-minifi-cpp,791,review,435319933,"I have moved `mtime_` out of `TailState` as it is not really part of the state, and I'm using an ad-hoc `{TailState, mtime}` struct in the one place where it is needed (`findRotatedFiles`).
I have also renamed `timestamp_` to `last_read_time_`.",non_debt,-
pulsar,5043,comment,546989952,rerun java8 tests,non_debt,-
spark,25919,review,338633807,"How can we distinguish 0 partitions after pruning, and not being partition pruned?",non_debt,-
zeppelin,1843,comment,270552773,@1ambda Can you check the personalized mode as well?,non_debt,-
jena,51,comment,91499263,"Hi Alexis, 
I'm glad we have got that cleared up.
@osma has touched an important point - the integration with full ACID transactions.  That's probably a major part of the fact the merge doesn't work.",non_debt,-
thrift,1050,summary,0,THRIFT-3879 Undefined evaluation order,non_debt,-
flink,11944,review,417785164,Good catch!,non_debt,-
cloudstack,1224,comment,164271374,"@bhaisaab I don't like maven but we are using it! cherry-picking is really not an argument and backporting is difficult for worse reasons then this one.
Using maven we better adhere to the conventions in the maven world as keeping our diversions from it correct will become increasingly difficult over time. I will meet you half way so we can abandon 4.5 first and continue to prove our fwd-merge schedule over several versions. We will face issues in this respect as well, btw, if at the time of 4.11 we will be fixing things in 4.6 ;)",design_debt,non-optimal_design
beam,1100,description,0,"quickly and easily:
  `[BEAM-<Jira issue #>] Description of pull request`
     Travis-CI on your fork and ensure the whole test matrix passes).
     number, if there is one.
     [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---",non_debt,-
spark,13300,comment,222043096,@rxin Please help double check! Many thanks!!,non_debt,-
druid,3399,comment,252780216,üëç   BUT would recommend to merge https://github.com/druid-io/druid/pull/3499 first.,non_debt,-
flink,839,comment,112161893,Looks good +1,non_debt,-
thrift,1401,comment,349099421,@RobberPhex  Any word on getting this merged?  Using the extension in PHP 7.0 is kinda broken otherwise... this is fixing a common issue that affected many PHP extensions moving to PHP 7.0.,non_debt,-
spark,15046,description,0,"For data sources without extending `SchemaRelationProvider`, we expect users to not specify schemas when they creating tables. If the schema is input from users, an exception is issued. 
Since Spark 2.1, for any data source, to avoid infer the schema every time, we store the schema in the metastore catalog. Thus, when reading a cataloged data source table, the schema could be read from metastore catalog. In this case, we also got an exception. For example, 
This PR is to fix the above issue. When building a data source, we introduce a flag `isSchemaFromUsers` to indicate whether the schema is really input from users. If true, we issue an exception. Otherwise, we will call the `createRelation` of `RelationProvider` to generate the `BaseRelation`, in which it contains the actual schema.
Added a few cases.",non_debt,-
spark,20670,comment,369484110,LGTM,non_debt,-
openwhisk,1874,comment,280988481,52039373-1874 comment-280988481,non_debt,-
incubator-heron,2557,review,151204609,suggest to move internal util functions to the beginning.,non_debt,-
kafka,6762,review,286572482,"The comment above doesn't seem to fit the case, perhaps the result of a refactor somewhere along the way. Shall we move it below to the actual `Dead` case?",documentation_debt,outdated_documentation
beam,4249,summary,0,SerializableCoder#structuredValue returns the object itself,non_debt,-
spark,29342,review,467157401,@agrawaldevesh - updated.,non_debt,-
arrow,1219,review,146153016,"Yes, in fact this is a ""fix"" so that a conda-installed version of pyarrow can be used in another project's setup.py (since `pyarrow.get_include()` will need to work both in pip and conda)",non_debt,-
storm,2241,review,129494647,"Looks like you're removing max spout pending and also configuration for backpressure. Does this patch also touch the mechanism of backpressure?
And let's address this in patch or file an issue and remove TODO.",requirement_debt,requirement_partially_implemented
rocketmq,64,comment,280278200,"Can you provide some test data, say before/after applying this patch, how many duplications are found respectively?
IMHO, we should make the API as concise as possible.",code_debt,duplicated_code
kafka,6340,comment,479994321,"Thanks @mumrah for taking a close look to these changes here as well. I replied to your two points, and if I didn't miss something, it seems that no code change is required.",non_debt,-
samza,196,description,0,"Changes
Brings up a test bed that contains embedded kafka broker and zookeeper to test the following scenarios.
A) Rolling upgrade of stream processors.
B) Reelection of leader upon failures.
C) Registering multiple processors with same processor id.
D) Zookeeper failure before job model regeneration upon leader death should kill all running stream applications.
NOTE:
Some tests are commented out since zookeeper exceptions are swallowed in ZKJobCoordinator/ZKUtils.",non_debt,-
arrow,2623,review,220653441,This automatic fix isn't my favorite though I see that it is necessary to not have a braking change in the api for  ParquetDataset (with the filters argument). Perhaps though it would be better to throw an error here and have this fix in that specific case instead of allowing a wrong nesting level in all cases.,code_debt,low_quality_code
geode,303,review,91422055,I thought it would be good to have the same Test. Any new create scenario will also have associated destroy command.,non_debt,-
tajo,852,description,0,Reports try finally statements which can use Automatic Resource Management of Java 7 or higher.,non_debt,-
incubator-doris,2431,review,357498715,"if user create view like:
`select k1,k2,k3,k4,k5 from tbl;`
without order by clause and aggregation method, we should select first few columns as sort columns, not all columns.
Check this commit: `https://github.com/apache/incubator-doris/commit/bf31bd238b05eae4fb096533b14861827a139c61`",non_debt,-
couchdb,470,review,110758097,Shouldn't this be couch_replicator_multidb_changes?,non_debt,-
spark,29964,comment,704881030,+1; it sounds nice.,non_debt,-
spark,3245,review,20504747,"i donot think so. because it is FIFO, we can remove head of request queue because head of queue is allocated by Yarn when we receive allocated Containers.",non_debt,-
incubator-mxnet,12426,description,0,"The PR applies the website theme to each version. The navigation will be the same, so the option for Clojure needs to be handled properly for old versions. For this I use the .htaccess file to redirect users to an API error page. For good measure, I also added a custom 404 error page.
This PR stacks on #12413 (has the same changes in build_all_version.sh), plus a change to copy the theme, and fixes my concerns there with the theme.
http://34.201.8.176/
You can test the redirect if you switch to an old version like 1.0.0 and go to API --> Clojure
You can look at the 404:
http://34.201.8.176/error/404.html 
I'm sure there's probably some fancy regex that would collapse the clojure rules to one line, but I'll let someone else get fancy.",documentation_debt,low_quality_documentation
hadoop,2737,review,602183347,"@functioner , we could make 10 as the default pool size.",non_debt,-
kafka,6163,review,252942686,"nit: since it's just an accessor, maybe we could drop the parenthesis?",code_debt,low_quality_code
tvm,217,review,125549797,"Will we have other tags? If not, we can simply use tag",non_debt,-
cloudstack,801,comment,153367941,"@jburwell  @remibergsma  Pls note all again that the changes to core are very minimal and are limited to convenience extensions only. I would expect running the CI should be good in verifying that the plugin changes don't break anything in core.
Thanks",non_debt,-
incubator-mxnet,4513,comment,270335078,@piiswrong i'll add it these days.,non_debt,-
tvm,6230,comment,670308769,70746484-6230 comment-670308769,non_debt,-
cloudstack,3113,comment,454822917,"Merging this one as the tests are looking good and we have 3 LGTMs.
Thanks for the PR @wido, and for the reviews @rafaelweingartner and @dhlaluku.",non_debt,-
spark,29661,review,486671789,"perhaps we should add a default case to handle other types which are not short, int or long.",non_debt,-
tvm,1047,description,0,70746484-1047 description-0,non_debt,-
spark,2157,comment,53669323,"yes,no problem :) close this issue",non_debt,-
kafka,4436,review,162450333,Ack.,non_debt,-
spark,17406,review,108049886,okay,non_debt,-
spark,5383,comment,91342733,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/29954/
Test PASSed.",non_debt,-
spark,22110,review,210207729,I don't think this should be a property of the data type. It's specific to the `OpenHashSet`. How about we add this method to `object OpenHashSet`?,non_debt,-
beam,2330,review,112816993,.toString() is unnecessary,code_debt,complex_code
spark,28401,comment,621591568,"I wish we had a merge bot, which could detect a LGTM from a committer and do the need full.",non_debt,-
reef,261,review,33703686,Does this class need to be `public`?,non_debt,-
arrow,4483,review,291247898,"Yes, it's what `DataFragment` is for. See `FileBasedDataFragment` -- it has a reference to an implementation of `FileFormat` which has the logic for scanning that kind of file",non_debt,-
spark,16400,comment,269686019,"Hi, @gatorsmile .
Could you review this `CREATE TABLE ... LOCATION` document issue when you have sometime?",non_debt,-
spark,2933,review,19377630,"I think it's better to keep this internal, it's a tradeoff between 1.0 and 1.1, most of the users do need to touch this.
We could document it later if user really need it.",design_debt,non-optimal_design
spark,5609,description,0,"We should let Thrift Server take these two parameters as it is a daemon. And it is better to read driver-related configs as an app submited by spark-submit.
https://issues.apache.org/jira/browse/SPARK-7031",non_debt,-
thrift,448,comment,183605715,"@nsuke, I switched it so we now consider a `long` like two `int`s (the high bits and low bits) and we combine their values like we combine `hashCodes` (just with a different multiplicative factor). For `double`s we get the `long` represented by their bytes and treat as before. Since we're changing the values, I also took the chance to pick arbitrary constants to combine with.
For what it's worth, though, the previous `hashCode` helper functions are pretty much what are described in Effective Java. Anyway, I'll rebase and squash after you have a look.
@jsirois That's a good idea. Unfortunately, I don't have the time at the moment to do that :(",design_debt,non-optimal_design
spark,19460,review,143571046,I get declaring a constant though it doesn't just pertain to byte arrays. I couldn't find a good place for it. I don't know if its reused so much that it needs this,code_debt,low_quality_code
spark,21432,comment,392299805,@mgaido91 Thanks for fixing this.,non_debt,-
kafka,8402,comment,609450055,test this please,test_debt,lack_of_tests
kafka,8828,comment,642272317,"@kkonstantine and @C0urante: thanks for the review. I think I've incorporated all of your feedback and addressed all of your questions. @C0urante, I've even tried to improve the failure message to say what needs to be done if the topic has an unacceptable `cleanup.policy`.
I'd appreciate another pass. Thanks!",non_debt,-
spark,8845,comment,144180734,@Attsun1031 Can you close this?,non_debt,-
arrow,8188,comment,698887227,@jorisvandenbossche PTAL,non_debt,-
spark,9741,comment,157553204,17165658-9741 comment-157553204,non_debt,-
reef,589,comment,151060064,"@markusweimer  @jwang98052 
This is the pull request for creating IPartitionedOutput dataset. Please review.",non_debt,-
beam,4541,description,0,"Get the ""query"" option from configuration not options which is from command line only. But when running the test by test suite(like --suite=DEFAULT), there is no query option from the command line. Thus we should always get the ""query"" from the ""configuration"" which covers both the command line and test suite.",non_debt,-
flink,15049,review,597725353,I don't see a lot of value in adding another interface for the sole purposes of differentiating between the 2 current implementations.,non_debt,-
incubator-dolphinscheduler,1629,comment,569509881,"# [Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/1629?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/1629?src=pr&el=continue).",non_debt,-
calcite,1688,review,361354388,"i think `||` will be better, if `!(Objects.equals(query.fetch, target.fetch)` is true, it will return null,  not need to consider second condition.",non_debt,-
kafka,10039,review,571271791,"Feels slightly odd to pass in the set of topics to load here, but I can't think of a good way to avoid it. Perhaps we could pass MetadataCache into LogManager and let startup call MetadataCache#getAllTopics? That might be more risky though since it changes the startup order in KafkaServer, maybe we can look into this as a follow-up.
Besides that, the name here seems strange. Maybe something like ""topicsToLoad""?",code_debt,low_quality_code
spark,4557,comment,74170332,Alright merging into master and 1.3.,non_debt,-
pulsar,2101,review,202403205,"if using an Optional, this becomes:
Looks cleaner to me.",code_debt,low_quality_code
spark,29104,review,459789858,"Why is one branch of the code checking for anyNull, and the other allNull ?
because in LongHashedRelation
kevEv.value it a java long type, it certainly could not call allNull
as in UnsafeHashedRelation
kevEv.value is a UnsafeRow, this is why separately branch needed.
anyway, I will drop these code to remain ""single column"" focussed",non_debt,-
samza,753,review,232373594,See kafka consumer config,non_debt,-
incubator-pinot,2475,review,167364254,"No, I insist it be there :-)",non_debt,-
groovy,735,description,0,"The JVM keeps a cache of URL connections, which keeps
the underlying files open for the lifetime of the VM
unless the `setUseCaches(false)` method is used.
This change makes it possible to reuse the Groovy compiler,
e.g. embedded in the Gradle daemon without preventing files
from being deleted on Windows.",non_debt,-
arrow,6671,comment,604239799,"I think that we don't need to care about AppVeyor because:
  * AppVeyor has few jobs now
  * AppVeyor cancels pending jobs when we push new commits to pull request",non_debt,-
madlib,77,review,93534065,Thanks. Fixed now.,non_debt,-
thrift,1411,comment,343609905,He @trotterdylan please review https://thrift.apache.org/docs/HowToContribute as we need a Jira ticket for this change.,non_debt,-
spark,15443,review,83371347,"We will support histogram for numeric types, but the logics in that agg function will be very different from this. You can refer to discussion on the [jira](https://issues.apache.org/jira/browse/SPARK-17074).",non_debt,-
storm,168,comment,51805865,"I have deployed this in a development cluster, and inconsistently see errors around the dreaded ""IllegalStateException: unread block data"", as well as some issues where my custom deserialization code in classes is appearing to receive incomplete blocks. I didn't see it for every topology, several went along fine, even after several redeploys. But occasionally I would hit deserialization problems. It's only with the gzip implementation. I have configured the DefaultSerializationDelegate and not seen any issues since.
@revans2 I believe you said you've been running this at your place for a while now (before the pull request), did you have any similar experiences?",non_debt,-
nifi,536,summary,0,NIFI-1037 Ported processor for HDFS' inotify events to 0.x.,non_debt,-
spark,16680,comment,277992951,"thanks, merging to master!",non_debt,-
flink,5239,review,162584582,üëç,non_debt,-
incubator-weex,910,description,0,"WXScrollerComponent scrollViewDidScroll, Collection NSHashTable was mutated while being enumerated",non_debt,-
kylin,1063,description,0,28738447-1063 description-0,non_debt,-
ignite,821,summary,0,GG-11133 TTL should be tracked in off-heap page structures,non_debt,-
samza,284,review,135369813,probably need to wrap e inside a SamzaException and rethrow here,non_debt,-
kafka,8367,description,0,"Call for review @abbccdda @guozhangwang 
System test run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3856/",non_debt,-
skywalking,3074,comment,512103984,"Check the e2e, whether this change breaks test. We test e2e w/ zookeeper",non_debt,-
incubator-mxnet,8227,review,144140978,I can enable it by checking MSVC version seems was due to the introduction of advanced template here https://github.com/dmlc/nnvm/blob/master/include/nnvm/tuple.h#L615,non_debt,-
spark,6558,review,31485428,"oh, I just realized that when we reuse the Decimal value, we do not really need to use the returned value. But, we have another place that needs the returned value. Can we add a comment at here?",code_debt,low_quality_code
beam,1048,review,82713525,They weren't before this PR; don't see a need to expand the API surface as part of this refactoring.,non_debt,-
pulsar,6196,review,374325392,Instead of `3.7` we should use `PYTHON_VERSION` variable,code_debt,low_quality_code
spark,22258,description,0,"BarrierCoordinator uses Timer and TimerTask. `TimerTask#cancel()` is invoked in ContextBarrierState#cancelTimerTask but `Timer#purge()` is never invoked.
Once a TimerTask is scheduled, the reference to it is not released until `Timer#purge()` is invoked even though `TimerTask#cancel()` is invoked.
I checked the number of instances related to the TimerTask using jmap.",non_debt,-
arrow,5028,review,311448316,"ah i see what you mean, sounds good.",non_debt,-
beam,2791,comment,298267241,retest this please,non_debt,-
trafficserver,803,comment,239984516,FreeBSD build _successful_! See https://ci.trafficserver.apache.org/job/Github-FreeBSD/538/ for details.,non_debt,-
ignite,5774,review,246697079,`[@code` -> `{@code`,non_debt,-
iceberg,1145,review,465467211,"the name is hidden inside BaseTable, yeah, we could use `Table.toString()`  to get that `name`.",non_debt,-
kafka,7223,description,0,"Make offsets immutable to users of RecordCollector.offsets. Fix up an
existing case where offsets could be modified in this way. Add a simple
test to verify offsets cannot be changed externally.
*More detailed description of your change,
if necessary. The PR title and PR message become
the squashed commit message, so use a separate
comment to ping reviewers.*
*Summary of testing strategy (including rationale)
for the feature or bug fix. Unit and/or integration
tests are expected for any behaviour change and
system tests should be considered for larger changes.*",non_debt,-
cloudstack,1098,comment,158686367,"@DaanHoogland I don' known the best way to explain the transifex process.
For all versions of CloudStack, we have a version of resources files on Transifex [1].
When you update the main resource file with the new version, the transifex config file is updated with all languages with already translated.
I don't know the best way to explain the internal behavior of the transifex client, but the config file is update after the first download of the main resource file (en_US) and the upload of the current L10N files to the new L10N files for the new versions. This PR reflect the changes.
You can retry the behavior from the master (up to date 2015/11/21) with theses commands:
To fetch the latest original L10N file form Transifex (already done because I'm put the 4.7 main resource files on the TX website today)
cd tools/transifex/
 ./sync-transifex-ui.sh download-source-language CloudStack_UI.47xmessagesproperties
To upload all L10N translations files (from 4.6 files) to transifex (already done by me today, but you can re-made the work during few days before the translator team changes the strings on Transifex)
./sync-transifex-ui.sh upload-l10n-languages CloudStack_UI.47xmessagesproperties
[1]https://www.transifex.com/ke4qqq/CloudStack_UI/content/",non_debt,-
couchdb,2338,description,0,"     with your text. If a section needs no action - remove it.
     Also remember, that CouchDB uses the Review-Then-Commit (RTC) model
     of code collaboration. Positive feedback is represented +1 from committers
     and negative is a -1. The -1 also means veto, and needs to be addressed
     to proceed. Once there are no objections, the PR can be merged by a
     CouchDB committer.
     See: http://couchdb.apache.org/bylaws.html#decisions for more info. -->
This change should allow users to supply all params in [POST](http://docs.couchdb.org/en/stable/api/ddoc/views.html#post--db-_design-ddoc-_view-view) that can be supplied for [GET](http://docs.couchdb.org/en/stable/api/ddoc/views.html#get--db-_design-ddoc-_view-view) now. This way we could avoid the `?key=""foo""` things that would probably cause a lot of pain for users.
     what problem it solves or how it makes things better. -->
So far this has been tested manually and it seems to be working.
     Does it provides any behaviour that the end users
     could notice? -->
     repositories please put links to those issues or pull requests here.  -->",non_debt,-
phoenix,325,description,0,"A suggested improvement is to add a Canary Test tool to the Phoenix Query Server. It will execute a set of Basic Tests (CRUD) against a PQS end-point and report on the proper functioning and testing results.
@joshelser It would be great if you could review this.
@vincentpoon @karanmehta93",non_debt,-
incubator-pinot,5766,comment,667437954,@siddharthteotia Addressed all the comments,non_debt,-
accumulo,221,review,103497233,Most the properties are being set in accumulo-env.sh.  Some of the log forwarding properties are set in Java but that should change with your new appender.,non_debt,-
flink,2016,comment,222136775,"Thanks a lot for addressing the comments.
I'll push the changes to travis, once its green I'll merge them.",non_debt,-
dubbo,3448,description,0,"fix #2619: is there a problem in NettyBackedChannelBuffer.setBytes(...)?
XXXXX
XXXXX",non_debt,-
trafficserver,2458,description,0,356066-2458 description-0,non_debt,-
spark,1270,review,17257206,"`RDD[(Set[Double], Set[Double])]` may be hard for Java users. We can ask users to input `RDD[(Array[Double], Array[Double])]`, requiring that the labels are ordered. It is easier for Java users and faster to compute intersection and other set operations.",design_debt,non-optimal_design
accumulo,501,review,191754179,"For endRow, null represents max.   For prevEndRow, null represents minimum.",non_debt,-
druid,8231,comment,517966932,@fjy the console auto renders based on your query as it does with all of them. If you set `resultAsArray: true` in the query context of a grouBy (this is a new option) the query response will have array rows. With this PR the console will be able to auto render it. Before this PR it would give you an error in the table.,non_debt,-
cloudstack,2896,comment,433614900,Packaging result: ‚úîcentos6 ‚úîcentos7 ‚úîdebian. JID-2389,non_debt,-
carbondata,2508,comment,424884106,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/543/",non_debt,-
pulsar,4714,review,302887349,I think the list is sorted in alphabet order. so you have to add them to be after `data-generate` and before `elastic-search`.,non_debt,-
netbeans,2606,summary,0,[NETBEANS-5161] Prevent IAE when resolving composite project dependencies,non_debt,-
samza,1035,description,0,As per subject,non_debt,-
flink,3535,comment,287728689,"I can not really assess what this change is doing in detail, meaning how it affects typical workloads. Would need a bit more context.
The change looks small an innocent and seems to have decent tests ;-)",non_debt,-
ignite,2732,summary,0,Ignite-gg-12751,non_debt,-
incubator-mxnet,3781,comment,260116824,@sxjscience Yeah this will cut out a lot of code in our framework that were just there to redundantly track tensor dims for the purposes of feeding them to Reshape layer. I'm very excited about this. A small increase in (optional) complexity in Reshape layer pays off with a large decrease in complexity in our framework code.,design_debt,non-optimal_design
incubator-mxnet,4405,description,0,34864402-4405 description-0,non_debt,-
flink,8859,review,350512524,"Did you mean a temporal object could exist without a catalog ? For example, the view ?",non_debt,-
spark,30072,comment,710745499,"Thank you, @viirya , @HyukjinKwon , @mridulm .
Merged to master.",non_debt,-
tvm,2292,review,241858156,using `unwrap` in libraries is generally a bad idea if it's possible to panic. please use `?` since you're already returning a `Result`,code_debt,low_quality_code
airflow,13209,comment,792993349,@dstandish this approach seems acceptable to me :+1:,non_debt,-
carbondata,3485,comment,560424246,"Build Failed  with Spark 2.3.2, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1077/",non_debt,-
nifi,1564,review,104316845,hitCounter should not be assigned from the return of setDictionaryTermMatch.  It should simply be incremented prior to the call.,non_debt,-
gobblin,2729,review,322481788,why are we doing + 1?,non_debt,-
storm,1692,summary,0,Fix STORM-2017,non_debt,-
cloudstack,2071,review,114259061,"@karuturi the interfaces are in 'tls' sub-package, but the impl are still in 'ssl'?",non_debt,-
madlib,257,comment,381447654,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/madlib-pr-build/450/",non_debt,-
cxf,524,comment,474016221,"JAXB has no word about JAX-RS so don't think so as well, it is a CXF specific feature AFAIK
will close this one to open another PR (using my IDE instead of github online edit since it needs more work than I thought)
thanks guys for the feedback",non_debt,-
spark,7940,review,36339415,"I think so, will re-purpose this PR to fix the thead-safy issue about UnsafeProjection. (revert the changes about broadcast)",non_debt,-
trafodion,1578,comment,391783684,Test Failed.  https://jenkins.esgyn.com/job/Check-PR-master/2673/,non_debt,-
flink,10358,comment,561655258,"@shuttie Got it, makes a lot of sense to me.
I was wondering at some point if it would be worthwhile to just ""unsafe arraycopy"" the char[] from the string to the byte[] in the memory segments or stream buffers. So basically no byte-wise logic at all in the serialization. That would increase the state size (all chars would have two bytes), but might save CPU resources.
Have you ever experimented with something that?",requirement_debt,non-functional_requirements_not_fully_satisfied
kafka,6177,comment,481373001,@abbccdda @guozhangwang Sounds good to me to resolve this problem separately. I'll do another pass on the PR today and hopefully we can merge this week.,non_debt,-
avro,910,review,437448312,"Hey @RyanSkraba ,
Really appreciate the help on this.  It's quite annoying.
I just pulled down the latest changes to the master branch and did ""Import [Maven] Project"" and when it's doing it's initial build, I get this error message.  Nothing special to it.  May be a Eclipse version issue. 
if you add both a `<version>` and `<versionRange>` tag, does that build?  That may be the resolution to make all versions happy.",non_debt,-
spark,28702,review,436319781,"Why not using assert?
Do we expect this test suite will be skipped if this condition is not true?",non_debt,-
kafka,5787,review,224774703,"Maybe `Controlling downstream updates from a KTable` or something similar, just a thought.",non_debt,-
spark,3579,description,0,"cc @aarondav @kayousterhout @pwendell 
This should go into 1.2?",non_debt,-
fineract,1050,review,439726409,üòÇüòÇüòÇ Completely forgot.,non_debt,-
arrow,5078,description,0,"Related to [ARROW-6206](https://issues.apache.org/jira/browse/ARROW-6206).
Specifically, ""-Dio.netty.tryReflectionSetAccessible=true"" for JVMs >= 9 and BoundsChecking/NullChecking for get.",non_debt,-
flink,5516,comment,369528309,Reopen this pull request to trigger a ci build.,non_debt,-
cloudstack,1403,review,63082200,@DaanHoogland Done,non_debt,-
flink,12556,review,438019081,20587599-12556 review-438019081,non_debt,-
beam,5921,comment,404040988,run java precommit,non_debt,-
calcite,32,summary,0,[CALCITE-92][CALCITE-486] Optimize away Project that merely renames fields,non_debt,-
arrow,1115,summary,0,ARROW-1554: [Python] Update Sphinx install page to note that VC14 runtime may need to be installed on Windows,non_debt,-
arrow,5508,comment,535832777,"@emkornfield I created a new benchmark to evaluate the performance of consumer directly. The improvement is not significant:
after: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  77326.747 ¬± 218.829  ns/op
before: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  79007.087 ¬± 63.994  ns/op
I think there are two reasons for this:
1. in our benchmark, the jdbc implementation is based on h2, and for this library the wasNull method implementation was simple: 
    @Override
    public boolean wasNull() throws SQLException {
        try {
            debugCodeCall(""wasNull"");
            checkClosed();
            return wasNull;
        } catch (Exception e) {
            throw logAndConvert(e);
        }
    }
It can be seen that the implementation is based on a simple flag, plus some simple checks. For other implementations with other RDBs, the implementation can be more heavy. For example, the following code gives the implementation of MySQL JDBC, which may involve megamorphic virtual calls:
  public boolean wasNull() throws SQLException {
    try {
      return this.thisRow.wasNull();
    } catch (CJException var2) {
      throw SQLExceptionsMapping.translateException(var2, this.getExceptionInterceptor());
    }
  }
2. The time for wasNull method was insignificant compared with the Arrow set methods.",code_debt,slow_algorithm
zookeeper,975,summary,0,ZOOKEEPER-3410:./zkTxnLogToolkit.sh will throw the NPE and stop the process of formatting txn logs due to the data's content is null,non_debt,-
hadoop,1179,summary,0,HDDS-1870. ConcurrentModification at PrometheusMetricsSink,non_debt,-
spark,4015,comment,95799659,"@marmbrus Any more comment on this before merging? It will be great appreciated if you merge this soon, as I did take lots of time in rebase again and again. :)",non_debt,-
calcite,719,description,0,I added branches that expect requests for MIN and MAX aggregation accumulators.,non_debt,-
beam,7811,comment,462484659,LGTM. Thank you Ahmet!,non_debt,-
carbondata,1417,comment,337614957,"Build Success with Spark 1.6, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/354/",non_debt,-
arrow,8757,comment,733751240,"Oh, I know why! The CI queue was full (my test runs were taking hours to go through). By the time the runner got to my PR (Wed, 25 Nov 2020 02:15:47 GMT), my final commit with the manual cmake autoformat had gone in about an hour before.",non_debt,-
spark,21142,comment,384182237,retest this please,non_debt,-
streams,133,comment,63707989,:+1:,non_debt,-
zeppelin,2921,comment,380782977,will merge this if there are no more discussions,non_debt,-
arrow,4322,comment,493617041,"For posterity, I came across this discussion around the original hard-coding of the rpath: https://github.com/apache/arrow/pull/2489#discussion_r215664651.",code_debt,low_quality_code
fineract,1295,summary,0,FINERACT-1133 [Backport PR] Added Mustache templates,non_debt,-
incubator-pinot,1192,summary,0,Adding regex predicate support,non_debt,-
spark,3518,comment,70188737,"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/25620/
Test FAILed.",non_debt,-
spark,25900,summary,0,Run python tests,non_debt,-
spark,16249,comment,272744836,"it turns out `$0` is not reliable (http://stackoverflow.com/questions/59895/getting-the-source-directory-of-a-bash-script-from-within).
Updated and tested on Ubuntu and Mac, with relative path, full path etc.",non_debt,-
nifi-minifi-cpp,975,review,589557792,"These maps are used to initialize some static class members in PutS3Objects. If `static` is removed, because of the order of the initialization those members will not be initialized.",non_debt,-
gobblin,1106,review,70877402,"Is there any reason to `catch Throwable`, instead of specific exceptions?",code_debt,low_quality_code
spark,17255,comment,285846909,17165658-17255 comment-285846909,non_debt,-
trafficserver,679,comment,222863351,"Honestly I would use _aux myself. Allen said somebody hates _ so i made it ts_aux. honestly it does not matter it just cannot be prn, aux, or con
It is useful for ""generated"" items to be in directories that are easy to find and delete outside some tool being used.",non_debt,-
spark,29387,review,474256467,note this is not always honored: if server side has this configured then the client side one will be ignored,non_debt,-
beam,1541,comment,265595532,LGTM after fixing the import orders.,non_debt,-
nifi,1800,comment,303611024,"couple of quick thoughts.
- we'll need to get all the version numbers aligned with whatever nifi version this would be committed into.  Currently that would be 1.3.0-SNAPSHOT.
- It would probably be a good idea to have the notion of 'nifi-leaderelection-api' which is not about zookeeper but rather just generic election/tracking of a leader for a given thing (a partition?) Then there would be zookeeper based implementations of those.  Processors then can leverage the api for their code but users can select whichever types of services exist (zookeeper being the obvious initial example).  The structure appears already in place for this other than the current naming and perhaps the API referencing zookeeper.  Thoughts?
- It would be good to have a processor which leverages this or some docs that go along with it to show suggested usage.",design_debt,non-optimal_design
spark,17117,review,104165225,"Yeah, your suggestion can work well, but I'm more prefer to my way, since it's more clear for developer to understand what happened.",non_debt,-
tvm,5167,comment,606314013,Hi @siju-samuel Thanks for the effort. Can you please give a usecase where we use quantized division? is there any quantized division operator?,non_debt,-
spark,20621,review,168924965,"I don't think it is enough to go always with the cast path, since it allows many format/strings, not allowed by the parse method. Thus I think it not safe to avoid the parse method.",non_debt,-
ignite,7813,review,427219960,Move `Commons.comparator(..)` code into `ExpressionFactory.comparator(..)` method,non_debt,-
trafficserver,1732,comment,297137226,clang format *failed*! https://ci.trafficserver.apache.org/job/clang-format-github/305/,non_debt,-
beam,8164,comment,483442080,Run Python PostCommit,non_debt,-
hbase,465,summary,0,HBASE-22808 HBCK Report showed the offline regions which belong to di‚Ä¶,non_debt,-
activemq-artemis,3048,review,397651002,"Largemessage can be concurrent accessed and used. In case of topic with multiple queues. 
Remember how we had to fix loads of concurrent access issue on core message the other year",non_debt,-
arrow,5011,review,312339810,nice catch. thank you so much.,non_debt,-
carbondata,791,comment,295166284,"LGTM
@chenliang613 kindly reivew",non_debt,-
netbeans,1312,review,322004752,Done :),non_debt,-
incubator-pinot,2371,summary,0,[TE] Self-Serve tuning flow 04: Alert page overview,non_debt,-
trafficcontrol,1613,summary,0,Add checks for definition of signing algorithm field,non_debt,-
flink,8438,review,287425587,I think we don't need this test here.,test_debt,expensive_tests
tinkerpop,308,description,0,"https://issues.apache.org/jira/browse/TINKERPOP-1297
Also Improved output of HTTP request errors to gephi. It is likely that this change will not allow Gephi to work with 0.8.x anymore.
Tested manually and ran tests with: 
VOTE +1",non_debt,-
incubator-mxnet,5703,description,0,"Changes to broadcast_to, broadcast_axis, linearregressionoutput, logisticregressionoutput, maeregressionoutput function documentation.
@mli, @zackchase, @nswamy @madjam",non_debt,-
orc,635,review,577084460,"The existing SArg application only happens on File, Stripe and RG statistics so even in the worst case it will RowIndexStride times better than what we see with filter.
With the filter the evaluation happens on every row.",non_debt,-
hudi,2374,review,550972066,Not anymore,non_debt,-
incubator-mxnet,9397,review,161283590,"This is something you'd need to discuss on dev. Currently between MSVC, Clang and gcc, some things generate errors and some don't.",non_debt,-
kafka,1215,comment,240870837,"@becketqin : Interesting, in theory, using more than 1 thread should still help since those threads can drive the I/Os on different disks in parallel.",non_debt,-
ignite,5635,review,241036804,"Hmm... We will avoid boilerplate, but seems like it will make code more error prone. Developer can forget to override this method for trainer which potentially supports updating and get an error while trying to update this model in the future whereas keeping it abstract forces developer to think if this trainer supports update and insert `NotImplementedException` more cautiously.",design_debt,non-optimal_design
incubator-doris,5522,summary,0,[Enhance] Show brokers' hostname,non_debt,-
phoenix,1096,review,563519743,Done,non_debt,-
drill,1864,review,329997181,New line after line 48.,non_debt,-
spark,22847,review,228600757,"it will not be JITted; it also should not not be too small, otherwise there will be many function calls.",design_debt,non-optimal_design
spark,15053,comment,246267263,Jenkins test this please,non_debt,-
samza,91,review,107049828,"I don't think it matters much, but the caller may be in a different place from where the object was instantiated.",non_debt,-
druid,2179,review,48675138,comma here should be a ;,non_debt,-
airflow,4787,review,261342578,I apply your suggestion.  I will introduce similar changes in other places in the project as seperate PR. I add your suggestion to the list of my tasks.,non_debt,-
spark,30204,review,516533952,nit: Can we unify this with `createYarnResourceForResourceProfile` in YarnAllocator ?,code_debt,low_quality_code
qpid-dispatch,1097,review,608990009,"Can we drop ""Adaptor"" from this class name?  When I first read this I was assuming it was a set of tests that pertained only to the HTTP1 Adaptor.
Also should this class be a subclass of ""object""?",code_debt,low_quality_code
dubbo,5160,summary,0,changes log,non_debt,-
spark,18519,review,132740552,"You're not changing anything here now, are you?",non_debt,-
spark,22469,comment,422816846,cc @cloud-fan,non_debt,-
lucene-solr,2423,review,581247428,"what is this? is it doing a union? For a union, it would be better to use BasicOperations.union on Automaton objects rather than mess around with regexes as strings",code_debt,low_quality_code
spark,20751,comment,371217148,LGTM - merging to master. Thanks!,non_debt,-
tvm,1157,review,193950613,alignment,non_debt,-
airflow,5019,comment,485388491,"@potiuk Don't be sorry, I rebase and squash commit into one commit. Waiting for CI.",non_debt,-
groovy,1236,comment,623920223,"I think the feature request has been denied, so we can close this PR",non_debt,-
netbeans,2484,summary,0,Bugfix xml schema completion (Fix catalog handling and tests),non_debt,-
drill,1723,review,272802660,This looks incorrect assignment.  V4 is assigned to V3 ?,non_debt,-
pulsar,9970,review,599613263,good idea,non_debt,-
kafka,3530,review,130078682,It would be nice to allow the staging receives to complete and verify that the closing channel is also empty like in the other test.,test_debt,expensive_tests
beam,11567,comment,627307430,Everything LGTM. Thanks again @kamilwu Merging :),non_debt,-
pulsar,43,review,80817924,"Here, to avoid creation of new ByteBuf we modify same DoubleByteBuf of the message with newly computed checksum.
However, while message creation if we see memory-leak then we create `SimpleLeakAwareByteBuf` or `AdvancedLeakAwareByteBuf` (based on `ResourceLeak Level`) instead   `DoubleByteBuf`. So, should we keep this check.",design_debt,non-optimal_design
spark,4141,comment,73290287,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/26921/
Test PASSed.",non_debt,-
netbeans,2252,review,453451304,"I see, this is the fix then.",non_debt,-
kafka,1251,review,92517670,"Are some of these fields supposed to be `final` (existing code, I know)?",non_debt,-
arrow,66,review,64469901,"This is actually somewhat problematic, since two arrays might be unequal but their unequal parts are ""masked"" by the parent bitmap. For example
so the data is technically equal, even though the children are different when you examine them without the ""mask"" of the parent bitmap
this suggests that the `Equals` method should accept an inclusion bitmap, which adds a lot of complexity. @emkornfield what do you think?",code_debt,complex_code
beam,3835,review,145115569,"I see, thanks, did not know about DoFn possibly having setup/teardown, just updated... I'm planning to have another PR at the next stage which will address all of the Configuration related improvements (custom content handlers, etc, and now including the possibility of passing the XML configuration fragment as you suggested). 
Re the shortcut and ParseResult success/failure, I've np with continuing looking into it in this PR, but may be it will be easier, esp for the reviewers, to merge what is already available, this IO is still Experimental so I guess it will be safe enough, but it's up to the team",non_debt,-
kafka,7626,comment,548535083,ping @mjsax and @vvcephei for review,non_debt,-
trafficserver,2342,review,132039635,"The maximum QUIC packet size depends PMTU. So, it can be the same as the maximum payload size of UDP.",non_debt,-
trafficserver,522,description,0,356066-522 description-0,non_debt,-
flink,11797,review,411080458,ditto,non_debt,-
incubator-pinot,141,review,65645131,"Sharing is fine for children of the same parent. It makes the 'remove' redundant, but saves a lot of garbage.",code_debt,low_quality_code
helix,834,description,0,"Fixes #833
In this PR, the routing table provider has been changed in a way
to include customized view feature.
TestRoutingTableProvider.testExternalViewWithType
TestRoutingTableProvider.testCustomizedViewWithoutType
TestRoutingTableProvider.testCustomizedViewCorrectConstructor
TestRoutingTableProvider.testGetRoutingTableSnapshot
TestRoutingTableProvider.testSnapshotContents
[INFO] Tests run: 914, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,559.344 s - in TestSuite
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 914, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  59:24 min
[INFO] Finished at: 2020-02-27T16:27:09-08:00
[INFO] ------------------------------------------------------------------------",non_debt,-
spark,27897,comment,600950809,"A separate PR https://github.com/apache/spark/pull/27952 was opened to focusing on HiveClient related code. 
For the fallback solution of Hive 2.2- versions that @cloud-fan suggested, I plan to handle it in `HiveExternalCatalog.listViews` rather than `HiveClient.listViews`. So will update here in this PR later. Thanks!
@maropu @dongjoon-hyun @cloud-fan",non_debt,-
kafka,7908,summary,0,KAFKA-9068: Fix incorrect JavaDocs for `Stores.xxxSessionStore(...)`,documentation_debt,low_quality_documentation
nifi,334,comment,211091503,"Perfect, working on it now",non_debt,-
spark,702,comment,42735249,Merged build started.,non_debt,-
incubator-mxnet,5518,description,0,"Pad value is not provided when BucketSentenceIter.next() returns DataBatch, and this may broke module.predict(). Since BucketSentenceIter will discard a few data insufficient for a mini-batch at the end of each bucket, so set pad = 0 seems okay.",non_debt,-
kafka,3662,review,132859349,"I tried breaking up the new test method in `WorkerSinkTaskTest` into multiple methods, but it actually just made it harder to read.",code_debt,low_quality_code
ozone,1844,comment,767393159,"Thanks @GlenGeng for the offer.  Is it repeatable (ie. after the test can you restore prior state), or do we have only one shot?",non_debt,-
openwhisk,10,description,0,This demonstrates the use of Travis-CI to run tests.,non_debt,-
ignite,4970,summary,0,9769 test flakiness,test_debt,flaky_test
netbeans,2571,comment,737151395,Test added.,non_debt,-
kafka,764,review,51615946,"Could we put those comments in a more prominent place like the beginning of the class? With v1 message format, we are adding a timestamp, a timestamp type attribute, and are using a relative for inner message. It would be useful to document the format in a bit more details for both the outer and the inner message. For example, should the timestamp type attribute be set for inner messages?",documentation_debt,low_quality_documentation
gobblin,2877,review,373897715,"I am using Hadoop's GlobPattern instead of java.util.regex, because I could not find any API in java.util.regex which tells if the string is a plain string or contains special characters. Do you know any API in java.util.regex? or should I put this reason in comment?",non_debt,-
flink,7764,comment,465875192,cc @uce and @zentol,non_debt,-
trafficserver,3558,description,0,Initial commit of the ASYNC_JOBS was not correctly initializing the ASYNC_JOB features.  Need to read some configs before attempting to initialize.,non_debt,-
apisix-dashboard,1673,review,602968683,"I mean creating a variable `_allHTTPMethods`, which is:",non_debt,-
kafka,6239,review,256163993,minor - no test coverage for lines 221 - 225 in the unit test,test_debt,low_coverage
incubator-heron,3123,review,242657195,+1 for reducing the redundancy.,code_debt,complex_code
nifi,2231,review,159024001,Microsoft recommends not to refer system table like change_tables instead use sys.sp_cdc_help_change_data_capture. https://docs.microsoft.com/en-us/sql/relational-databases/system-tables/cdc-change-tables-transact-sql,non_debt,-
ozone,752,review,402095314,Fixed.,non_debt,-
spark,17779,comment,297635333,merged to master,non_debt,-
nifi,602,comment,230517821,"Ok sounds good. I gave the Integration Test thing (and the QueryCassandra -- actually in AbstractCassandraProcessor) fix a try, the ITs don't work (yet) but feel free to take a look: https://github.com/mattyb149/nifi/tree/cassandra_time
I'll write up the Jira for QueryCassandra, and if I get a chance to test the fix I will submit the PR myself and you can review if you like :)
Will do a final look-around then merge, thanks again!",non_debt,-
tvm,7657,summary,0,[docs] Set USE_LLVM OFF when build VTA on pynq board,non_debt,-
incubator-pinot,6409,review,557020612,"We need to handle the case for distance > 0, and not return the same point",non_debt,-
pulsar,5271,comment,534781690,rerun cpp tests,non_debt,-
spark,1285,comment,47998047,Good catch! Can you add a unit test for queue input stream? It could be in the InputStreamsSuite.scala.,test_debt,lack_of_tests
superset,10473,review,466556166,"Typing here seems not needed. If source is optional, you can also give it a default value, otherwise maybe make it non-optional. There is actually an [eslint rule](https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/require-default-props.md) for this in the PropTypes world.",code_debt,low_quality_code
carbondata,4072,comment,795431956,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12444/job/ApacheCarbonPRBuilder2.3/5543/",non_debt,-
airflow,15238,comment,814493510,@FloChehab Can you look at it?,non_debt,-
echarts,12947,comment,661637238,"`legendDesc`
`legendAsc`
`valueDesc`
`valueAsc`",non_debt,-
beam,11503,comment,628793076,"Don't submit yet, there are some breaking internal tests",non_debt,-
cloudstack,4385,review,564464767,@shwstppr possible to optimise multiple checks for _getHostId()_ and _getStorageId()_ not null here ?,non_debt,-
skywalking,6525,description,0,"    ‚ö†Ô∏è Please make sure to read this template first, pull requests that don't accord with this template
    maybe closed without notice.
    Texts surrounded by `<` and `>` are meant to be replaced by you, e.g. <framework name>, <issue number>.
    Put an `x` in the `[ ]` to mark the item as CHECKED. `[x]`
-->
     ==== üêõ Remove this line WHEN AND ONLY WHEN you're fixing a bug, follow the checklist üëÜ ==== -->
     ==== üîå Remove this line WHEN AND ONLY WHEN you're adding a new plugin, follow the checklist üëÜ ==== -->
     ==== üìà Remove this line WHEN AND ONLY WHEN you're improving the performance, follow the checklist üëÜ ==== -->
     ==== üÜï Remove this line WHEN AND ONLY WHEN you're adding a new feature, follow the checklist üëÜ ==== -->",non_debt,-
incubator-mxnet,6273,review,116819550,"It's not good to have all the channels start with ""On the ...""
Better to change the prompt above to 
""Converse with the MXNet community via the following channels: ""
 dev@mxnet.apache.org.
Note those email addresses need to be mailto links (I believe Markdown (on GitHub at least) converts automatically), not code snippets.",non_debt,-
spark,1807,comment,51397316,Looks good to me! Thanks!,non_debt,-
incubator-doris,764,review,268427081,"I will combine function call.
union_block and merge_block will be remained.",non_debt,-
hadoop,575,review,264257746,"""if the stream is not in state Open"" would be clearer.",documentation_debt,low_quality_documentation
spark,3962,comment,71411565,"in cluster mode, AMActor donot need to subscribe to disassociated event. because sometime driver has some errors, Now AMActor donot understand what happened in driver. so if AMActor subscribe to disassociated event and finish with FinalApplicationStatus.SUCCEEDED, that's incorrect to do so. @andrewor14",non_debt,-
trafficserver,4526,description,0,356066-4526 description-0,non_debt,-
cloudstack,4448,comment,724618330,@ravening I don't see the probably with requiring an internal DNS entry.  The operator can always enter the external DNS IP for it if they need to.  I can only see this causing pain elsewhere in the code.,design_debt,non-optimal_design
gobblin,741,review,54046406,"This seems to have the same problems as before. If this line fails, `current.jst` will either be a partially copied file (if line 247 is not executed, e.g., SLA kill or OOM) or gone.
There doesn't seem to be an easy way to guarantee that if the new `current.jst` isn't successfully generated, the previous `current.jst` must be restored. So I still think modifying `FsDatasetStateStore.getLatestDatasetState` is a better approach. I don't think it violates any contract, because `current.jst` is just a convenient way to find the latest `jst` file (so that we don't need to list all `jst` files and sort them). If somehow `current.jst` failed to be generated, I don't see a problem of using the latest `jst` file.",non_debt,-
spark,20619,description,0,"ParquetFileFormat leaks opened files in some cases. This PR prevents that by registering task completion listers first before initialization.
Manual. The following test case generates the same leakage.",design_debt,non-optimal_design
beam,592,comment,259489198,"Rebased, AutoValue use, etc. Not fully ready for review anyway.",non_debt,-
spark,3074,comment,62652752,@pwendell can you help take a look?,non_debt,-
helix,834,review,395332428,Done.,non_debt,-
kafka,2686,comment,286578364,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2176/
Test PASSed (JDK 7 and Scala 2.10).",non_debt,-
superset,11418,review,544788506,Can the type be more specific than `any`?,non_debt,-
thrift,1834,summary,0,Add pointer helpers for missing types,non_debt,-
spark,28708,review,442609316,sure,non_debt,-
spark,2907,review,19663363,Thanks for the much better solution. That works just fine.,design_debt,non-optimal_design
spark,15627,comment,259044378,"@kishorvpatil @tgravescs It seems this pr is breaking functionalities of `--files` or `--archives`.
Using `--files` or `--archives` with files which are not included to `--jars` doesn't work.",non_debt,-
guacamole-client,511,review,602658052,"Sure, makes sense. I've got the changes almost done - just need to clean up the unit tests.",test_debt,expensive_tests
airflow,11631,review,508137239,Can you use [get_docs_link](https://github.com/apache/airflow/blob/master/airflow/utils/docs.py) function to generate link to fixed version?,non_debt,-
spark,25310,review,309068923,how about adding an example like;,non_debt,-
tvm,4274,review,343909482,document all public fucntions,non_debt,-
beam,4953,comment,376330345,"Thanks, LGTM.
R: @chamikaramj for merge.",non_debt,-
couchdb,3336,summary,0,Add a .devcontainer configuration for 3.x,non_debt,-
hbase,774,review,340397832,Add a `<p/>`? Otherwise it will be reformatted by others easily.,non_debt,-
kafka,1284,comment,215544073,LGTM,non_debt,-
airflow,1721,comment,240172723,@jgao54 would it be possible for you to run this in Py2?,non_debt,-
arrow,6008,review,359538125,Schema should not be mandatory,non_debt,-
dubbo,5069,description,0,"Fix issue #5052. When generic invokes, the beginning `invocation` object (before any filters) will be used to be a callback method's parameter, such as like `onResponse`, `onError` of interface `org.apache.dubbo.rpc.Filter.Listener`. However, `invocation`'s function `getMethodName` will return value:`$invoke` when it is a generic invoke.
Thus, it will cause `endCount` method not counting the origin method status. Then the `executes` limit is not accurate enough.",non_debt,-
beam,8311,comment,487220917,Run Java PreCommit,non_debt,-
pulsar,4040,summary,0,Website docs for 2.3.1,non_debt,-
spark,14916,review,77205359,"@tgravescs 
What about  `SignalUtils.scala`
`log.error(""RECEIVED SIGNAL "" + sig)`
when we kill the app using yarn kill we get this:
_ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM_
can we use it to trigger cleanup?",non_debt,-
druid,2524,review,81665494,"Would prefer `List<KeyValueMap>` here, it's generally easier to work with.",code_debt,low_quality_code
trafficserver,877,comment,240897067,"Fwiw, it's important to point out the the intent of this plugin is to provide some statistics for future analysis, our known use case is to produce an update to the set of WKS for 7.0.0.",non_debt,-
kafka,4731,review,175421245,"@omkreddy Sorry, we should update `currentConfig` in `DynamicBrokerConfig.initialize`. Thanks for raising the JIRA. While it would have been good to have this in 1.1, I am thinking we don't need to make this a blocker since it only impacts the first time a dynamic config is added. We will need a restart the first time and thereafter it should work as expected. We could highlight this in the docs for 1.1. What do you think?",non_debt,-
arrow,9626,review,587325457,"Does this mean that conversion could give the wrong results (in addition to being leaky)? If so, can you add a test showcasing that? (I believe you need the different chunks to be unequal...).",design_debt,non-optimal_design
spark,15199,comment,249086274,"Thanks all, it should be fixed in master only, my mistake.",non_debt,-
spark,27873,review,390789651,"I don't know why we pick -20000 and 20000 as the boundaries, just to be safe to always test the boundary values.",non_debt,-
beam,7093,review,235414200,Done,non_debt,-
thrift,11,description,0,  https://issues.apache.org/jira/browse/THRIFT-556,non_debt,-
couchdb,1596,summary,0,Fix couch server race condition,non_debt,-
spark,18986,comment,323618879,"Yea, since this topic is important for some users, I mean we better move the doc into `./docs/` ( I feel novices dont seem to check the code documents).",architecture_debt,violation_of_modularity
spark,11901,description,0,"This PR for ticket SPARK-13019 is based on previous PR(https://github.com/apache/spark/pull/11108).
Since PR(https://github.com/apache/spark/pull/11108) is breaking scala-2.10 build, more work is needed to fix build errors.
What I did new in this PR is adding keyword argument for 'fractions':
`val approxSample = data.sampleByKey(withReplacement = false, fractions = fractions)`
`val exactSample = data.sampleByKeyExact(withReplacement = false, fractions = fractions)`
I reopened ticket on JIRA but sorry I don't know how to reopen a GitHub pull request, so I just submitting a new pull request.
Manual build testing on local machine, build based on scala-2.10.",non_debt,-
hbase,1869,review,438941753,Actually isRegionOnline() is waiting in a loop until this region's status become opened and that server is online.  So there is no bug as such right?  isRegionOnline() might return false iff the server is being stopped,non_debt,-
flink,8035,review,268481298,what is BatchCompatibleStreamTableSink?,non_debt,-
bookkeeper,1088,comment,364621361,@dlg99 any ideas on my comment?,non_debt,-
trafodion,1786,comment,458865276,Can one of the admins verify this patch?,non_debt,-
skywalking,4368,comment,586702568,"LGTM
After https://github.com/apache/skywalking/pull/4214 about es index number reduced. This is another optimization.",non_debt,-
spark,3165,comment,62288954,I'm actually happy to just drop this though if we can update the documentation in our wiki to suggest people use hub. @JoshRosen or @rxin would one of you guys be able to put a few lines in https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools with the process you use? I can then verify and if it's all good I can just drip this PR.,documentation_debt,outdated_documentation
storm,339,summary,0, STORM-586: TridentKafkaEmitter should catch updateOffsetException.,non_debt,-
pulsar,7590,summary,0,ISSUE 7415 fix sidebar v2.5.0,non_debt,-
spark,20795,review,173997911,Normalize `FunctionIdentifier` when looking up it too?,non_debt,-
arrow,2239,review,202104769,Is setting unfinished here needed?,non_debt,-
storm,901,summary,0,STORM-1200. Support collations of primary keys.,non_debt,-
spark,12146,review,59121792,Sorry. I still do not understand the reason of having this. What's wrong of having DROP VIEW in SQLContext?,non_debt,-
incubator-mxnet,18315,comment,630422383,"@mxnet-bot run ci [unix-cpu, centos-gpu]",non_debt,-
spark,9843,comment,160314300,"I understand the issue you're pointing out, but it hasn't been a practical problem, even with hundreds of thousands of terms. The inclusion of explicit zeros in the output SparseVectors has been a practical problem, which is what led me to submit this JIRA and PR.
This is my first contribution to Spark, and I'm trying to adhere to the contribution guidelines. The guidelines suggest that ""simple, targeted"" changes are more likely to be accepted than ""big bang"" changes. It sounds like you're telling me this PR won't be accepted without making an additional optimization which adds ~100 lines of code, requires additional tests, and fixes an issue that was already present in the code before my changes. Is that what you're saying?",non_debt,-
druid,67,review,2830111,AWESOME!,non_debt,-
spark,837,comment,44756501,"Can you add ""[SPARK-1495][SQL]"" to the PR title?",non_debt,-
airflow,6449,review,339362280,Totally agree. Would be nice if we the template changes depending on the files that changed.,non_debt,-
gobblin,1135,description,0,It incorrectly converts strings to numbers. @pcadabam can you review ?,non_debt,-
airflow,12685,comment,735219234,"This is the change we discussed some time ago and captured in #12261. The Production image during CI build is now built from wheels rather than directly from sources to reflect a ""real"" installation case. 
This change modifies the ""CI"" production image build to first:
1) Using 'pip download"" + constraints file it downloads all .wheel packages that are needed to install airflow with the chosen ""extras"" for the production image
2) Buillds all providers packages that are selected for the production
3) Builds airflow .whl package
4) Prepares the production image using those wheel files rather than PyPI.
This way this production image for development tests reflects the exact production ""content"" - all packages (including airflow) are installed, but at the same time we are using latest sources to build those packages, so the image can also be used in K8S tests because it is build from the current (PR) sources. 
After preparing the image I am also checking if all the providers are installed as expected",non_debt,-
cloudstack,2903,summary,0,Set http level to INFO as default,non_debt,-
iceberg,1128,review,443691512,Why is `name` needed? The `MessageType` has a name that can be used instead of passing it in separately.,code_debt,complex_code
flink,4461,summary,0,[FLINK-7350] [travis] Only execute japicmp in misc profile,non_debt,-
flink,1929,review,60927886,unrelated change,non_debt,-
cxf,272,summary,0,CXF-7354:¬†Use SLF4J markers to differenciate payload-logging,non_debt,-
spark,14082,comment,231715047,"Since both @shivaram and @felixcheung signed this off, I'm merging this to master and branch-2.0.
Thanks @keypointt for working on this and @shivaram and @felixcheung for the review!",non_debt,-
pulsar,1046,comment,356745986,@merlimat addressed comments.,non_debt,-
beam,12050,review,444838467,"ok, I added the requested test for bw compatibility
btw you can see the buggy behavior there (lost information whether baseFilename is file/directory)",non_debt,-
samza,813,summary,0,SAMZA-2004: Add ability to disable table metrics,non_debt,-
spark,10832,summary,0,"[Spark-12485][Rename ""dynamic allocation"" to ""elastic scaling""]",non_debt,-
tvm,5320,review,408990283,"Out of curiosity, why is it better to handle the fix here than in the `AnnotateTarget` pass? It seems that it would work if `AnnotateTarget` just didn't generate the extra `compiler_end` annotations.",non_debt,-
beam,2910,review,114929783,"Naming fun: shorten to `register` and all the overloads can be the same, though you could have `registerCoderFactory` and `registerCoder` separate, too.",non_debt,-
carbondata,810,comment,295074466,"Build Success with Spark 1.6.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/1690/",non_debt,-
ignite,8840,review,586505404,fixed,non_debt,-
nifi-minifi-cpp,788,review,428031207,Unnecessary semicolon at the end of the line.,code_debt,low_quality_code
flink,14832,description,0,Port #14591 to release-1.12,non_debt,-
reef,157,review,28922004,This looks like a pure formatting change.,non_debt,-
tinkerpop,912,description,0,"https://issues.apache.org/jira/browse/TINKERPOP-2023
Gremlin Server no longer supports automatically creating self-signed certificates.
Cluster client no longer trusts all certs by default as this is an insecure configuration. (TINKERPOP-2022)
To revert to the previous behavior and accept all certs, it must be explicitly configured.
Introduces JKS and PKCS12 support. JKS is the legacy Java Key Store. PKCS12 has better cross-platform support and is gaining in adoption. Be aware that JKS is the default on Java 8.  Java 9 and higher use PKCS12 as the default. Both Java keytool and OpenSSL tools can create, read, update PKCS12 files.
Other new features include specifying SSL protocols and cipher suites.
The packaged `*-secure.yaml` files now restrict the protocol to `TLSv1.2` by default.
The implication of all of the above changes means that the packaged `*-secure.yaml` files no longer ""just work"". Minimally, the server files must be configured with a key/cert.
PEM-based configurations are deprecated, to be removed in a future release.
`mvn clean install -DskipIntegrationTests=false  -pl :gremlin-server` passes all tests
VOTE +1",non_debt,-
pulsar,4325,review,286153741,I think here can add SchemaType.JSON,non_debt,-
nifi,1884,comment,307099100,"Looks You guys updated dockerfile to use 1.3.0 release... but it's not available yet at :
https://archive.apache.org/dist/nifi/",non_debt,-
incubator-mxnet,10311,review,189448683,gru,non_debt,-
spark,21370,review,194292067,"This PR also changed `__repr__`. Thus, we need to update the PR title and description. A better PR title should be like `Implement eager evaluation for DataFrame APIs in PySpark`",non_debt,-
flink,6330,review,203302518,yes they should be nullable,non_debt,-
gobblin,1862,description,0,"This PR makes various changes to improve how throttling server uses config library.
* Add a policies endpoint to throttling server to query the policy for a particular resource.
* Add expiring resources to broker so policies can be reloaded (to always get the newest one from config library).
* Refactor config library client to make loading of few configs more efficient.
* Refactor Hadoop fs config store to make it less confusing.",code_debt,low_quality_code
helix,478,review,325376537,"Let's pass the map only, Map<GlobalRebalancePreferenceKey, Integer>.
ClusterConfig is too much for the rebalancer.",code_debt,complex_code
systemds,843,summary,0,[MINOR] Fixes bug causing stats output to be cleared in JMLC,non_debt,-
flink,845,summary,0,[FLINK-2232] StormWordCountLocalITCase fails,non_debt,-
geode,3025,summary,0,Feature/geode 2113c - implement SSL over NIO for peer-to-peer communication,non_debt,-
kafka,3136,comment,303699786,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4335/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
beam,4096,review,150110429,Yes,non_debt,-
superset,2547,review,109751124,"[not that important] I'm surprised this doesn't cause an `eslint` error, was it complaining about not being a boolean? `!!this.props.database && this.props.database.allow_run_async` or `Boolean(this.props.database) && this.props.database.allow_run_async` could be alternatives.",non_debt,-
parquet-mr,456,review,170675141,Why is nullCounts checked for being null but minValues and maxValues being used without a similar check?,code_debt,low_quality_code
beam,12938,review,550847398,nitpick - there should be blank line above the comment like before,code_debt,low_quality_code
tajo,651,comment,124796163,"+1
Thanks @jihoonson and I found that it worked successfully. 
Ship it. :)",non_debt,-
trafficserver,3876,comment,399461203,Looks like some compilers is unhappy with this initialization :-/,non_debt,-
commons-lang,19,summary,0,Fix LANG-948,non_debt,-
druid,5418,review,179293634,"Removed `reportParseExceptions` here, it should always be true (always thrown, handling depends on config)",code_debt,low_quality_code
airflow,12383,review,524536973,"I don't think we really need this -- with multiple webserver worker processes any refresh would lead to confusing state (some on new list, some on old, and no way to tell which is which) unless we have some way to make _all_ workers perform it.",code_debt,complex_code
hbase,673,review,332390100,"OK, again, here we unwrap the optional by checking whether it is null and inside the method we checl it again...
Where does this Optional come from? If we could not change the root, let's keep it as is?",non_debt,-
kafka,8918,review,451281466,I think you missed this one.,non_debt,-
phoenix,665,review,363534291,It wasn't a complete push. I updated the commit. I think you are right about using isNullOrEmpty.,non_debt,-
incubator-mxnet,20015,comment,797712366,@samskalicky its already merged in v1.x,non_debt,-
incubator-mxnet,19987,summary,0,Forward-port #19972 to master.,non_debt,-
flink,760,review,31632977,Good catch :-),non_debt,-
hadoop,1033,comment,513190749,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
spark,7842,review,43212805,The curly braces are not needed. The same for the following `case` statement.,code_debt,complex_code
spark,26840,comment,564766377,Thanks all!!,non_debt,-
spark,18887,review,140699378,The test cases in `kvstore` are just unit test cases. We also need integration tests for ensuring they work as expected.,test_debt,lack_of_tests
spark,8785,review,48715367,"This call is effectively just cloning the properties, but we're already doing the clone inside of `.jdbc()` itself, so we don't need this.",code_debt,complex_code
spark,27431,comment,581204072,cc @hvanhovell,non_debt,-
carbondata,1884,comment,361573546,Can one of the admins verify this patch?,non_debt,-
spark,4018,comment,69773792,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/25472/
Test PASSed.",non_debt,-
spark,21200,review,185664585,"It's not okay catch and ignore all throwables. E.g. OOMs should NEVER be ignored as it leads absolutely unexpected situations.
At best, you can catch `NonFatal(ex)` and ignore those (only after logging as a warning). For other throwables, log as a warning, and rethrow.",code_debt,low_quality_code
carbondata,1299,comment,327213625,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/536/",non_debt,-
usergrid,265,summary,0,[USERGRID-613] Added fix mixed cases in two-dot-o.,non_debt,-
tvm,2477,comment,463320789,@tqchen Done.,non_debt,-
beam,12698,comment,686530763,Run Load Tests Python Combine Flink Streaming,non_debt,-
kafka,2929,review,113856373,remove. this import already exists.,build_debt,over-declared_dependencies
hive,767,description,0,"LockedDriverState is a nested class within Driver, while it is used outside of it as well, and it is complex enough to be a class on it's own. DriverState should be it's nested class, and transitions / locking should be facilitated by functions within it.",design_debt,non-optimal_design
spark,29093,comment,657913561,cc @dongjoon-hyun,non_debt,-
brooklyn-server,363,comment,298045628,More goddamn merge conflicts.  Any objections to commit-then-review on this one?,non_debt,-
nifi,2968,summary,0,NIFI-5456: AWS clients now work with private link endpoints (VPC),non_debt,-
spark,13584,review,77271690,"sure, I'll change it",non_debt,-
nifi,4753,review,561119292,"For consistency, `Zookeeper` should be replaced with `ZooKeeper` to match the official naming.",code_debt,low_quality_code
skywalking,4175,review,363077341,"Move this as the second one. 
The first line of this page includes 6 too, https://github.com/apache/skywalking/blob/master/docs/README.md",non_debt,-
spark,22375,review,217495874,"I see, so the example above passes in codegen off and fails with codegen on with this fix, while using `Map(3 -> 7, 6 -> -1)` passes codegen on and fails codegen off, am I right?
What I am thinking about (but I have not yet found a working implementation) is: since the problem arise when we say we expect `null` in a non-nullable datatype, can we add such a check? I mean, instead of pretending the expected value to be nullable, can't we add a check in case it is not nullable for being sure that it does not contain `null`? I think it would be better, because we would be able to distinguish a failure caused by a bad test, ie. a test written wrongly, from a UT failure caused by a bug in what we are testing. What do you think?",design_debt,non-optimal_design
flink,10254,comment,555794388,@flinkbot  run travis,non_debt,-
arrow,1062,comment,327658255,"Thanks @trxcllnt. It will take me a day or two to work my way through the diff and give comments. If @TheNeuralBit could also chime in about the direction for taking the JS codebase that would be great. 
We should have a mailing list discussion about creating a JS roadmap / laundry list of JIRAs so that we can work toward integration tests and other proof of compliance with the Arrow specification (e.g. having binary data files here is OK, but the ideal scenario would be to have an N x N integration test matrix, where N is the number of different Arrow implementations). Currently we only have a 2 x 2 matrix where C++ and Java test against each other (and with themselves)",non_debt,-
kafka,1664,review,77421658,Can you restructure this to use `val` - it helps to have a single block on the RHS that encapsulates the full assignment logic (as opposed to being exposed to the method's entire scope).,architecture_debt,violation_of_modularity
activemq-artemis,1486,comment,324314739,@dudaerich Thanks Erich.  This looks good.  I will merge.,non_debt,-
lucene-solr,2200,review,556698348,"Classloading won't help, because we still need a separate JVM. When JVM loads classes, it trys in parent classloader first. If the class is already there it won't load. So we need at least one separate process.
I don't like to try many times each with separate JVM. Maybe only try once (like in the other test with codecs). It may not fail every time, but sometimes test fails.
I am also not sure if we really need a test for this. If we may get a static checker that finds classes that initialize their subclasses in their own static initializer, we can prevent similar cars in future.",test_debt,flaky_test
arrow,6105,review,373346495,"How about detecting `GType` from `arrow::fs::FileSystem::type_name()` like we did `garrow_array_new_raw()`?
It's useful when we create a binding for a function that returns a generic `arrow::fs::FileSystem` such as `arrow::fs::FileSystemFromUri()`.",non_debt,-
cloudstack,4748,comment,790543546,@blueorangutan test,non_debt,-
kafka,9442,description,0,"Just as `grantingVoters()` and `rejectingVoters()`, `unrecordedVoters()` method be optimezed by using `votersInState(State)`",non_debt,-
incubator-pinot,1890,review,141491163,"It may be useful to set up SegmentIndexCreationDriverImpl (via config) so that it can log a message every (say N) rows. I understand this is not something you did, but maybe it is useful to add that information. You decide.",non_debt,-
kafka,3171,comment,305728085,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4782/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
hadoop,1587,comment,538112086,"Testing s3a ireland
-a full run of everything (kicking off another) with s3guard and ddb 
-this test suite with s3guard off, on and local. Verifying that without s3guard, the guarded versions of the tests are not executed",non_debt,-
infrastructure-puppet,1260,comment,445444406,+1 by humbedooh in hipchat,non_debt,-
tvm,6297,review,472353682,s/uint/size_t/,non_debt,-
spark,9867,review,46597229,"You should be able to have a single run with ""-Pkinesis-asl -Pyarn -Phive -Phive-thriftserver"" - I even think ""-Phive"" is unnecessary, I think it only affects packaging right now.
""-Phadoop2.2"" is unnecessary, that's the default.",code_debt,complex_code
incubator-weex,1437,review,210464939,"If line 277 crashes, the file will still be open.
         FileWriter x = null
         try{
                x= new FileWriter
          }finally{
                if(x != null) x.close()
          }",non_debt,-
beam,6733,comment,430804819,"R: @rohdesamuel 
CC: @melap",non_debt,-
flink,8721,comment,502049333,"Hi @aljoscha 
I have rebased master and addressed comments.",non_debt,-
bookkeeper,22,comment,196151999,+1 good catch.,non_debt,-
spark,25651,comment,534813380,"@rdblue yea it's possible. In this PR, I try to adopt your suggestion to make it clear that `TableProvider.getTable` should take all the table metadata, so the method signature becomes
`TableProvider` has another `getTable` method which needs to infer schema/partitioning, and previously the method signature was
To make it consistent, I change it to use `properties: Map[String, String]`, also rename it to `loadTable` since we need to touch many files anyway.
We can still keep the old method signature with a TODO to change it later, so that this PR can be much smaller.",design_debt,non-optimal_design
incubator-mxnet,9111,description,0,"added SELU and ELU activation functions as recommended on this thread https://github.com/apache/incubator-mxnet/issues/8422
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here",non_debt,-
superset,8037,description,0,"Choose one
In some cases, a result existed in the results backend but a query with the corresponding `results_key` did not exist in the queries table. This used to throw a 500 because `one()` expected a query with that `results_key` to exist.
Now, it will return a generic error if no query exists. 
As a possible future fix for this error, we could use the information in the query object within the `blob` instead of looking up the corresponding query.
Tested on devbox. Confirmed that a key now returns the desired error instead of 500:
@etr2460 @john-bodley",non_debt,-
flink,8136,description,0,"*This work is a preparation for FLINK-11726.*
*In `SingleInputGate#create`, we could remove unused parameter `ExecutionAttemptID`.
And for the constructor of `SingleInputGate`, we could remove unused parameter `TaskIOMetricGroup`.
Then we introduce `createSingleInputGate` for reusing the process of creating `SingleInputGate` in related tests.*
This change is a trivial rework / code cleanup without any test coverage.",code_debt,dead_code
carbondata,3136,comment,468114436,"@tianyouyangying +1 for @qiuchenjian 's suggestion.
1. Please change all chinese to english.
2. Please change PR title to '[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to avoid ambiguity'
@qiuchenjian The origin SQL in doc actually are two statements, we should seperate them to avoid ambiguity.
Thanks.",documentation_debt,low_quality_documentation
kafka,5068,review,191512386,Do we actually want this to be `Long.MAX_VALUE`? Seems error prone since the normal thing to do with the TTL is add it to the current time and that will cause overflow. Should we have some sentinel value for infinite TTL?,code_debt,low_quality_code
airflow,442,comment,323134581,"Hi, sorry this is old but I can't see on master that this is changed: [link](https://github.com/apache/incubator-airflow/blob/3b589a9f73bed018bf7e2c7b7265bfce5da91ca0/airflow/hooks/mysql_hook.py ).
Is there any plans to support `mysqlclient` or another python3 friendly driver?
Thanks!",non_debt,-
openwhisk-wskdeploy,103,description,0,"This PR is for code review, **this should not be merged yet.** It still needs to be debugged and the actual deployment of entities is not completed yet.  Also, it **may be missing some files** since I tore everything down and put it back together.  I'll add back the missing files (like report.go and version.go).  Sorry about that :-)
The goal here is to refactor the code to modularize it better so we can add ""big"" features.  
The major refactors are:
- refactor utils.go into separate classes
- refactor manifest and deployment functions into parsers and readers
- refactor classes into packages that are more descriptive of their functions, e.g. parsers, utils, deployers
The features added in this code:
- support for multiple packages in the service deployer.  This is mainly targeted towards multiple packages in the deployment.yaml, not the manifest.yaml
- add parameter and annotation binding from the deployment file into the deployment plan.
- add support for sequence notation per use case` openstack.`
- add placeholder for dependency specification in a package.",architecture_debt,violation_of_modularity
spark,25856,review,338886220,"The following checkValue already disallows 0, right?",non_debt,-
spark,29403,review,496341174,"Also, if I'm correct, it would be good to include a test case for this case.  What happens if you actually do operations on the null value stored as an enum?  Is the nullability of the resulting schema correct. Do operations like `.isNotNull` work correctly?",test_debt,lack_of_tests
incubator-heron,2071,description,0,43158694-2071 description-0,non_debt,-
spark,22197,review,212812755,nit: We can have just `exception when duplicate fields in case-insensitive mode` as test title. Original one is too verbose.,code_debt,low_quality_code
arrow,7591,comment,651837534,@kszucs may you please have a look at this when you get a chance. There's a change to the prepare-test Ruby script,non_debt,-
spark,23929,comment,470663982,Merging to master.,non_debt,-
spark,22977,comment,437619916,I think also there is a hive metastore test that downloads spark release jar?,non_debt,-
tvm,1680,review,214412218,No need of the semicolon. Same comment applies to other similar lines in Python,non_debt,-
pulsar,5578,review,343518665,yes will do it in a separate pull request.,non_debt,-
trafficserver,1675,comment,318111169,"This PR seems a little confused. It seems like a legitimate bug, but this is not set up as a backport of an upstream fix. I'd like to see that fixed so I can merge, but I cannot merge this as is. I am going to close it, but please reopen it if it can be fixed, or open a new one as a proper backport of an upstream fix.",non_debt,-
commons-lang,47,review,27015336,I added these in Javadoc too.,non_debt,-
spark,29909,review,497951985,"Sorry, since always work at `SQL`  module",non_debt,-
spark,17286,comment,287317227,retest this please,non_debt,-
arrow,4262,review,281675439,Not sure why you had to change this header file?,non_debt,-
airflow,8575,comment,806276124,"Hi @turbaszek @mik-laj, any updates on this?",non_debt,-
camel,1557,summary,0,"CAMEL-11048 Jetty Producer always uses ""Transfer-Encoding: chunked"" header",non_debt,-
carbondata,4051,review,544069025,Clean files for MV is not supported?,non_debt,-
usergrid,493,comment,199080050,14135471-493 comment-199080050,non_debt,-
incubator-heron,2256,summary,0,Support components name generated by SummingBird.,non_debt,-
spark,6828,comment,112662507,"There are 2 Struct UDFs in Hive:
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStruct.java (struct)
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNamedStruct.java (named_struct)
For the previous one, we will give the default names for its fields, but for the later, we will create the struct type by given names. We need to implement both of them, but this PR seems only for solve the bug of the former one.",non_debt,-
incubator-mxnet,13362,review,239658900,"can we also output the layout : 
""Need CuDNN for layout support"" << param_.layout.value()",non_debt,-
flink,13300,review,511757686,Revert changes on this file? I think one indent is correct.,non_debt,-
hadoop,2051,comment,638901440,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
reef,761,review,49510586,"The other members, too?",non_debt,-
incubator-mxnet,7941,summary,0,DO NOT MERGE: Just a test of adding properties to JenkinsFile,non_debt,-
beam,573,description,0,"quickly and easily:
  `[BEAM-<Jira issue #>] Description of pull request`
     Travis-CI on your fork and ensure the whole test matrix passes).
     number, if there is one.
     [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---
Some of the timestamps were not adjusted when [BEAM-145](https://issues.apache.org/jira/browse/BEAM-145) was fixed to respect the `WindowFn`'s timestamps.",non_debt,-
lucene-solr,772,review,302183057,Is the term guaranteed to be a UTF8-encoded string?,non_debt,-
incubator-mxnet,12768,comment,428218284,"See my comment in the issue, were you able to reproduce? Do you have some evidence to declare it as a flaky test?  Unless there's more data I would be against disabling this test with the information that we have so far to prevent lowering the quality bar.",test_debt,flaky_test
kafka,5131,comment,395010640,"@ijuma I think `KafkaFuture` can implement `CompletionStage`. 
Don't mind below comment since I figured out we don't need any additional execution facility as we already have a CompleteableFuture that we can use.
--------------------------------------------------------------------------------------------
There are CompletionStage.*Async(...) methods that do not accept an executor but, as their documentation says, 
So this means we need to supply that ""execution facility"" maybe in the form of 
 * using ForkJoinPool.commonPool()
 * or firing up a new background thread for each task (which is what CompletableFuture does in some circumstances) 
A full blown ThreadPoolExecutor in clients, I think, would be an overkill",non_debt,-
hive,2083,review,605381114,Done,non_debt,-
airflow,2553,review,139544947,"Fixed, I'll pushed that as soon as I find how to do lazy logging in Python ^^ (it was definitely not a long thing to fix...^^ )",non_debt,-
camel,1585,description,0,"‚Ä¶rcept/enrich logged messages
this is continued from https://github.com/apache/camel/pull/1559
Hi @davsclaus , @objectiser , 
Here is an outline sketch for the CAMEL-11054. Let's start from here:
* adds `addCamelLoggerListener()`/`getCamelLoggerListeners()` on `CamelContext`
* `CamelLogger` holds a list of `CamelLoggerListener`
* for Log EIP, `LogDefinition` copies `CamelLoggerListener`s from `CamelContext` to `CamelLogger`
* for Log Component, `LogEndpoint` copies `CamelLoggerListener`s from `CamelContext` to `CamelLogger`
* `CamelLoggerListener#onLog()` to receive a log event and return an enriched log message
Questions:
* do we want to extract `enrich()` from `onLog()` so it could be done separately?
* do we want separated handler like `onWarn()`, `onInfo()` and etc. rather than `onLog()`?
* any other event we want to handle in `CamelLoggerListener`?
* As both of Log EIP and Component use `CamelLogger`, it's easy to add it on both of them. Is it OK or do we want it only on Log EIP?
Any comment would be appreciated!",non_debt,-
trafficserver,3746,comment,394279910,Cherry-picked to 8.0.x,non_debt,-
flink,675,summary,0,[FLINK-2008] Fix broker failure test case,non_debt,-
spark,4050,comment,71347965,"If we use a inputFormat that don‚Äòt instanc of org.apache.hadoop.mapreduce.lib.input.{CombineFileSplit, FileSplit},  then we can't get information of  input metrics.",non_debt,-
pulsar,2241,description,0,"In #2237, `administration-auth` is removed. However it is not removed from sidebard. It is causing failures on building website.
Remove `administration-auth` from the sidebar. Also remove codebase page, which doesn't make sense to be there because code changes are happening very frequently. The page quickly becomes out-of-dated.
Additionally cleanup a few things in the sidebar.",code_debt,dead_code
superset,8172,review,322853272,could you attach a gif to show the behavior?,non_debt,-
trafficserver,646,comment,220368272,"I think we should squash these three commits to one, right?",non_debt,-
spark,15230,review,83146530,let's explain more that hive metastore will treat the `EXTERNAL` property as a signal to change table type.,non_debt,-
skywalking,4546,review,396049971,45721011-4546 review-396049971,non_debt,-
beam,4332,comment,356359562,"@chamikaramj thank you for your suggestions.  @jbonofre @iemejia could you also take a look? I also added io-it-suite-local profile that was missing and jenkins job definition.
I added only the reshuffle and it seems to be a little bit helpful. I didn't optimise it further due to a  problem: different ""consolidatedHashes"" get calculated for each test run for datasets bigger than 600 000 rows. This makes it unable to determine hash for a large scale dataset (eg. 40 000 000 rows). The amount of read and written rows is the same. I also have the same problems while running JdbcIOIT on larger datasets. Also, as I checked, the database content seems to be all right. I can create a JIRA for that after you review this PR and agree that this behavior is odd, ok?
600 000 is approx. 160 MB. I wouldn't call that a large scale test but I think it is something we can start with and then increase the scale and optimize it gradually if needed and if possible (e.g. after tackling the hash calculaction problem i described). What do you think?",non_debt,-
calcite,2381,description,0,WIP Do not review,non_debt,-
kafka,6363,review,280294974,"It'd be really great to have unit tests for many of these methods. The `performTaskAssignment(...)` method is already pretty lengthy, and there are just a few unit tests whereas there seem to be lots of permutations and branches. Not only would they help with confidence, but they'd help with regression testing if/when we have to get back into this code.",test_debt,lack_of_tests
beam,3988,summary,0,"[BEAM-3052] ReduceFnRunner: Do not manage EOW hold or timer, set GC hold and timer always",non_debt,-
parquet-mr,154,comment,111540874,"@lunchev, I've not had a chance to get to the review backlog in a little while. Sorry about that! I'll try to get this reviewed in the next couple of weeks.",non_debt,-
carbondata,3085,comment,455942906,"Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/2584/",non_debt,-
drill,1897,comment,554203211,"@vvysotskyi, thanks much for the review and for committing the code. Thanks also for showing the disassembly of the generated code for the for loops. You analysis is convincing and I'll change the loops in the next PR.",non_debt,-
spark,12259,review,60783900,`val labels = vectorRDD.select('label).as[Double].collect()`,non_debt,-
spark,22878,review,241273157,nit: use the api using `jsonFormatSchema`,code_debt,low_quality_code
attic-apex-malhar,551,review,105841722,"This is the fix for the bug which is in IncrementalCheckPointManager. (Comparing key bucket id with time bucket).
Discussion about exclude expired time buckets while saving data happened in the below PR:
https://github.com/apache/apex-malhar/pull/516",non_debt,-
hadoop,1900,comment,601190652,"Thanx @vinayakumarb  for checking. Seems comment isn't coming, some problem, will check. it isn't coming for patches too.
Anyway the build seems cleans.",documentation_debt,outdated_documentation
kafka,6363,review,278752888,This is getting a bit confusing -- this seems the same as `currentWorkerAssignment`. What mutates that causes this to be different?,code_debt,low_quality_code
cloudstack,4250,comment,679976928,@blueorangutan package,non_debt,-
spark,30212,comment,809028142,Gentle ping @cloud-fan,non_debt,-
nifi,4759,description,0,"Thank you for submitting a contribution to Apache NiFi.
_Enables X functionality; fixes bug NIFI-YYYY._
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
hadoop,885,review,289591884,whitespace:tabs in line,code_debt,low_quality_code
attic-apex-malhar,88,description,0,‚Ä¶Connected method.,non_debt,-
incubator-pinot,976,review,105290266,fixed.,non_debt,-
spark,22646,review,223168936,This exception message looks a bit confusing. We can say the given type is not supported and we only support the certain type (`java.util.List` and `java.util.Map`).,code_debt,low_quality_code
spark,10252,review,47457824,I guess it's changed by auto format.. I will remove it.,non_debt,-
rocketmq,1682,description,0,"1 nameserver, 1 broker, 1 producer
producer send messages to a topic, when broker is down, producer still send heartbeat to broker
because 
org.apache.rocketmq.client.impl.factory.MQClientInstance#cleanOfflineBroker
does not work",non_debt,-
incubator-heron,1431,description,0,"Implementation of IScalable scheduler interface.
Fixes #1430",non_debt,-
tvm,6039,comment,660275132,@icemelon9 Can you please manage this PR?,non_debt,-
spark,2761,comment,58776141,"I also think, it's difficult to apply new style checker only to new codes. I cleaned up codes in origin/master for the style checker suggested in this PR.  So, if this PR is merged, then we can enforce the new style to developers and all developers have to do is to check the style of the code changed by them.",code_debt,low_quality_code
spark,1241,review,16760326,"this no longer supports file consolidation, does it?",non_debt,-
shardingsphere,4729,description,0,For #4525.,non_debt,-
spark,20265,summary,0,[SPARK-21783][SQL] Turn on ORC filter push-down by default,non_debt,-
spark,15432,comment,258138244,I will rebase and clean up this tomorrow.,non_debt,-
trafficserver,1209,comment,282382019,clang-analyzer build *successful*! See https://ci.trafficserver.apache.org/job/clang-analyzer-github/177/ for details.,non_debt,-
skywalking,3956,description,0,45721011-3956 description-0,non_debt,-
incubator-mxnet,8034,review,140924608,"Sure, I can make it a part of Sanity Test since we plan to have that stage whether it is a full build or smoke test. 
@gautamkmr @sandeep-krishnamurthy any opinions?",non_debt,-
spark,9229,review,56207992,Not necessary to have `@param` and `@return`.,non_debt,-
trafodion,1645,comment,404802438,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2820/,non_debt,-
fineract,1384,comment,706692687,"@vorburger darrrn... I had a feeling that I forgot something. Thanks for pointer, will do.",non_debt,-
drill,1606,review,247444833,This is already calculated in the HashJoinBatch.partitionNumTuning() method. You should just use the partitionCount computed from there. You can get the number of partitions from the PartitionStatSet,non_debt,-
zeppelin,60,comment,101208857,@1teed Thanks for the contribution. Tested with a centos-zeppeling docker image and works fine! Looking forward to the merge!,non_debt,-
pulsar,1341,comment,370602899,retest this please,non_debt,-
flink,4611,comment,325389996,"Changes LGTM @zentol ! 
When Travis gives a green light, feel free to merge!",non_debt,-
druid,3788,comment,268051850,"@erikdubbelboer ok that makes sense.
Can you please:
1. Add a comment to the code as to why this does not use `_`
2. Add unit tests to verify functionality.",test_debt,lack_of_tests
orc,448,description,0,"Bumps [netty-all](https://github.com/netty/netty) from 4.1.17.Final to 4.1.42.Final.
- Additional commits viewable in [compare view](https://github.com/netty/netty/compare/netty-4.1.17.Final...netty-4.1.42.Final)
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/orc/network/alerts).",non_debt,-
hbase,2454,comment,702578836,"We have two blocking calls in `NormalizerWorker` thread:
1. While retrieving new table for normalizing from Queue
2. RateLimiter blocking threads trying to submit plans",non_debt,-
servicecomb-java-chassis,514,review,162260357,"final, pls",non_debt,-
druid,2926,review,62581331,@drcrallen if added why this is false ?,non_debt,-
spark,364,comment,40168409,"Test failure was due to a random behavior in RDDSuite, which is fixed in https://github.com/apache/spark/pull/387 .",non_debt,-
hawq,392,review,54617226,indentation is off the hook,code_debt,low_quality_code
spark,28258,review,418488518,Updated the log messages.,non_debt,-
zeppelin,1875,comment,272051284,"@felixcheung 
I think it's ok since all LICENSEs will be merged into a single file
- https://github.com/apache/zeppelin/blob/master/dev/create_release.sh#L93-#L96",non_debt,-
helix,1663,review,588774064,"This test should probably be named as TestZkClientAsyncFailureMetric since your new code is only adding metrics. The aync function should be already covered in existing tests. 
With that said, have you checked whether it's possible to only add metrics testing to existing tests by mocking the results? That would save us one additional test file. If there is no available code path or too many efforts involved, I'm fine with having a separate test too. But please check the zkclient and monitor test.",code_debt,low_quality_code
spark,26261,comment,546601038,"jenkins, retest this, please",non_debt,-
tvm,7314,description,0,"This PR improves the implementation of GPU `argwhere` added in https://github.com/apache/tvm/pull/6868, using exclusive scan (see https://github.com/apache/tvm/pull/7303). 
The current implementation of `argwhere` is very inefficient, because it uses atomic to update the write location. Since all threads compete for the single location, this effectively makes it a sequential kernel. Moreover, since the output indices need to be lexicographically sorted, the current implementation involves sorting along each axis.
Since `argwhere` is literally an instance of stream compaction, this is a perfect application of exclusive scan. Now, `argwhere` simply consists of 
* A single call to exclusive scan on a boolean flag array to compute the write indices.
* Compaction using the write indices (just copying elements with nonzero condition).
both of which are highly parallel operation. Thus, both atomic and sort are gone, vastly simplifying the implementation. Moreover, it also brings huge speed up, as shown below.
All numbers in milli sec
please review @zhiics @Laurawly @mbrookhart @tkonolige @anijain2305 @trevor-m",code_debt,slow_algorithm
airflow,7007,comment,570477304,"I agree the 2nd option sounds lot better, I have changed the PR and updated all the example DAGs by importing just the function from module instead of __init__.py.",non_debt,-
incubator-mxnet,20004,summary,0,Remove USE_MKL_IF_AVAILABLE flag,non_debt,-
samza,235,review,124903942,"It will be cleaner to simply pass the metrics registry associated with this component and register more granular group of metrics under `ZkUtilsMetrics`. Overloading it with `ZkJobCoordinatorMetrics` is confusing as this component - ZkUtils is also accessed from CoordinationService. 
HTH. Thanks!",design_debt,non-optimal_design
kylin,140,comment,392261013,CI has passed,non_debt,-
superset,11125,description,0,"‚Ä¶le (#10981)""
This reverts commit e93d92e8ac6d4f85d193943e27d585fb6e01568c.
See linked issue
-CI, manual verification",non_debt,-
calcite,1825,review,383130171,No need to use ```LinkedHashMultimap``` because the key should be unique.,code_debt,low_quality_code
carbondata,1789,comment,357279619,"Build Success with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/1503/",non_debt,-
incubator-mxnet,15511,summary,0,Numpy Identity operator,non_debt,-
karaf,24,description,0,"Mojos use available Aether subsystem (Sonatype or Eclipse) by looking at what's
available in PlexusContainer.
It's easy to add Maven deps for both Sonatype and Eclipse versions of Aether, but it's not that easy to depend on both `maven-core:3.0.3` and `maven-core:3.1.0+`.
Mojos don't get particular Aether component autowired directly by type - they can however get autowired entire `PlexusContainer`. We then look at the available Aether subsystems and the use a `DependencyHelper` layer to access correct Aether (both Sonatype and Eclipse).
`karaf-maven-plugin` may use `maven-core` API directly for Sonatype version of Aether (for dependency on `maven-core:3.0.3`), but it also can use Eclipse Aether and the methods from Maven API which reference Eclipse Aether when used with Maven 3.1.0+ - it however requires a bit of `java.lang.reflect`.",non_debt,-
trafficserver,2304,review,129945358,Added.,non_debt,-
druid,4223,description,0,"...so that core extensions are not wiped out
currently that profile is unusable as it bundles contrib extensions only whereas expected behavior is to have both core+contrib extensions bundled",non_debt,-
beam,5422,comment,391486077,Oups I screwed the correct title by double clicking merge by mistake. Thanks @timrobertson100 and @rangadi for the proper review.,non_debt,-
camel,1114,comment,239095564,Thanks for the PR it has been merged. Do you mind closing this?,non_debt,-
karaf,728,comment,456086960,Sounds good. Let me know if I can help. The `cxf-specs` feature should be taking care of exporting these packages in the container when they're not provided by the container itself.,non_debt,-
spark,5736,comment,97282994,Thanks to @andrewor14 and @vanzin's comments,non_debt,-
spark,23271,comment,445728476,ok to test,non_debt,-
beam,12762,review,489745304,"Ah, just remembered that I checked in helpers in both python and java a python/apache_beam/io/gcp/bigquery_io_metadata.py 
You may want to move the santize function to those files.
_is_valid_cloud_label_value",non_debt,-
arrow,1180,summary,0,ARROW-1541: [C++] Fix race conditions in arrow_gpu with generated Flatbuffers files. Do not put generated files in source tree,non_debt,-
tvm,7559,review,585981227,addressed in #7570,non_debt,-
kafka,2832,comment,293337249,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2885/
Test FAILed (JDK 8 and Scala 2.12).",non_debt,-
trafodion,646,comment,239472021,+1 Change looks good. @prashanth-vasudev should we merge now or do you intend to make changes to address comments above? Those comments could be addressed later if you wish.,non_debt,-
reef,1178,review,87037022,Sounds about right :),non_debt,-
openwhisk,4782,review,361962838,Good idea! Will try to implement it.,non_debt,-
camel,4496,description,0,"#4490 does not cover all the cases. While performing end-to-end tests with this snapshot, I found that traces were still not aggregating correctly. This PR fixes the problem.
I am not sure why the tests did not catch this issue, so unfortunately cannot provide better automatic coverage.
Below are the contribution guidelines:
https://github.com/apache/camel/blob/master/CONTRIBUTING.md",test_debt,low_coverage
carbondata,2207,comment,394710103,"SDV Build Success , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/5228/",non_debt,-
carbondata,3382,comment,530653898,"Build Success with Spark 2.1.0, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.1/471/",non_debt,-
spark,7304,comment,119761734,Merged build started.,non_debt,-
incubator-pinot,6009,review,487584167,19961085-6009 review-487584167,non_debt,-
airflow,10380,comment,679216493,"There are integrations available:
- https://drawio-app.com/create-mermaid-diagrams-in-draw-io/ 
- https://github.com/nopeslide/drawio_mermaid_plugin
Or we could just have the png and steps to reproduce like copy/paste text to https://mermaid-js.github.io/mermaid-live-editor/",non_debt,-
incubator-mxnet,13654,review,241911517,"Can you add comment saying why we have excluded these test cases?
Do we have custom tests for these somewhere?",test_debt,lack_of_tests
pulsar,9064,comment,752293183,/pulsarbot run-failure-checks,non_debt,-
druid,1306,review,28990774,fixed,non_debt,-
flink,2093,review,66788457,"Then we can add a `LOG`, or maybe we should even throw an exception. What do you think?",non_debt,-
cloudstack,1021,comment,152996985,"registerUserKeys log based on this change
2015-11-02 17:15:19,061 INFO  [a.c.c.a.ApiServer](2001334745@qtp-678426242-0:ctx-608cad64 ctx-6e00b6fc) (logid:bfb12e9d) (userId=2 accountId=2 sessionId=9g6dq5sjxlk7137t68hfkux8) 0:0:0:0:0:0:0:1 -- GET command=registerUserKeys&response=json&id=e2fdf8e4-73be-11e5-8882-249684d59f9c&_=1446464718702 200 {""registeruserkeysresponse"":{""userkeys"":{}}}",non_debt,-
incubator-brooklyn,525,comment,75642639,@richardcloudsoft @grkvlt can you take a look at this please?,non_debt,-
orc,25,description,0,35144191-25 description-0,non_debt,-
gobblin,3140,review,512079641,Seems unrelated to the change proposed in this PR. Can we remove this?,non_debt,-
spark,19763,review,152914613,"After rethinking about this, I think it is better to indicate this threshold also determines the number of threads in parallelism. So it should not be set to zero or negative number.",code_debt,low_quality_code
parquet-mr,341,review,60160998,maybe make these values relative to MAX_SLAB_SIZE so that if it's ever changed these tests remain valid,non_debt,-
airflow,3806,description,0,"  - https://issues.apache.org/jira/browse/AIRFLOW-2956
 Adds ability to specify kubernetes tolerations for dags using kubernetes pod operator
  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.",non_debt,-
jmeter,337,description,0,"Fixed bug whereby calling `registerError` with the following data set `[""A"", ""B"", ""C"", ""D"", ""E"", ""F""]` would return `[[""A"", 1], [null, null], [null, null], [null, null], [null, null]]` instead of `[[""A"", 1], [""B"", 1], [""C"", 1], [""D"", 1], [""E"", 1]]`.
Improved JavaDoc for `registerError`
Also removed JavaDoc which did not add anything to the method names.
Made the code more readable and at the same time fixed a subtle error.
On my spock branch:
- Bug fix (non-breaking change which fixes an issue)
[style-guide]: https://wiki.apache.org/jmeter/CodeStyleGuidelines",code_debt,low_quality_code
pulsar,5043,comment,535733584,run java8 tests,non_debt,-
spark,27845,comment,596042225,Can one of the admins verify this patch?,non_debt,-
druid,222,review,5907138,what if it's null?,non_debt,-
incubator-mxnet,18688,comment,656859038,"I ran into this issue developing a PR I have yet to submit.  The discrepancy in model outputs is caused by the fact that when cudnn calculates the running variance, it uses the 'sample variance', while this test is comparing in all cases to the 'population variance'.  The difference is that the sample variance uses a factor of N-1 in the denominator, while the population variance uses N (where N is the number of elements in the sample).
My upcoming PR will include a fix for this, and after it's merged, if you want you could revert this commit that changed the problem sizes, since that is not the real issue here.",non_debt,-
beam,10835,comment,587648002,Run Python Dataflow ValidatesRunner,non_debt,-
carbondata,1810,review,162288811,ok,non_debt,-
superset,6719,comment,458423396,Handing it over to @betodealmeida who has a better sense of what should be in Superset vs in the dbapi driver and/or SQLAlchemy dialect,non_debt,-
spark,3670,review,23883301,Tested this in isolation and it does work for directories.  A new test that I added should exercise this path.,non_debt,-
hbase,1627,review,418984035,Is this really needed? Isn't BULK_LOAD_HFILES_BY_FAMILY loaded as false by default if not set on LoadIncrementalHFiles#195? Or perhaps you can move this for a separate method that could be overridden by  TestLoadIncrementalHFilesByFamily to avoid duplicating whole setUpBeforeClass in the child class?,code_debt,complex_code
spark,18797,description,0,"Update breeze to 0.13.1 for an emergency bugfix in strong wolfe line search
https://github.com/scalanlp/breeze/pull/651
N/A",non_debt,-
incubator-mxnet,15161,review,292706143,Are you using any functions in this library? if not please get rid of this line.,build_debt,over-declared_dependencies
airflow,7516,review,383040768,"I want to use this decorator also to prevent regression in scheduler performance. Some methods are critical and I have optimized it to use very few queries, but it can be easily broken.  This context manager will allow us to detect a regression regarding it.",design_debt,non-optimal_design
druid,1374,comment,123751832,"@fjy sure, I wanted to make the PR reviewable and write docs while review progressed.",non_debt,-
accumulo,1000,review,262118131,I tried to fix this but the formatter put it back this way,non_debt,-
tajo,454,comment,96620163,"@sirpkt thanks for updating your patch. 
I've tested the following query and found that the result of Tajo is different from that of pgsql.
**Tajo**
**PostgreSQL**
As you can see, some values are null in Tajo.",non_debt,-
beam,11145,comment,599899591,Run Python PreCommit,non_debt,-
systemds,131,summary,0,[SYSTEMML-540] [SYSTEMML-445] Initial implementation of conv2d/maxpooling builtin functions and GPU backend,non_debt,-
incubator-mxnet,17808,comment,605590914,"We can get rid of WIN_GPU_MKLDNN tests altogether but that still leaves us with the flakiness of WIN_GPU as can be seen in these builds
For roughly same code of this PR & same windows AMI, below are the results so far
WIN_GPU | WIN_GPU_MKLDNN | Build Number | Link
-- | -- | -- | -- |
‚úñÔ∏é| ‚úîÔ∏é|15 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/15/pipeline
‚úñÔ∏é| ‚úîÔ∏é| 14 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/14/pipeline
‚úîÔ∏é|‚úîÔ∏é| 12 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/12/pipeline
‚úîÔ∏é| ‚úñÔ∏é| 13 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/13/pipeline
Ofcourse your tests on local have a different story to tell...",test_debt,flaky_test
incubator-pinot,5700,review,459633576,"Unused, please remove",code_debt,dead_code
accumulo,1330,review,316892853,"Regarding this naming convention, should these be `*.bind.address` or `*.address.bind`? What do you think? `master.bind.address` reads more naturally, but `master.address.bind` supports logically grouping address-related configs in a configuration hierarchy, which can be better for parsing, and more easily incorporate future changes.
An example of a future change which could benefit from the `*.address.bind` form could include work that address an explicit public advertisement address, in the case of the bind address not being publicly reachable (in the case of more complicated networking setups, such as those on some cloud services' infrastructure):
Example:
I'm leaning towards the `<service>.address.<addressType>` naming convention, rather than the `<service>.<addressType>.address` convention that you currently have here, but am open to discussion... because naming is hard. What do you think?",code_debt,low_quality_code
spark,13737,comment,229593343,"@rxin try running `./python/run-tests` locally. It fails for me. Even if `./dev/run-tests` _might_ pass because of packaging/timing, I think having `./python/run-tests` fail in a release is not acceptable.",non_debt,-
kafka,2079,description,0,2211243-2079 description-0,non_debt,-
incubator-mxnet,19505,summary,0,Enable coverage reports for PR diffs,non_debt,-
hadoop,1143,comment,514249120,"Thanks @ChenSammi for working on this. The change just enables the read path with topology awareness feature enabled. Can we just close this issue as resolved with HDDS-1713 and open up a new jira to just enable the read with topology awareness feature on?
The actual change does not fix the problem as described. What do you think?",non_debt,-
spark,29843,comment,727915423,"To be ruthless about even my own code
* 3.2.2. is lowest risk
* 3.3.3 has a lot of s3 and abfs changes, but there's enough changes elsewhere to make it more traumatic
* And a 3.3.1 is really needed/due for the stabilisation there.
It would probably be safest to build/release with a 3.2.x but allow 3.3.x to be built with if someone really wanted it.",non_debt,-
commons-lang,182,comment,298031716,Thanks!,non_debt,-
camel-quarkus,878,review,391477642,Could you please switch the test to work with `classpath:` URI so that the it is runnable outside Camel Quarkus source tree without any additional setup?,non_debt,-
beam,3219,summary,0,Update maven-dependency-plugin to version 3.0.1,non_debt,-
incubator-mxnet,7983,description,0,Changed Discuss URL to - http://discuss.mxnet.io,non_debt,-
nifi-minifi-cpp,481,description,0,"Thank you for submitting a contribution to Apache NiFi - MiNiFi C++.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
beam,9098,review,304868969,"I'm not sure if this check is worth having a separate class, especially as we move to more modular IO that does not necessarily have to be at the root.",non_debt,-
hadoop,931,comment,508137612,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
thrift,1219,description,0,"The original fix only took care of the generated file for the service, if the service has a method that uses a structure from an included thrift file, use clauses were emitted.
The issue was re-opened because if a structure in one thrift file contains a field for a structure defined in another thrift file, the perl generated code doesn't work properly as described in the defect since these live in Types.pm, and it needs the same use clauses emitted.",non_debt,-
arrow,3574,summary,0,ARROW-4337: [C#] Implemented Fluent API for building arrays and record batches,non_debt,-
iceberg,2294,review,596510752,nit: also `equals` here,code_debt,low_quality_code
spark,7769,comment,126424306,@mengxr @jkbradley test this please,non_debt,-
incubator-mxnet,2791,description,0,34864402-2791 description-0,non_debt,-
trafodion,464,comment,217017345,Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/703/,non_debt,-
incubator-mxnet,6183,comment,300849702,"Yes I'd suggest just removing the tutorials portion for now until the
process is better settled.
On Thu, May 11, 2017 at 9:29 AM, Naveen Swamy <notifications@github.com>
wrote:",non_debt,-
kafka,3422,comment,310703708,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5650/
Test PASSed (JDK 8 and Scala 2.12).",non_debt,-
kafka,6509,comment,479722213,"Thanks, all. I've closed voting and marked the KIP as accepted.",non_debt,-
carbondata,3584,review,389603171,"Suppose to be && instead of || ?
line 123: 
boolean usePartitionInfoForDataMapPruning = table.isHivePartitionTable() && filter != null && !filter.isEmpty() && partitions != null
and use the same flag to reset in line 138, it is confusing now",code_debt,low_quality_code
carbondata,933,comment,303107434,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/carbondata-pr-spark-2.1/65/<h2>Build result: ABORTED</span></h2>[...truncated 633.00 KB...]	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.maven.plugin.MojoFailureException: Timed out after 0 seconds waiting for forked process to complete.	at org.scalatest.tools.maven.AbstractScalaTestMojo.runForkingOnce(AbstractScalaTestMojo.java:319)	at org.scalatest.tools.maven.AbstractScalaTestMojo.runScalaTest(AbstractScalaTestMojo.java:242)	at org.scalatest.tools.maven.TestMojo.execute(TestMojo.java:106)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)	... 31 more[ERROR] [ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR]   mvn <goals> -rf :carbondata-spark-common-testBuild was abortedchannel stoppedSetting status of ea1b4a7f4d8ec8bf0a80cfcc989097949c0065a2 to FAILURE with url https://builds.apache.org/job/carbondata-pr-spark-2.1/65/ and message: '(Spark 2.1) 'Using context: Jenkins (Spark 2.1): Maven clean install",non_debt,-
spark,1484,comment,69383220,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/25329/
Test PASSed.",non_debt,-
spark,29715,comment,692354511,"No. That's a side improvement which can be dropped, not a major goal.
As I commented, fixing the problems on DataStreamWriter isn't the purpose of introducing DataStreamWriterV2. This is rather providing symmetric user experience between batch and streaming, as with DataFrameWriterV2 end users can go through running batch query with **catalog table** on writer side, whereas streaming query doesn't have something to enable this.
The problems I described in previous comment are simply the problems on Structured Streaming - let me explain at the end of comment, as it might be going to be out of topic.
I see DataFrameWriterV2 has integrated lots of other benefits (more fluent, logical plan on write node, etc.) which should be great to have in DataStreamWriterV2, but I think they're not a key part of *WriterV2. Supporting catalog table is simply the major reason to have it.
Regarding the problems on Structured Streaming - 
I kicked the incomplete state support on continuous mode out from Structured Streaming, but I basically concerns about ""continuous mode"" itself, as it's rather applying hacks to workaround architectural limitation. (+ No one cares about it in community.) 
And as I had initiated discussion earlier (and has been commented in various PRs), I think complete mode should be kicked out as well. The mode addresses some limited cases but is treated as one of valid modes which adds much complexity - some operations which basically shouldn't be supported in streaming query are supported under complete mode, and vice versa. Because the mode doesn't fit naturally.
It's useful for now because Spark doesn't support true update mode on sink - and once Spark can support update mode on sink, content in external storage should be just equivalent to what the complete mode provides, without having to dump all of the outputs. (Or that's just because of missing feature - queryable state.) Probably we can simulate complete mode via having a special stateful operator which only works with update mode.
Specific to micro-batch, supporting DSv1 is also a major headache - lots of pattern matchings in MicroBatchExecution are to support DSv1, and even there're workarounds applied for DSv1 (e.g #29700). I remember the answer in discussion thread that DSv1 for streaming data source is not exposed to the public API which is great news, but I see no action/plan to get rid of it. Is there something DSv2 cannot cover the functionality which is possible in DSv1? If then why not prioritize to address the problem?",design_debt,non-optimal_design
accumulo,279,review,126737922,HBase does this with their `TableName` class to avoid tons of objects hanging around needing GC. The API is pretty nice as an end-user:,non_debt,-
incubator-mxnet,17793,review,390011098,alignment.,non_debt,-
beam,11291,comment,607929824,Run Load Tests Python GBK Flink Batch,non_debt,-
spark,31264,comment,766672542,@MaxGekk @maropu Please let me know if there's anything else I need to do to get this moving.,non_debt,-
spark,22755,comment,434190727,LGTM,non_debt,-
ozone,856,summary,0,HDDS-3475. Use transactionInfo table to persist transaction information.,non_debt,-
spark,1317,comment,48206949,Merged build started.,non_debt,-
spark,9991,comment,160691520,"Sorry, I don't follow. `SparkListenerApplicationEnd` is posted by `SparkContext.stop`, which is the same place where you're adding the hook to clean up the listener. So it should behave exactly the same way, no?",non_debt,-
drill,381,review,54017535,"It looks like this flag is designed to allow the adjustment to only happen once, is that actually what we want? If the row size is growing it would seem like a good idea to allow for several batch size adjustments. It also removes another boolean state to manage.",non_debt,-
superset,6563,comment,450139305,@mistercrunch Any thoughts? Line chart annons seems use moment library to parse dates and it would work fine with standard date/time formats,non_debt,-
servicecomb-java-chassis,1013,review,238560805,ok,non_debt,-
qpid-dispatch,308,summary,0,DISPATCH-1005 - Fixed system_tests_ssl.py to work on rhel6 and other ‚Ä¶,non_debt,-
kylin,1218,summary,0,KYLIN-4485 Create a self service interface for cube migration,non_debt,-
spark,1749,summary,0,[SPARK-1997] mllib - upgrade to breeze 0.8.1,non_debt,-
lucene-solr,2200,review,556614634,Could we do this by creating a new class loader instead of a whole new process?,non_debt,-
spark,3118,review,19986239,Scala style (braces):,non_debt,-
beam,13594,comment,749705019,Run Spark ValidatesRunner,non_debt,-
daffodil,3,review,148564978,"I understand this is hardcoded now, but expected this naming convention would change. So are we going to continue with the NNN-SNAPSHOT convention for naming?",code_debt,low_quality_code
carbondata,691,comment,289054025,Can one of the admins verify this patch?,non_debt,-
carbondata,3436,comment,557416915,"Build Failed  with Spark 2.3.2, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/982/",non_debt,-
arrow,7958,comment,673646755,@andygrove  I personally find them helpful -- I don't follow all notifications for rust/arrow updates so it is helpful,non_debt,-
flink,3900,comment,301477442,@zentol do you agree to merge this now. Its the last thing I would like to get into RC1.,non_debt,-
tomee,142,review,207653483,Shouldn't we have a wrapper for this?,non_debt,-
apisix-dashboard,1102,review,564459527,Style.,non_debt,-
beam,3459,comment,312008421,"Yes, I was trying to find resources about how dependency scopes relate to ITs, but I could not find anything useful. What I read implied that it would include `<scope>test</scope>`. We need to solve this without changing the SDK, because this sort of module is not generally going to be part of our project, but just some externally developed IO.",non_debt,-
incubator-mxnet,8302,review,155828922,The documentation need to be updated. Does copying from mkl-dnn array to sparse ndarray work??,documentation_debt,outdated_documentation
kafka,2810,comment,291888673,The tests that failed are known transient failures that have been fixed in trunk.,non_debt,-
airflow,6287,comment,539683723,"# [Codecov](https://codecov.io/gh/apache/airflow/pull/6287?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/6287?src=pr&el=continue).",non_debt,-
cloudstack,1740,comment,353858935,"@yvsubhash you are right... My problem is not related to this one here. The snapshots in XenServer is working as you said. However, sometimes if exceptions happen during copy of the snapshot to the secondary storage, snapshots in the primary storage are not cleaned. I am still investigating and trying to find a way to solve this problem.
BTW thanks for the reply!",design_debt,non-optimal_design
camel-quarkus,2385,description,0,193065376-2385 description-0,non_debt,-
apisix,1370,review,400053650,add space after comma,non_debt,-
activemq-artemis,1822,comment,370971191,@franz1981 ok to merge this?,non_debt,-
flink,10037,review,340618306,"I consider this to be part of a private helper method where `FlinkDistributionOverlay` is the private helper method. Hence, I believe this is ok. Happy to reduce the access of the constructor to package private.",non_debt,-
nifi,4221,comment,704430785,"Hey @taftster I made the changes and cherry picked them to your branch, not authorized to push them however. Can you either give me access or if you have another suggestion would be fine with me.",non_debt,-
pulsar,7968,comment,686501656,/pulsarbot run-failure-checks,non_debt,-
attic-apex-malhar,533,review,103970336,"Suggest changing this to something like: com.example.fixedwidthparser.Ad
to emphasize that it should be the fully qualified class name (note that this is what is done in the example).",non_debt,-
zookeeper,741,summary,0,MAVEN MIGRATION - ZOOKEEPER-3226 - add profile for C build,non_debt,-
spark,28979,comment,656916760,retest this please,non_debt,-
spark,26594,review,353931395,nit. `//  Need` -> `// Need`. (reducing one space between `//` and `Need`.).,code_debt,low_quality_code
airflow,3994,review,237007226,"Dict *labels* doesn't contain *try_number* key, as it's not set in *WorkerConfiguration* make_pod. That's the reason why pods are not deleted.
https://github.com/apache/incubator-airflow/blob/v1-10-stable/airflow/contrib/kubernetes/worker_configuration.py#L197",non_debt,-
spark,18865,review,137930066,17165658-18865 review-137930066,non_debt,-
flink,7186,review,238983295,"Sorry for late response because of a little busy these days.
I agree with your above comments. So whether the task executor can be released is based on whether there are active channels in this executor. The task executor can only exit after all the tcp connections are closed gracefully.
In theory as long as the downstream received all the data from the network, then the upstream side can be released normally, no need to wait all the received data are processed completely by downstream side. But we have on existing ack mechanism to notify upstream side of this, so it is easy to rely on close request currently.  Based on downstream's consumption to release upstream's resource, it may get extra benefits in failover scenarios in future for persistent output files in upstream side, because the upstream do not need to restart during consumption exception in downstream side.
But I just a little wonder it might bring potential effects in future via close request. For example, if there are 10 downstream tasks reuse the same tcp connection, and 9 tasks are finished earlier and only one tail task delay long to finish, then all the 10 partition views must be released together until the last downstream task finished. Although it might be no bad effects for delay releasing partition views currently.
I would continue reviewing other parts of this PR and it may take some days on my side. :)",non_debt,-
carbondata,2568,comment,424882714,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/541/",non_debt,-
trafficserver,1745,comment,298338235,clang format *successful*! https://ci.trafficserver.apache.org/job/clang-format-github/334/,non_debt,-
geode,5743,review,523291215,so cool!,non_debt,-
pulsar,4546,comment,502910548,@zymap @Jennifer88huang Glad to help.,non_debt,-
karaf,807,description,0,832676-807 description-0,non_debt,-
echarts,57,summary,0,1.1.2 release,non_debt,-
camel,1871,description,0,"¬† ""CAMEL-11617:spring-boot - service-call tests uses hardcoded port numbers""",non_debt,-
airflow,6469,review,364126065,Done,non_debt,-
flink,1953,comment,216247252,"@rmetzger this is good enough for me, since there are also tests. What do you think?",non_debt,-
trafficserver,2359,summary,0,Remove the correct entry from priority queue and insert the new node into the queue,non_debt,-
spark,31721,review,589378403,"nit: please use the upper case where possible, `LIST ARCHIVES`.",code_debt,low_quality_code
netbeans-website,9,description,0,"‚Ä¶feature' section are automatically enlarged when clicked.
'colorbox' is licensed under the MIT license
https://github.com/jackmoore/colorbox/blob/master/LICENSE.md
src/content/download/nb90/index.asciidoc contains an example of usage
when the document is in asciidoc format",non_debt,-
zeppelin,391,comment,155602970,"Thanks for taking care profiles for MapR distribution.
Looks good to me and merge if there're no more discussions.",non_debt,-
carbondata,3629,comment,593783427,@akkio-97 please update the latest CI UT build on these changes,non_debt,-
kafka,1023,comment,194037189,Thanks for the patch. LGTM,non_debt,-
carbondata,1718,comment,355005406,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/2666/",non_debt,-
kafka,2175,comment,268546249,"Thanks for the PR, cc @junrao.",non_debt,-
spark,11198,comment,186102602,LGTM pending test,non_debt,-
airflow,11541,review,507190029,"Same here  - there are possibly few hundreds of places all over our code where "" no quote on the left side inside [[ ]] "" is  followed. I'd prefer to keep it - it makes the code a bit more readable.",code_debt,low_quality_code
spark,7418,review,34866421,"Oh, right! Here. Thanks for explanation.",non_debt,-
zeppelin,1715,comment,264497838,Merge to master if there're no more discussions.,non_debt,-
spark,23546,review,249115818,`HCFS path where files with the client:// scheme will be uploded to in cluster mode.`,non_debt,-
flink,439,comment,76031299,"I like the idea - that means that the webfrontend now shows what values are actually used, even if they are not in the config.
To make it even nicer, it would be cool to:
- Show which values come from the config (top) and show for which ones what default is used (in a second section below)
- add a Unit test that checks that all keys from the `ConfigConstants` are converted. I think it is easy to check this via enumerating all fields in the class using reflection.",test_debt,lack_of_tests
spark,7046,summary,0,[SPARK-8639] [Docs] Fixed Minor Typos in Documentation,documentation_debt,low_quality_documentation
beam,187,comment,210549419,R: @davorbonaci,non_debt,-
druid,10544,comment,729263851,"This PR is no longer WIP. Build is passing, there is good amount of test coverage and I have also tested it successfully on small test clusters running on kubernetes without a zookeeper cluster at all. This PR is ready to be merged.
While I am sure caveats would pop up when this gets used in long running large clusters, this is a good starting point to be released as an experimental feature in next Druid release and noted as such in the docs introduced.
sidenote:
if someone has experimented with running k8s clusters in travis builds, please feel free to work/comment on https://github.com/apache/druid/issues/10542 as that would set the stage of testing the code here on real k8s cluster in the travis builds.",non_debt,-
flink,2381,summary,0,[FLINK-4414][cluster management] Remove restriction on RpcService.getAddress,non_debt,-
spark,27601,comment,706443248,"This issue is present in branch-2.4. It's a good to have but not critical, can we still backport it to branch-2.4? 
cc: @cloud-fan @dongjoon-hyun",non_debt,-
skywalking,4228,comment,575036602,"# [Codecov](https://codecov.io/gh/apache/skywalking/pull/4228?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/4228?src=pr&el=continue).",non_debt,-
phoenix,447,summary,0,PHOENIX-4918 Apache Phoenix website Grammar page is running on an old‚Ä¶,non_debt,-
cloudstack,2613,comment,404858588,@rhtyd rebased.,non_debt,-
trafficserver,4097,review,216351934,"My interpretation of fini_received flag means that we have seen a FINI frame or otherwise the client side has indicated that the connection is gone.  
The call to release_stream with a nullptr argument just goes through and sees if the state of the world is ready to shutdown (all stream counts set to 0 and a FINI has been seen).",non_debt,-
carbondata,1571,comment,347845111,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1568/",non_debt,-
spark,19977,comment,353715980,"I got the overhead, so I pushed the commit to support it.",non_debt,-
spark,20502,review,165847251,Still worth checking for nonEmpty? the behavior may be different in the second change here otherwise.,non_debt,-
geode,5196,review,436309760,"If system property java.home is defined, please use `System.getProperty(""java.home"")+""/bin/java""` instead.  Only use `java` if system property java.home is not defined",non_debt,-
spark,411,comment,40448662,"Jenkins, add to whitelist.",non_debt,-
incubator-pinot,6262,review,525535130,"That's why I explicitly tested reading the last doc ids (`_lastSequentialDocIds`).
Modified the test to test sequentialDocId starting from 0 - 31",non_debt,-
zeppelin,3322,comment,470856893,"LGTM, thanks @liuxunorg",non_debt,-
arrow,4198,summary,0,ARROW-5209: [Java] Provide initial performance benchmarks from SQL workloads,non_debt,-
reef,1205,comment,268099418,"Yes, the last one is in Dispose. Driver may shut down before receiving it. I just relaxed the checking little bit.",non_debt,-
spark,26395,comment,553024056,+1,non_debt,-
incubator-weex,741,description,0,"1Ôºådestroy clean adapter
1Ôºåif resuse",non_debt,-
storm,2433,review,152902841,Two lines looks duplicated. We could remove duplicated two lines via following:,code_debt,duplicated_code
carbondata,1817,comment,358534297,"Build Success with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/1689/",non_debt,-
spark,1561,comment,50182962,"Looks good to me, this is definitely cleaner",code_debt,low_quality_code
spark,708,summary,0,Converted bang to ask to avoid scary warning when a block is removed,code_debt,low_quality_code
arrow,8067,review,478525557,Yup :-),non_debt,-
storm,2204,comment,316832758,"I did a bunch of testing and I am +1 on merging this in.  There is still the issue with double escaping the contents returned for a file, but I think we can fix that after if we want.",non_debt,-
calcite,765,summary,0,[CALCITE-2421] Improve RexSimplify when unknownAsFalse is true,non_debt,-
systemds,773,summary,0,"[WIP] [SYSTEMML-445] Upgraded CUDA/CuDNN versions and added LSTM, batch normalization kernels",non_debt,-
carbondata,2488,review,202609488,so how can we get the version info for a table?,non_debt,-
spark,11041,comment,183879767,@mark800 are you following up on this?,non_debt,-
spark,18592,comment,328446316,"@gatorsmile I opened a new pr, so if you get time, could you check #19188? Thanks!",non_debt,-
nifi,3021,comment,428365005,Thanks much @mcgilman !,non_debt,-
cloudstack,2402,comment,356922073,@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.,non_debt,-
beam,7549,comment,455493852,"The names here are generally conventions.
In your client, you can do
    git remote -v
to list all your ""remotes"" i.e. other repositories that you push and pull
to. You can also do
    git branch
to list all your branches.
By default when you clone a fresh repository, you get a remote called
""origin"" that points to wherever you cloned it from. New (and most
existing) branches have a ""master"" branch by default (e.g. beam HEAD is the
master branch on https://github.com/apache/beam). So
    git pull --rebase=interactive origin master
pulls the branch ""master"" from the remote ""origin"" (presumably
github.com/apache/beam) into your own branch, and the --rebase=interactive
means take all your local commits and attempt to apply them on top of
origin/master, rather than creating a merge commit, interactively.
pulls all the remote changes into my repo (but not into any of my branches)
then ""git rebase [-i]"" rebases my current branch on top of it.
    git push [remote_name] [branch_name] [--force]
will push branch_name (defaults to the current branch) to remote (branches
can have a default remote set up) and the force means overwrite what's
there if it differs (needed after a rebase).
Another thing you can to save your work is if you're on a branch and
concerned about messing it up, you can do ""git checkout -b
some-branch-backup"" to make a copy of your current branch and then if you
mess up your current branch horrendously you can delete it (git branch -d
branch-name) and re-create the branch from your backup (git branch -b
some-branch some-branch-backup). Hopefully that's useful for being able to
experiment more freely, which will help in learning git.
On Fri, Jan 18, 2019 at 1:16 AM CraigChambersG <notifications@github.com>
wrote:",non_debt,-
kafka,7517,review,335607040,Huh. It looks like removing this results in not seeing all the tombstones we expect. I'll leave it in.,non_debt,-
kafka,4994,review,188192583,Use StringBuilder? And perhaps rename method too?,non_debt,-
hadoop,2553,comment,753006191,Merging in as #2577 #2579 #2580; closing this one to avoid confusion,non_debt,-
netbeans,1581,description,0,Added -J-Djdk.lang.Process.allowAmbiguousCommands=true to netbeans.conf because of Maven spacing issue: https://lists.apache.org/thread.html/bf415874d97739bd23eef134a246a8a7241c011372b36cc8650bf901@<dev.netbeans.apache.org>,non_debt,-
samza,986,review,277919393,Why creating this class other than use DefaultCouchbaseEnvironment.Builder  directly?,non_debt,-
spark,12036,review,57815957,"yah, I should put the resources into it.",non_debt,-
kafka,3010,comment,300525753,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3705/
Test FAILed (JDK 7 and Scala 2.10).",non_debt,-
trafficcontrol,4014,summary,0,Rewrite federation user GET/POST/DELETE,non_debt,-
superset,9671,summary,0,"FilterBox,BigNumber,WorldMap: Handle empty results",non_debt,-
kafka,8265,review,391317111,nit: `if {} else {log.trace()}`.,non_debt,-
cloudstack,4301,comment,684872369,Packaging result: ‚úîcentos7 ‚úîcentos8 ‚úîdebian. JID-1878,non_debt,-
carbondata,1808,comment,364020675,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3577/",non_debt,-
flink,14238,review,531619417,After second level headings a `{% top %}` is recommended.,non_debt,-
ignite,7830,description,0,For team city run,non_debt,-
cloudstack,4447,review,518509360,I tested this with a host with 8 CPUs and it failed trying to allocate 16 on XCP-ng 7.6,non_debt,-
storm,1131,comment,187287074,+1 on Trident support.  Thanks!,non_debt,-
spark,12122,description,0,"Currently, `SimplifyConditionals` handles `true` and `false` to optimize branches. This PR improves `SimplifyConditionals` to take advantage of `null` conditions for `if` and `CaseWhen` expressions, too.
**Before**
**After**
**Hive**
Pass the Jenkins tests (including new extended test cases).",non_debt,-
carbondata,1874,comment,362064998,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3263/",non_debt,-
arrow,4827,summary,0,ARROW-5880: [C++][Parquet] Use TypedBufferBuilder instead of ArrayBuilder in writer.cc,non_debt,-
spark,993,comment,48433227,Merged build started.,non_debt,-
spark,12102,review,58289671,"why is this a function that returns a function, rather than a function?",non_debt,-
spark,3237,comment,63824131,"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/23670/
Test FAILed.",non_debt,-
ozone,1516,comment,714884328,Thanks @xiaoyuyao 's review.  I have submitted a new commit fix the above issues.,non_debt,-
incubator-doris,2585,description,0,"1. Ignore .vscode dir
2. Ignore C++ file in be/src/gen_cpp/ dir",non_debt,-
incubator-pinot,4924,comment,565638501,"# [Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4924?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4924?src=pr&el=continue).",non_debt,-
spark,50,comment,36947949,I've created SPARK-1201 (https://spark-project.atlassian.net/browse/SPARK-1201) to cover optimizations in cases other than DISK_ONLY.,non_debt,-
incubator-mxnet,7082,review,133588261,Right. For distributed training we want to reduce the network traffic and use `row_sparse_pull` instead. We may change it if it's not user-friendly. One distributed trainig example with sparse weight is here: https://github.com/eric-haibin-lin/mxnet/blob/sparse/example/sparse/linear_classification.py#L171,non_debt,-
beam,9359,summary,0,[SQL] Add support for GCS entries in DataCatalog,non_debt,-
spark,17227,comment,285531634,17165658-17227 comment-285531634,non_debt,-
rocketmq,207,review,157440960,It's possible to just change the certificate file,non_debt,-
bookkeeper,97,review,102810918,This comes from the original patch from @ivankelly. In production I usually use PKCS12. I can make a new option for the format. Do you think it wild be useful?,non_debt,-
beam,12529,comment,672001509,run seed job,non_debt,-
spark,25707,comment,528685769,"Yea, it looks making sense to me.",non_debt,-
madlib,425,comment,519652224,We also need to add user docs along with examples for the new function,documentation_debt,outdated_documentation
ignite,6127,summary,0,IGNITE-11233 Fix for .NET build: Ignite Build for Java 11 does not reuse ignite-tools from Build Apache Ignite,non_debt,-
beam,1727,comment,270223022,LGTM,non_debt,-
trafficcontrol,2002,description,0,fixed some issues and improved testing on .../about endpoint,non_debt,-
kafka,7525,comment,554034177,"Thanks, @omkreddy!",non_debt,-
incubator-pinot,2314,comment,357294336,In favor of another one,non_debt,-
skywalking,4783,review,433078278,done,non_debt,-
kafka,4154,review,147772101,Actually just one comment. Is the producer also consistent in accounting for the message set overhead?,non_debt,-
spark,11407,summary,0,Update CHANGES.txt and spark-ec2 and R package versions for 1.6.1,non_debt,-
druid,3180,review,71416663,"Similar comment to the IN filter, it might be worth only doing this if a long predicate is actually requested. (but just once if a long predicate is requested more than once)",non_debt,-
pulsar,7774,comment,672578813,"If we revert #7690 changes, the deadlock issue will reappear, so we should not it. Can the issue of #7706 be reproduced on the master branch? If so, could you fix the code in the master branch?",non_debt,-
airflow,6843,comment,567218797,"This command was always run when executing the `` airflow worker`` command.  The option to disable autostart of this server has been introduced recently, so we can assume that everyone who starts worker must have this port free.  These logs should be shared from workers, so no one could technically run it without playing with strict process isolation (linux net namespace) and other tricks.",non_debt,-
shardingsphere,4541,summary,0,remove TablesContext.find(),non_debt,-
airflow,9618,comment,654341478,I sent DM to you. ;-),non_debt,-
beam,1775,comment,272341454,@robertwb This fix Python postcommit failure ([build link](https://builds.apache.org/view/Beam/job/beam_PostCommit_Python_Verify/1019/)),non_debt,-
beam,5723,review,197965060,Or we can have a GeneratedRowCoder interface too.,non_debt,-
pulsar,548,review,126598589,"forgot this, will remove",non_debt,-
trafodion,778,comment,256024948,"jenkins, extra tests",non_debt,-
samza,347,review,180646742,"I haven't measured how much time increase it causes. The purpose of this change is to make each test independent of each other. If we use BeforeClass and AfterClass tags, then these methods need to be static and the variables instantiated (i.e. producer, systemAdmin) needs to be static as well. These variables will then be instantiated exactly once for the test suite and used by all unit tests in this test suite. Thus kind allows interference between tests and make make test development difficult.
During development of this patch the interference must have caused some problem which required me to make this change. I don't exactly remember what that problem is now. But in general it seems like good practice to make unit test independent of each other. What do you think?",test_debt,expensive_tests
arrow,9387,review,568793531,Moved the fallback path into this method (mimicking GetReadableBuffer).,non_debt,-
storm,3293,summary,0,[STORM-3657] set storm.messaging.netty.authentication to topoConf OR daemonConf,non_debt,-
spark,28345,review,415390714,17165658-28345 review-415390714,non_debt,-
spark,26412,comment,559009730,retest this please,non_debt,-
hadoop,951,review,295488918,"Makes sense. If you need to override for a single test operation, can always add a @VisibleForTesting setter.",non_debt,-
kafka,8337,review,401303331,"Actually I just copied over the implementation from some other class, and wasn't getting any warnings...I should probably do a thorough pass over all the warnings to see what else is missing, but this (and the raw types) is enabled now.
Thanks for catching",non_debt,-
arrow,9009,comment,751373522,"Looks good, thanks!",non_debt,-
incubator-mxnet,3103,review,75800047,Got it. Thanks for the clarification.,non_debt,-
kafka,2995,comment,299880264,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3647/
Test FAILed (JDK 8 and Scala 2.11).",non_debt,-
hadoop,1538,comment,535837339,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
incubator-heron,81,review,51090091,Can we get rid of this?,code_debt,low_quality_code
incubator-mxnet,13145,description,0,"Add documentation on GPU performance on Quantization example so end user knows that GPU performance is expected to be slower than CPU
Fixes https://github.com/apache/incubator-mxnet/issues/10897
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here
@ThomasDelteil @reminisce 
@mxnet-label-bot [pr-awaiting-review]",test_debt,lack_of_tests
tvm,6333,review,476681151,done,non_debt,-
nifi,3681,review,320906392,This should probably log a warning to notify administrators that the value provided was invalid and a default value will be used.,non_debt,-
pulsar,359,comment,294627442,@merlimat I am not sure about which `failover tests` we should run ?,non_debt,-
hive,1706,review,540697885,"No this test was added for testing calcite based rewrite. The text based rewrite modified the plan because I forgot to turn it off in this test case.
Turned off and reverted the out.",non_debt,-
spark,3213,review,20406513,"Done and done @davies, @yhaui
On Sat, Nov 15, 2014 at 5:17 PM, Yin Huai notifications@github.com wrote:",non_debt,-
trafficcontrol,256,comment,278974534,There is a possibility that you might have some patches against your 5.3.x that might make this work. But it won't compile against 5.3.2 provided by Traffic Server.,non_debt,-
spark,20197,summary,0,[SPARK-21293][SPARKR][DOCS] structured streaming doc update,non_debt,-
spark,29728,review,487921098,"Total nit, but isn't it clearer to write `isSupportedComparison` and avoid inverting the logic everywhere?",code_debt,low_quality_code
kafka,4636,summary,0,KAFKA-6054: Add 'version probing' to Kafka Streams rebalance,non_debt,-
flink,12521,review,436553652,Same here.,non_debt,-
ozone,750,comment,610937389,"Sorry for late comment, but today I learned that Hadoop has a [TimedOutTestsListener](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/TimedOutTestsListener.java).
It's configured in the `pom.xml files`, for example here: https://github.com/apache/hadoop/blob/1189af4746919774035f5d64ccb4d2ce21905aaa/hadoop-hdfs-project/hadoop-hdfs/pom.xml#L236
Wouldn't it be more effective to use a similar listener? (If yes, I would prefer to fork it instead of adding one more Hadoop dependency, especially after HDDS-3353 and HDDS-3312).",non_debt,-
airflow,10211,description,0,"---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.",non_debt,-
geode-native,294,description,0,"Several files were missing licenses, I added them except for solaris.patch because I do not know how to add comments to a patch file",non_debt,-
incubator-weex,478,comment,311581778,Ref https://issues.apache.org/jira/browse/WEEX-52,non_debt,-
nifi-minifi-cpp,74,review,109758278,i want to return the same so that it can be reused in next trigger.,code_debt,low_quality_code
trafficcontrol,4160,review,354405110,Remote Procedure Call (as opposed to a native Go API for example).,non_debt,-
spark,16758,review,98775275,Any updates to the state will be stored and passed to the user given function in subsequent batches when executed as a Streaming Query.,non_debt,-
spark,15415,review,101698275,"Document what it means when this is not set and that this must be >= 1.
Also say this is not set by default.",non_debt,-
activemq-artemis,49,comment,114989236,"[ActiveMQ-Artemis-PR-Build #476](https://builds.apache.org/job/ActiveMQ-Artemis-PR-Build/476/) SUCCESS
This pull request looks good",non_debt,-
drill,1181,comment,377430718,@paul-rogers I added metrics for merge join also. I refactored AbstractRecordBatchMemoryManager to handle batches from multiple streams. Please review when you get a chance.,non_debt,-
drill,1809,summary,0,DRILL-6951: Row set based mock data source,non_debt,-
lucene-solr,1910,summary,0,Use github actions cache,non_debt,-
nifi,3948,description,0,"Thank you for submitting a contribution to Apache NiFi.
The default value for array fields was always being set even if not specified in the schema
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
nifi,1496,review,110565159,"I was using SplitText for testing, but I agree with you, 0 is preferable. Changed default value from 1 to 0.",non_debt,-
beam,7999,description,0,"Jenkins job that uses Java 11 enabled Dataflow Worker harness to run validatesRunner test array on Dataflow
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) <br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.",non_debt,-
flink,757,comment,117042891,"Hello Sachin, could you explain what the discrete sampler does?",non_debt,-
spark,25048,review,300427664,"Thank you for investigating this, @HeartSaVioR .
The original logic looks not safe. Do you think if there is any other better way?",requirement_debt,non-functional_requirements_not_fully_satisfied
airflow,11121,review,519171363,^^ @mik-laj,non_debt,-
kafka,9715,review,539391823,"BTW, my thought was",non_debt,-
spark,29156,description,0,"Add CTE hint resolve in `org.apache.spark.sql.catalyst.analysis.ResolveHints.ResolveJoinStrategyHints#apply`
Add a UT in `org.apache.spark.sql.test.SQLTestUtils#test`
Branch 2.4, when resolve CTE in `org.apache.spark.sql.catalyst.analysis.Analyzer.CTESubstitution`, we have a chance `executeSameContext` to apply all rules to CTE include hint resolve.
Branch 3.0, because `CTESubstitution` is moved to a separated class, we miss the feature as follow:
`
scala> sql(""create temporary view t as select 1 as id"")
res0: org.apache.spark.sql.DataFrame = []
scala> sql(""with cte as (select /*+ BROADCAST(id) */ id from t) select id from cte"")
org.apache.spark.sql.AnalysisException: cannot resolve '`id`' given input columns: [cte.id]; line 1 pos 59;
'Project ['id]
+- SubqueryAlias cte
   +- Project [id#0]
      +- SubqueryAlias t
         +- Project [1 AS id#0]
            +- OneRowRelation
`
No
Unit test.",non_debt,-
arrow,4421,review,295525649,Sounds fine to me,non_debt,-
beam,12010,comment,648516228,"Retest this please
I'll wait for Allen's review here. Thanks.",non_debt,-
beam,11532,comment,630903624,Retest this please,non_debt,-
beam,3936,review,142818337,This will slow down this performance critical code.,code_debt,slow_algorithm
spark,4654,comment,76954011,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/28231/
Test PASSed.",non_debt,-
infrastructure-puppet,1326,description,0,Add a github webhook relay for projects,non_debt,-
storm,2547,summary,0,Storm 2913 2914 1.x,non_debt,-
zeppelin,3331,review,265334757,is this defined else where?,non_debt,-
airflow,9281,comment,643734783,Should it be detected by pylint or other linter?,non_debt,-
jena,456,summary,0,JENA-1585: Fuseki core reorg,non_debt,-
hadoop,2473,review,528784174,thanks,non_debt,-
zookeeper,726,comment,442395535,@eolivelli - import static added. Let me know if this looks ok !,non_debt,-
cloudstack,2706,comment,398365937,@PaulAngus @DaanHoogland this is ready for merge once the cloudstack-common rpm is verified to not install the libuuid i386 dependency.,non_debt,-
hadoop,2707,comment,815726209,":confetti_ball: **+1 overall**
This message was automatically generated.",non_debt,-
flink,9890,review,335896695,Keep four spaces indentation for these parameters. Same for method `createProjectionRexProgram`,code_debt,low_quality_code
carbondata,3177,comment,489409351,"Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/3286/",non_debt,-
zeppelin,1838,summary,0,[ZEPPELIN-1832] Fixed a bug in zombie process when Zeppelin stopped.,non_debt,-
kafka,6375,comment,469877930,"Java 11 failed with `kafka.server.LogOffsetTest.testGetOffsetsBeforeNow` 
retest this please",non_debt,-
beam,13886,comment,776370307,cc: @lazylynx,non_debt,-
brooklyn-server,664,summary,0,Add contributing guidelines and PR template,non_debt,-
openwhisk,2361,comment,309427426,pg3/742,non_debt,-
couchdb,2101,review,314395651,206417-2101 review-314395651,non_debt,-
cloudstack,1604,comment,262719310,@rhtyd a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests,non_debt,-
infrastructure-puppet,1604,summary,0,clean up transport map & docs,code_debt,low_quality_code
drill,2045,summary,0,"DRILL-7683: Add ""message parsing"" to new JSON loader",non_debt,-
shardingsphere,5020,review,400787874,ok,non_debt,-
incubator-mxnet,15381,review,298887765,"I don't think this function will have the correct signature after rendering. Please build and check, and override the function signature if needed.",non_debt,-
incubator-mxnet,16448,comment,541376829,@junrushao1994 you're the man!,non_debt,-
incubator-mxnet,10074,review,173891556,34864402-10074 review-173891556,non_debt,-
flink,14003,review,532046446,"DDL statements are e.g. CREATE, ALTER, DROP. The HELP command will show all the types of statements you can run, which also includes DML and DQL.",non_debt,-
cloudstack,1519,comment,215347296,"@dsclose I think it is better to split this PR into some isolated PRs, as the issues are isolated.
to be honest, some commits looks good to me ( as we have similar fix in our production), others need testing.",test_debt,lack_of_tests
spark,7031,comment,116213031,"Jenkins, retest this please.",non_debt,-
druid,2231,comment,173443345,"Actually , looking at com.alibaba.rocketmq.common.ServiceThread from http://grepcode.com/file/repo1.maven.org/maven2/com.alibaba.rocketmq/rocketmq-common/3.2.6/com/alibaba/rocketmq/common/ServiceThread.java/ it looks like ServiceThread does a lot of synchronization and notifying on itself. That makes this impl much harder to review without an intimate understanding of ServiceThread.
Regarding testing, The general recommendation if the framework does not have a unit-test friendly tool, is to use EasyMock to force the behaviors you are testing for.",design_debt,non-optimal_design
ignite,1836,description,0,Do not merge,non_debt,-
arrow,9356,review,580411319,This cast is unnecessary since the `Sum` method already returns a `double`.,code_debt,complex_code
arrow,4167,comment,544971543,"Great progress @nevi-me !
I'm traveling this week but will start helping with integration testing next week.",non_debt,-
beam,11776,comment,632724995,"R: @aaltay Could you take a look?
Website staging is not working at the moment, fix is in progress: https://github.com/apache/beam/pull/11796",non_debt,-
tvm,3531,review,305483155,Moving to C++ and better error message,code_debt,low_quality_code
spark,6593,description,0,"This also helps us get rid of the sparkr-docs maven profile as docs are now built by just using -Psparkr when the roxygen2 package is available
Related to discussion in #6567 
cc @pwendell @srowen -- Let me know if this looks better",documentation_debt,low_quality_documentation
carbondata,3834,review,460376152,"suggestionÔºö  convert ""_temp"" to constants",non_debt,-
airflow,5575,description,0,Adds Instacart to readme.md only,non_debt,-
spark,15245,comment,250036300,LGTM,non_debt,-
incubator-mxnet,6237,comment,301376249,"@madjam, we have updated the tutorial with new content, same for the symbol, 
Your changes are based on old content.",non_debt,-
spark,29461,review,483509863,+1 to emphasize the distinct clause in SQL.,non_debt,-
skywalking,5722,review,511915125,[Endpoint slice](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/) resources should be listened to either.,non_debt,-
helix,107,summary,0,Five improvements,non_debt,-
bigtop,753,comment,794605277,Oops. I forgot to add changes in top directory. updated.,non_debt,-
samza,103,review,109215517,"This is a bit concerning that we are commenting out a good number of tests here. I would prefer to fix them, before check-in.",test_debt,lack_of_tests
drill,1330,review,197961951,"I would rather rely on the logback.xml to control log levels rather than system options. You can also use markers to tune the output at the debug level even further within a class if necessary.
https://examples.javacodegeeks.com/enterprise-java/slf4j/slf4j-markers-example/",non_debt,-
tvm,4867,comment,584951383,70746484-4867 comment-584951383,non_debt,-
spark,1313,comment,48174912,Merged build finished.,non_debt,-
beam,6938,comment,436064925,"beam_PerformanceTests_Python passed in [this build](https://builds.apache.org/view/A-D/view/Beam/job/beam_PerformanceTests_Python/1650/). Squash commits to one and wait for merging since LGTM is received.
@aaltay",non_debt,-
flink,12815,review,449585541,20587599-12815 review-449585541,non_debt,-
zeppelin,941,comment,222912035,LGTM,non_debt,-
spark,345,comment,39877610, Merged build triggered.,non_debt,-
reef,666,comment,159513996,"@dafrista , could you review this?",non_debt,-
trafficserver,762,summary,0,TS-4601: Connection error from origin_max_connection with origin_max_‚Ä¶,non_debt,-
spark,24094,review,266309009,"If the key is already in the case insensitive map, we should fail and say duplicated keys detected.",non_debt,-
tajo,618,review,33929768,It seems to be wrong.,non_debt,-
apisix,781,summary,0,ASF: added license header.,non_debt,-
systemds,910,comment,633149881,"LGTM. Thanks @Baunsgaard - overall, this is a fine patch. Down the road, we might want to create consistency of parameters among all classification and regression scripts, and add further svm debugging tools (or an verbose option), where we actually compute the introduce confusion matrix and show it to the user (as done in the msvm-predict algorithm).",non_debt,-
beam,1984,review,100885021,Ditto.,non_debt,-
druid,3371,comment,259284787,is this still WIP?,non_debt,-
spark,4537,comment,89694634,"@koeninger , I can't visit [this url](https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/28872/) , it's 404. ??",non_debt,-
spark,16395,review,102133390,"@cloud-fan If I use Literal(d20170102), then this literal has dataType ""DateType"" as it is consistent with the corresponding column.  I tested it with a SQL statement: SELECT key1 FROM testTable WHERE key1 = cast('2017-01-02' AS DATE) and found that the literal has an internal type SQLDate (which is an integer).   Hence, I probably should use Literal( DateTimeUtils.fromJavaDate(d20170102), IntegerType).  Am I right?  Please advise.",non_debt,-
cloudstack,2382,review,159235216,"no, not really. must be a result of intellij's extract method",non_debt,-
accumulo,484,review,188314105,It seems like this method is no longer needed.,code_debt,dead_code
ignite,8077,description,0,"https://issues.apache.org/jira/browse/IGNITE-13288
Marked the following events as internal:
- EVT_CLUSTER_ACTIVATED
- EVT_CLUSTER_DEACTIVATED
- EVT_BASELINE_CHANGED
- EVT_CLUSTER_STATE_CHANGED
There are discovery events that are listened to by all nodes.
It will be useful to include these events to listen on all nodes by default too. 
All of them are rare, system and cluster-wide.",non_debt,-
storm,2108,review,132810994,"Nit: If this is a float, shouldn't we keep it that way in Schema?",non_debt,-
spark,32062,summary,0,[SPARK-34965][BUILD] Remove .sbtopts that duplicately sets the default memory,non_debt,-
arrow,2789,comment,461917811,Fails in appveyor after rebase,non_debt,-
superset,9287,comment,597956300,"@villebro part of the tabs migration process, which happens once the user visits sqllab when this feature is enabled, [involves using the tab id(int) cast to a string as the `sql_editor_id`](https://github.com/preset-io/incubator-superset/blob/master/superset/views/sql_lab.py#L260). Once all tabs are migrated over the type will be consistent (numeric strings). The before migration and in between state is where the error occurs (or doesn't in the case of MySQL/SQLite/others probably).",non_debt,-
accumulo,1995,review,607994959,I think these error message would be more useful if they included the full path of the client props file that was used.,code_debt,low_quality_code
spark,25047,review,303140694,"So JPS has some down sides. 
 1 if the workers are running in containers it might not work as it relies on /tmp and if that is mounted to something other then your /tmp you won't see the process.
 2. if someone is debugging a worker or its not responding jps can hang.
We may want to look at  using kill -0 for linux/mac as it will quickly tell you if its alive, not sure on windows, assume it would require something else.  Here is an stack overflow post that talks about it:
https://stackoverflow.com/questions/21460775/verify-if-a-process-is-running-using-its-pid-in-java
Note this is assuming you are saving the PID in the allocation file or using it from the PID directory.
I also think we should only run kill -0 or whatever when it is absolutely necessarily. Meaning the Worker can't allocate anything and then I would only look at the exact PIDS that have something allocated.",non_debt,-
carbondata,2345,comment,393630591,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/6215/",non_debt,-
druid,7705,review,287170344,Icon please,non_debt,-
skywalking,4822,summary,0,Keep ID as ref ID in the readSampledRecords,non_debt,-
beam,6640,summary,0,scratch,non_debt,-
brooklyn-server,264,comment,234017875,Merging now. I'll address my most recent comments in a new PR later this evening.,non_debt,-
cloudstack,2699,review,194376281,"Well, I did not see anything yet in the Github. However, if you add just a snippet of comments explaining why we need such method, I am fine with it.",documentation_debt,low_quality_documentation
spark,5400,review,27923456,nit: spaces around `{`,code_debt,low_quality_code
airflow,8809,comment,749032679,@TobKed closing as you will take over this on your terms. As we discussed offline - thank you once again!,non_debt,-
airflow,4895,comment,471386977,"@mik-laj This PR have to way to pass token, One is pass token by `Operator`, another is set it in `dingding_webhook_default` connection.
Second way have no safe problem, allow pass to operator just provide other way to send message.",non_debt,-
trafficcontrol,3966,comment,540120851,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/4435/",non_debt,-
spark,29855,review,503114602,"This follows the way it is done in the `fetchBlocks` API.
Should we remove it?",non_debt,-
geode,3175,review,255775292,This doesn't quite make sense. We created 3 mapping with various group values. I would expect the list only show the 3 mappings we created. Here there are 4 mappings in the list result. I would consider one of them duplicate.,non_debt,-
trafficserver,2229,description,0,"To avoid race of `eventProcessor.allocate(sizeof(PollCont))` vs `new ((ink_dummy_for_new *)get_UDPPollCont(thread)) PollCont(thread->mutex)`
Fix #2226",non_debt,-
gobblin,104,comment,95396477,LGTM barring the typo,documentation_debt,low_quality_documentation
beam,4751,comment,372546237,"Bein' grumbly about wedged tests and timeouts, as you do",non_debt,-
ignite,1082,comment,291875497,Already in master,non_debt,-
beam,11856,comment,696608739,"How can I run a batch pipeline with V2 runner? I got the following error when submitting the job:
`
The workflow could not be created. Causes: (c0594c9b86bfe05d): The workflow could not be created due to misconfiguration. If you are trying any experimental feature, make sure your project and the specified region support that feature. Contact Google Cloud Support for further help. Experiments enabled for project: [enable_streaming_engine, enable_windmill_service, shuffle_mode=service], experiments requested for job: [use_runner_v2]"",`
I used `--experiments=use_runner_v2` without `--streaming` being set.",non_debt,-
beam,13282,comment,725210904,Run Java KafkaIO Performance Test,non_debt,-
spark,17348,review,107335293,"the column name changes with timezone, but what about the value? can you also check the result?",non_debt,-
arrow,2978,review,234370685,"Yes for sure, good point. Well, definitely if `ALTREP` ends up working out as expected, etc. we can reconsider, but at least for now, it would help to lower the version. To give you more background, a bunch of scripts in Spark rely on older versions of R, while it's possible to upgrade those clusters, it adds overhead that would be nice to avoid at least while we can.",non_debt,-
ignite,546,description,0,31006158-546 description-0,non_debt,-
flink,1813,comment,229693884,@subhankarb We should also add Redis Sink to the fault tolerance guarantee table for the connectors in the documentation. It can be found at `flink/docs/apis/streaming/fault_tolerance.md`.,documentation_debt,outdated_documentation
gobblin,2656,review,292076607,"The reason is I get rid of `this.dags.get(dagId);` and there's no reference to a `dag` associated with this `dagId`. So clean up has to happen before `remove`, or `dag` object won't be fetched. 
Will add comment to make it clear",documentation_debt,outdated_documentation
kafka,4138,description,0,"Even though this class is internal, it's widely
used by other projects and it's better to avoid
breaking them until we have a publicly supported
test library.",design_debt,non-optimal_design
couchdb,1842,review,246573811,This is going to crash updater with `illegal_docid` on any design doc.,non_debt,-
airflow,4091,review,228726712,33884891-4091 review-228726712,non_debt,-
spark,7881,comment,127100344,retest this please,non_debt,-
kafka,6162,review,265995291,Added test cases for both.,non_debt,-
airflow,5274,description,0,"AIRFLOW-1501 introduced GoogleCloudStorageDeleteOperator (https://github.com/apache/airflow/pull/5230)
This PR adds the operator to the integration docs.",non_debt,-
spark,9193,review,42587435,"nit: this is kinda hard to read with the double negatives. consider:
?",code_debt,low_quality_code
arrow,2406,comment,411551607,The C++/Python build ran in 31 minutes this time,non_debt,-
calcite,1518,comment,548171400,"I think it will be a little hard to write a good tests for `pathPrefix`. 
One way would be to intercept RestClient HTTP calls and check that correct prefix is set.",non_debt,-
trafficserver,3711,description,0,"@SolidWallOfCode , it should be a simple addition. Can you please review this?",non_debt,-
spark,524,comment,41750424,"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/14575/",non_debt,-
spark,19424,review,144111358,"The evaluation order of these filters must be the same? If the orders are different, they are still the same, right?",non_debt,-
jmeter,237,comment,269805400,"@max3163 , are you using GraphiteBackendListener or something else (I don't fully understand your comment on ""I'm thinking to send a PR to share it too"".
Thanks",non_debt,-
spark,27208,summary,0,[SPARK-30481][CORE] Integrate event log compactor into Spark History Server,non_debt,-
netbeans,2812,review,601675547,OK,non_debt,-
trafficserver,7526,comment,810664610,Opened #7660 for 8.1.x.,non_debt,-
trafficserver,2570,comment,332364253,"@jablko --
This is reproducible for me with two different Ubuntu 14.04 LTS server 64-bit VMs. The OS OpenSSL is 1.0.1f, and I have compiled and installed OpenSSL 1.1.0f into $HOME/opt/openssl.
Result is:
Additional info:
The -rpath or -R containing $HOME/opt/openssl is NOT contained in the libtool command above so the linking fails. If I manually run the above command adding the -R then it works. So I added the -R via the change to the Makefile.am.",non_debt,-
incubator-pagespeed-ngx,261,comment,16283485,LGTM,non_debt,-
spark,21734,review,201258595,"this would mean that if you have your running application accessing different namespaces and you want to add a new namespace to connect to, if you just add the namespace you need the application can break as we are not getting anymore the tokens for the other namespaces.
I'd rather follow @jerryshao's comment about avoiding to crash if the renewal fails, this seems to fix your problem and it doesn't hurt other solutions.",non_debt,-
flink,15221,comment,810809614,Hmm... there are test failures in REST API tests. Could you take a look at those @akalash ?,non_debt,-
spark,8864,review,40624641,`#' @family subsetting functions`,non_debt,-
incubator-mxnet,5430,description,0,34864402-5430 description-0,non_debt,-
carbondata,3244,comment,502949896,LGTM,non_debt,-
zeppelin,3240,review,236241046,Probably one small section that lists the current limitations will be useful. The one which is probable worth highlighting is about the container image used by the interpreter other than spark. It will be the same as Zeppelin container image.,non_debt,-
fineract,366,comment,309149263,"@nazeer1100126 In regards to two-factor authentication, I was thinking something along the lines of using global configurations or a configuration endpoint just for two-factor. Ultimately the goal of this PR is to make the SMS & Email more generic so they can be used not only for login email and sms campaigns.
As for the SMS service, setup of tenant & sms bridges in the message gateway directly from Fineract is part of my GSoC proposal but it should be discussed further.",non_debt,-
incubator-mxnet,3859,comment,261063209,can you rebase the pr to solve the conflict? namely https://github.com/dmlc/mxnet/blob/master/docs/community/contribute.md#submitting-a-pull-request,non_debt,-
pulsar,1663,summary,0,Fix python functions,non_debt,-
kafka,7389,review,377350027,"We generally transition when we enqueue the relevant request, and we enqueue the abort and init at the same time. To my mind that means we go straight from `ABORTING` to `INITIALIZING`, but I'm not wedded to it if you think going through `UNINITIALIZED` makes more sense.",non_debt,-
calcite,1991,review,454539517,I will create another JIRA to support on-demand rule matching.,non_debt,-
beam,1230,comment,260433002,"Added a test for the builders for Read, and cleaned up the code a bit for Write (since topic should never be null there).",code_debt,low_quality_code
spark,27946,review,395206016,Sure~,non_debt,-
spark,11796,comment,200517197,retest this please,non_debt,-
storm,1468,summary,0,STORM-1885. python script for squashing and merging prs.,non_debt,-
druid,4284,comment,303822859,"so if I understand this correctly, the key change here is the change from using different creation methods for bitmap results to using a unified `BitmapResultFactory` interface in order to facilitate better metrics collection of the bitmap operations. I think that makes some good sense overall, but I'm a bit unclear on what the future intentions of `BitmapResultFactory` are. Should there be specialty methods for every type of bitmap operation someone might want to do? How do you want to handle future additions to that factory?  Is there a way someone can have a custom `BitmapResultFactory` and not have it break in arbitrary future versions?",non_debt,-
spark,16220,review,91646897,add comment explaining why this should not be inside a lock. so that this is less likely to regress in future.,non_debt,-
beam,2983,comment,300281555,"Its just that the message from the failure will not be the top exception so users will be:
failed to validate MyIface.class
...lines of stack trace...
actual validation failures
...some more lines of stack trace...",non_debt,-
airflow,7163,comment,659601976,Great work @roitvt,non_debt,-
beam,9730,review,331592994,"use `re.search()`.  also, I don't think this needs to be private, so remove the leading underscore.
maybe add a comment like:  ""we don't use json.loads to test validity because we don't want to propagate json syntax errors downstream to the runner""",documentation_debt,outdated_documentation
hbase,1077,review,368823636,May be good to update doc for `hbase.coprocessor.regionserver.classes` in section `Restricting Coprocessor Usage` of cp.adoc?,documentation_debt,outdated_documentation
spark,29402,description,0,"This PR proposes to `include` `_images` and `_sources` directories, generated from Sphinx, in Jekyll build.
**For `_images` directory,**
After SPARK-31851, now we add some images to use within the pages built by Sphinx. It copies and images into `_images` directory. Later, when Jekyll builds, the underscore directories are ignored by default which ends up with missing image in the main doc.
Before:
After:
**For `_sources` directory,**
To show the images correctly in PySpark documentation.
No, only in unreleased branches.
Manually tested via:",documentation_debt,low_quality_documentation
apisix,3710,review,584531199,"Hi, I noticed that there has a red title ü§î we will not use it.
Also, we have 40+ users and there have 25 users in this picture, we need all users.",non_debt,-
spark,126,review,11357428,Why not just rename `body` to `response` in the first place?,non_debt,-
nifi-minifi-cpp,71,comment,291787919,@phrocker Just wanted to confirm this was done before a final review.  I think it likely makes sense to get this one in before we merge #73,non_debt,-
druid,8209,review,310329604,this method has been removed,non_debt,-
ozone,1200,description,0,"Make `curl` less verbose to avoid connection-related messages being mixed into the response from Recon.
https://issues.apache.org/jira/browse/HDDS-3958
Verified output in [acceptance test](https://github.com/adoroszlai/hadoop-ozone/runs/869256000):",non_debt,-
camel,1603,description,0,@tristantarrant may you check if I missed something on infinispan side,non_debt,-
airflow,6311,comment,608306828,Still want to revisit this.,non_debt,-
nifi,4067,summary,0,NIFI-7178 - Handle the case when schema is not available,non_debt,-
superset,1253,review,81885223,"Getting linting error here saying component should be written as a pure function, but in stateless components 'this.props' can't be accessed",non_debt,-
spark,4583,comment,74452766,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/27536/
Test PASSed.",non_debt,-
zeppelin,2811,comment,402938821,"@jan0sch flink 1.5 has a critical issue in scala-shell for yarn mode. I have fixed it in FLINK-9554, and will update this PR to 1.5.1 if it is released before this PR merging.",non_debt,-
spark,13860,comment,229268989,: ) Just found one in `AstBuilder.scala`. Let me know if anything I still missed.,non_debt,-
druid,2800,review,60473748,what's the point of replacing with null ? that will raise NPEs for no reasons,non_debt,-
beam,6707,review,229053783,Is this a debugging println? Should this line be removed?,code_debt,low_quality_code
spark,28085,review,407334754,17165658-28085 review-407334754,non_debt,-
kafka,2492,review,100844023,No sure -- we also have upgrade info for 0.9. and 0.10 -- maybe this need a general clean-up (so not part of this PR)?,code_debt,low_quality_code
beam,2220,comment,286517853,"I'm not sure if the Jenkins job is running distributed or in one slave. If it's the later case, I think it should be bounded by max number of cores which is four.
@jasonkuster can you provide more details?",non_debt,-
beam,3819,description,0,"1. remove some logs on critical path
2. register ""TimestampedValue"" in Kryo to reduce the serialized size of event value
---",non_debt,-
arrow,3006,review,235668748,I've fixed it.,non_debt,-
trafficcontrol,4816,comment,648252948,"hmm, I thought I did.  Maybe I didn't commit",non_debt,-
flink,11415,review,395585557,Yeap...,non_debt,-
couchdb,3015,review,488882226,"It was carried over from the old couch_replicator_scheduler_job.erl and then from couch_replicator.erl 
https://github.com/apache/couchdb-couch-replicator/commit/b48d7bdc49d107f33d96f08603006a1c9edc322f#diff-ba1cca81bdc216835256f72cc6a72fa5R374
Beyond that not sure about its origin. Technically it is possible, say, for a couch_replicator_auth plugin to link to a process that then exit normally so we end up in this part of the code and it won't be a worker, changes reader or any other known process in this module. So perhaps we should change to a comment indicating that instead of leaving the commented code in there?",documentation_debt,low_quality_documentation
camel,623,summary,0,CAMEL-9166 for 2.15.x,non_debt,-
storm,3379,review,583943296,Why do we want to remove this check?,non_debt,-
guacamole-client,455,review,355073430,Yessir!  Fixed,non_debt,-
arrow,4500,review,291834025,Yes. I've added it.,non_debt,-
carbondata,2153,comment,381357670,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/5022/",non_debt,-
arrow,7692,review,452483726,does this reference need to be mutable?,non_debt,-
incubator-pinot,4235,review,288316724,Did you look into the possibility of having a single validateIndexingConfig method?,non_debt,-
spark,19208,review,148859043,Only use Since annotations for public APIs,non_debt,-
flink,12315,comment,633776377,Thanks a lot for the review @tillrohrmann . I've updated PR and verified it with Yarn 3.1.0.,non_debt,-
ozone,1916,review,573453733,"Yes, we need to do iniilization.",non_debt,-
flink,11321,review,401445976,Why not support other function definitions?,non_debt,-
arrow,5213,review,318643171,"It would be nice to have some javadocs for the new methods, although they are fairly self-explanatory",documentation_debt,outdated_documentation
netbeans,2617,description,0,This is a new feature in the protocol version 3.16.0.,non_debt,-
hadoop,2076,review,441558016,"hmm. how about just calling toString, which queries them anyway -doesn't it?",non_debt,-
flink,6076,review,190841926,"+1 for getting this into the docs. I offer some grammatical improvements. Also, is it correct to describe ""operators are required to completely process a given watermark before forwarding it downstream"" as a general rule, meaning that it might have exceptions, or should we simply say ""operators are required ..."" without adding this caveat?
I changed behaviour to behavior because most of the docs seem to be using American spellings rather than English ones, but I'm not sure if we have a policy regarding this.",documentation_debt,low_quality_documentation
flink,13306,review,483439597,"do you means: static final ImmutableMap<FunctionDefinition, Function<CallExpression, OrcSplitReader.Predicate>> FILTERS ?",non_debt,-
lucene-solr,2022,review,515170806,"right; they may choose an indexing approach based on a strategy (the names indicate algorithm/metric, but could be anything), or NONE. That can be useful for calculating vector score only for the purpose of ranking hits matched by other means.",non_debt,-
spark,10745,review,49801194,No this is what was supported in the old SqlParser.,non_debt,-
apisix,2329,review,495666587,"my fault,I should remove irrelevant code,actually,test_base.py has nothing to do with this PR",code_debt,dead_code
nifi,71,comment,130314880,"Why is propertyMap marked volatile?  The value is only ever set once at construction time.
If the answer is because of thread safety, the contents of the HashMap are not ""protected"" just because the reference to the map is marked volatile.  puts/gets to the map do not inherit the memory barrier protections associated to the volatile reference.  c.f.  http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap
Maybe a review of the concurrency issues of this processor is in order before accepting this merge request?  I'm pretty sure that, even though the class will mostly behave correctly since values are set during OnScheduled and OnStopped, these are not ""safely published"" to the map.  While unlikely, other threads could potentially see stale values in this map.
Either this class should likely be using ConcurrentHashMap here, or it should republish an entirely fresh map by calling ""new HashMap()"" instead of ""clear()"".",requirement_debt,non-functional_requirements_not_fully_satisfied
karaf,512,summary,0,add Eclipse's .apt_generated_tests/ directory to .gitignore,non_debt,-
airflow,10346,summary,0,Breeze was slightly too chatty when there was no dirs created,code_debt,low_quality_code
orc,163,description,0,Treat 0 byte files as an empty ORC file with schema of struct<>.,non_debt,-
geode,71,comment,171934710,"Hey Vince,
Thanks for reviewing the pull request.
I knew about the license problem regarding project MultiAxisChartFX but we'll need to create a new JIRA to address this, instead of reimplementing the logic within GEODE-342, don't you think?. We could start doing that once the build process is standardized through gradle. Please let me know.
Cheers.",non_debt,-
camel,700,comment,160263784,Thanks for the new PR. It has been merged. Do you mind closing this?,non_debt,-
brooklyn-server,899,comment,346040396,"looks great.  really nice tests.
a few minor comments, that's all.",non_debt,-
trafficcontrol,3760,comment,516056868,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/4045/
Test PASSed.",non_debt,-
spark,9430,description,0,Add label support in include_example.,non_debt,-
incubator-brooklyn,164,comment,56559915,very slick,non_debt,-
pulsar,1821,comment,394024643,retest this please,non_debt,-
kafka,8883,summary,0,KAFKA-9896: fix flaky StandbyTaskEOSIntegrationTest,test_debt,flaky_test
tajo,426,review,26902968,17971138-426 review-26902968,non_debt,-
ignite,6509,review,279779959,31006158-6509 review-279779959,non_debt,-
zookeeper,1506,summary,0,Resources are auto closed within try catch block.,non_debt,-
incubator-pinot,6451,review,559757218,19961085-6451 review-559757218,non_debt,-
flink,4561,comment,324585254,I merged. üëå Could you please close this PR?,non_debt,-
spark,28874,comment,648303691,ok to test,non_debt,-
spark,21119,review,184839158,Use `.. note::`?,non_debt,-
beam,9032,comment,510766791,I am not seeing Java precommit tests are triggered. I am less familiar on how does that is enabled. Guessing have to set at here: https://github.com/apache/beam/blob/master/build.gradle#L132,non_debt,-
spark,30144,review,605525533,`groupingSetExprs.flatMap(_.asInstanceOf[GroupingSet].groupingSets)` -> `groupingSets.flatMap(_.groupingSets)`?,non_debt,-
incubator-brooklyn,866,comment,135365840,"@aledsage Thank you for the elaborative comments!
I fixed the addressed issues and added new config for powershell commands for: post-install, pre-launch, post-launch
You can review it again now.",non_debt,-
spark,28403,comment,621539070,"Thanks, merged to master.",non_debt,-
zeppelin,2194,comment,289690375,Tested it out and it works very well!,non_debt,-
beam,5217,summary,0,Upgrade Gradle to 4.7,non_debt,-
cassandra,716,review,476643686,"Why primitive tuples only?
` CREATE TABLE collect_further_things (   k int PRIMARY KEY,   v tuple<set<int>, map<int,set<float>>, float> );`",non_debt,-
helix,1494,review,515570884,"nit: offline ""duration"" for consistency",non_debt,-
shardingsphere,3619,description,0,"For #3602 .
Changes proposed in this pull request:
- Refactor SyncTaskController
- Refactor LogManager
- Refactor DataSyncTask",non_debt,-
ignite,6554,review,341580318,Points,non_debt,-
carbondata,954,review,118870584,This won't work if there is no alias to table. if there no alias then it should be passed as none,non_debt,-
zookeeper,684,review,238789004,"Yes, I think we randomly chose one, it depends on how much data you have, we'll update the doc for the best practice. Also there is metric to show the cache hit rate, if it's too low, maybe we need to raise the cache size.",documentation_debt,outdated_documentation
flink,11095,review,379828410,"It seems to me we could simply call `Files.newOutputStream(restoreFilePath)` here, which perfectly matches the ""overwrite"" semantic.",non_debt,-
flink,11393,comment,599385217,"Ping @JingsongLi for review.
@fhueske It'd be great if you can have a look too.
Thanks.",non_debt,-
flink,9363,description,0,"-->
Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion.
1. Introduces a RelWriter `RelDisplayNameWriterImpl` to reuse code of ""explainTerms"" to generate operator names
2. Fix some operator names are not set in blink planner
This change is a trivial rework / code cleanup without any test coverage.",code_debt,low_quality_code
shardingsphere,9687,summary,0,Use single SQL to load index names for MySQL meta data loading,non_debt,-
spark,157,comment,37761089,That is weird - you can see the use of SPARK_JAVA_OPTS just a few lines above in the patch you submitted.,non_debt,-
superset,13090,review,575157301,"If this is supposed to return zero rows, would a more obvious false where clause be more descriptive? Something like `where 1 = 0`? The other examples above are easy to reason about as they have a static query that doesn't reference a table.",non_debt,-
carbondata,1386,comment,333775342,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/988/",non_debt,-
trafficserver,5000,comment,464885742,"I had to redo most of the formatting after adding the ""no empty lines before comments"" setting, the previous run had damaged the formatting in a way that it could not be fixed even with this config change.",non_debt,-
spark,29686,comment,689199606,Is it going to update in the PR description but forgotten?,non_debt,-
pulsar,2120,summary,0,shading dependencies in pulsar client,non_debt,-
arrow,2202,comment,401787754,"+1, failure is due to Plasma, I was on a slightly too old master. Not relevant for this PR.",non_debt,-
cloudstack,4131,comment,707804396,Packaging result: ‚úîcentos7 ‚úîcentos8 ‚úîdebian. JID-2169,non_debt,-
kafka,6680,description,0,"* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`",code_debt,complex_code
attic-apex-malhar,24,comment,140223786,Changed author to MalharJenkins.,non_debt,-
spark,17130,review,104006906,"~I think it is better to explicitly declare the data instead of manipulating strings, that way it is very clear what the input data is for the example.~ On second thought, never mind this comment - it's pretty clear the way it is",code_debt,low_quality_code
incubator-doris,1297,review,293240865,Already added.,non_debt,-
tvm,4138,comment,544311340,A bit curious about how topi calls are collected during optimizationÔºåis it through certain pass?,non_debt,-
trafficserver,7279,description,0,"Some messages get excessively noisy under high traffic conditions if
something about their mechanism goes wrong. The pipe logging feature,
for instance, will emit warning and error messages on every single log
event if the reader goes down or the pipe buffer fills up. This can
result in thousands of log messages being emitted per second, which
makes reading the logs difficult and causes disk space issues.
This commit addresses this issue by adding throttled versions of the
common logging messages so they only emit a message on some set
interval (60 seconds as a default). The following functions are added:
SiteThrottledStatus
SiteThrottledNote
SiteThrottledWarning
SiteThrottledError
SiteThrottledFatal
SiteThrottledAlert
SiteThrottledEmergency
As a bonus, these are implemented using a generic Throttler class which may also be
useful in other applications where throttling is desired.",design_debt,non-optimal_design
spark,17250,comment,285797389,Forgot to stop the ```StreamingContext``` added in ```KinesisDStreamBuilderSuite```. Updated the code to stop the context after all tests have run.,non_debt,-
spark,10469,review,49408899,Note that this and other classes for read/write are Experimental,non_debt,-
servicecomb-java-chassis,947,review,225761072,io.vertx.core.http.HttpClientOptions#DEFAULT_TRY_USE_COMPRESSION,non_debt,-
trafficserver,6001,comment,539771774,"Can you add a doc ?
Also can you update the release notes if we add this new feature to the master - https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html ?
How about remap plugin? can you check the corresponding symbols for remap plugin as well?
What other features are you planning for this ?",documentation_debt,outdated_documentation
storm,1761,description,0,"... not called by multiple threads at a time
See https://issues.apache.org/jira/browse/STORM-2184",non_debt,-
pulsar,9292,review,566071650,"isn't it better to throw UnsupportedOperationException ?
also, if I retrieve only a subset of the data, how do I know that there would be more ranges ?",non_debt,-
ozone,1870,comment,771303816,cc @amaliujia,non_debt,-
carbondata,3142,comment,470833635,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2669/",non_debt,-
arrow,7507,review,562065051,"It doesn't seem necessary to put this in `RecordBatchStreamReader`, you can move it to the implementation class.",architecture_debt,violation_of_modularity
incubator-weex,1550,summary,0,"Performance info is not an error, change it to debug info",non_debt,-
fineract,1599,description,0,"This PR contains the following updates:
---
:date: **Schedule**: At any time (no schedule defined).
:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.
:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.
:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.
---
---
This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#github/apache/fineract).",non_debt,-
kafka,7848,description,0,2211243-7848 description-0,non_debt,-
openwhisk,4031,review,255882825,Note that the `namespace` and `action` would remain same for all test. Though we do a cleanup after each run it may be better to use different name for each test run,code_debt,low_quality_code
openwhisk,638,summary,0,Document the Watson speechToText action,non_debt,-
nutch,131,description,0,206370-131 description-0,non_debt,-
arrow,2314,comment,407287310,"# [Codecov](https://codecov.io/gh/apache/arrow/pull/2314?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2314?src=pr&el=continue).",non_debt,-
spark,11006,comment,178279347,"LGTM, other than the naming issues (StandingQuery, etc. in the code)",code_debt,low_quality_code
druid,9618,description,0,"Fixes #9617 
https://github.com/apache/druid/blob/master/dev/committer-instructions.md#pr-and-issue-action-item-checklist-for-committers. -->
In each section, please describe design decisions made, including:
 - Choice of algorithms
 - Behavioral aspects. What configuration values are acceptable? How are corner cases and error conditions handled, such as when there are insufficient resources?
 - Class organization and design (how the logic is split between classes, inheritance, composition, design patterns)
 - Method organization and design (how the logic is split between methods, parameters and return types)
 - Naming (class, method, API, configuration, HTTP endpoint, names of emitted metrics)
-->
This PR has:
 kafkaEmitterConfig
PS: We built a custom jar and have been using the custom emitter jar in our production cluster. It works well with ssl enabled Kafka.",non_debt,-
kafka,2303,comment,270284340,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/466/
Test PASSed (JDK 8 and Scala 2.11).",non_debt,-
carbondata,2634,comment,412819112,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/7909/",non_debt,-
skywalking,4443,comment,597527424,"It is packaged locally. Run the command you put in the CI control file, all things run automatically on MacOS and Linux.",non_debt,-
flink,15195,review,594017064,The case will not happen because the node will not be translated to WindowJoin if `WindowSpec` is different.,non_debt,-
incubator-heron,2793,review,176873377,"Math.abs(hash) can give negative result? Sounds strange to me.
bitmask is dangerous because it assumes the var is 32 bits. In case someone changed the data type it can cause other problem.",non_debt,-
cloudstack,2464,comment,367304540,@borisstoyanov a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.,non_debt,-
beam,13161,comment,726411143,@dpcollins-google - Would you be able to update based on the comments?,non_debt,-
camel-quarkus,1137,summary,0,Bump Testcontainers to version 1.14.1,non_debt,-
arrow,9777,summary,0,ARROW-12051: [GLib] Keep input stream reference of GArrowCSVReader,non_debt,-
spark,22007,comment,411514466,"This change looks non-risky to me.
cc @swoen @HyukjinKwon",non_debt,-
spark,11673,comment,196234027,cc @marmbrus,non_debt,-
incubator-pinot,6239,review,518946846,Can we put `@Deprecated` annotation instead of removing the function?,non_debt,-
spark,20681,review,171910649,ok,non_debt,-
carbondata,4039,comment,745974586,retest this please,non_debt,-
beam,13411,description,0,"The original `AwsOptions` has a flat `region`, which is in conflict with the one in `DataflowPipelineOptions`
This PR renames the flag, matching the ver1 sdk (io.aws).
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.",non_debt,-
arrow,9141,comment,756918737,"https://github.com/bkietz/arrow/runs/1670186993 shows a couple of other warnings still, though they're not related to the recent dataset work",non_debt,-
ignite,1558,description,0,Test fixed and extended. Added notes to GridToStringBuilder about infinite looping in additional values.,non_debt,-
superset,3346,review,134524525,39464018-3346 review-134524525,non_debt,-
kafka,7881,summary,0,WIP MINOR: move ZK ACL lookup outside of inWriteLock in AclAuthorizer,non_debt,-
ignite,8686,review,586497377,Check added,non_debt,-
cloudstack,3795,review,363682565,Why not use Arrays.asList()?,non_debt,-
superset,3852,comment,344171641,Yeah that looks sound.,non_debt,-
zeppelin,80,comment,111033924,"@doanduyhai 
Also i'd like to know if you're planning to add more commit here for Java to Scala and some improvements. Or if you prefer to merge it first and make new pull request for further improvements.",non_debt,-
servicecomb-java-chassis,1761,summary,0,[SCB-1917]add a test case for testing provider invoke it's own servic‚Ä¶,non_debt,-
zookeeper,1417,comment,665952705,"okay, so I'll have to fix a few checkstyle issues :)",code_debt,low_quality_code
pulsar,9481,comment,775964246,/pulsarbot run-failure-tests,non_debt,-
spark,2931,review,19454808,"I don't think it makes sense to tie this to WriteAheadLogFileSegment. On one hand the naming HDFSBackedBlockRDD is supposed to be general, on the other you tie it to recovery through the use of WriteAheadLogFileSegment.",non_debt,-
storm,237,review,24169370,Everything at the top of this chunk that's removed is included in the new bit (except for bootstrap),non_debt,-
spark,22960,review,231413853,I think we can just get rid of it. I can't imagine both functions are specifically broken alone in `selectExpr`.,code_debt,complex_code
trafficserver,7142,review,488118370,0 means that it will retry 1 time.  I didn't change the current difficult to understand behavior (see above @maskit comment).,non_debt,-
flink,59,description,0,"Tuple types can be returned by the KeySelector. Enabling tuple types to be used as KEY for grouping.
Added test cases using TupleComparatorTestBase. Testing Tuple3<Tuple2,Tuple2,Tuple2>.",non_debt,-
thrift,1039,comment,238129818,"The `thrift` directory makes sense.
The test # 4 was relying on `sleep` in test script that is inherently flaky.
I've removed a double pclose in parent process code so we'll see if it fixes the problem in #368 result.
Another problem is that Appveyor CI is failing due to include error.
Not sure why it's only happening on Windows.",test_debt,flaky_test
flink,12176,review,426165664,do not need `ifPresent`,code_debt,complex_code
ambari,2048,description,0,"The use of 'zookeeper' as the Livy recovery store does not seem to be supported by Ambari. The Ambari script is trying to create a HDFS directory with the value of livy2-conf/livy.server.recovery.state-store.url which is actually a Zk quorum. Ideally, it needs to look at livy2-conf/livy.server.recovery.state-store and only create the directory if it is 'filesystem'. In this case, the store is 'zookeeper'.
Manually tested.",non_debt,-
arrow,8371,comment,706095916,I can happily report that the iterator is the only issue and the build passes locally for me when I add a `const` at the end of the declaration and to all implementations.,non_debt,-
iceberg,1083,review,438894029,I think the best way to verify is by checking that the test fails if we remove `ignoreResiduals()` from the action.,non_debt,-
dubbo,4655,summary,0,optimize some code styles,code_debt,low_quality_code
thrift,221,comment,57221845,merged,non_debt,-
shardingsphere,7753,summary,0,Decouple LogicSQL and schema,non_debt,-
spark,16464,description,0,"spark.lda passes the optimizer ""em"" or ""online"" as a string to the backend. However, LDAWrapper doesn't set optimizer based on the value from R. Therefore, for optimizer ""em"", the `isDistributed` field is FALSE, which should be TRUE based on scala code.
In addition, the `summary` method should bring back the results related to `DistributedLDAModel`.
Manual tests by comparing with scala example.
Modified the current unit test: fix the incorrect unit test and add necessary tests for `summary` method.",non_debt,-
airflow,13616,comment,758630982,"Yeah I can look into that. Maybe we should create an issue for this so that we agree on the how (terminology, objects, location in the values.yaml) before starting the dev? Wdyt ?",non_debt,-
incubator-mxnet,14534,review,271872150,34864402-14534 review-271872150,non_debt,-
flink,5063,comment,347451529,"I assume you had also tried periodic watermarks in your setting. I'm curious why they didn't work for you. 
A periodic watermark assigner extracts the timestamp from each record and just emits a watermark when it is asked. From a correctness point of view, this should be the same as punctuated watermarks, just with lower overhead and higher watermark latency.",non_debt,-
openwhisk,2151,review,114561790,"The `factory()` returns a `Future` which means it needs an `ExecutionContext`. Apparently, the `factory` passed in the class constructor has no `ExecutionContext` in its parameter list. Which `ExecutionContext` will it use?
I would expect that it uses the same `ExecutionContext` as the `ContainerProxy` which is an actor - but `ExecutionContext` is wired in `factory`.",non_debt,-
kafka,5498,comment,460004213,KAFKA-7235 has been merged. Does that mean that this is unblocked or is there more to be done?,non_debt,-
arrow,3816,summary,0,ARROW-3770: [C++] Validate schema for each table written with parquet::arrow::FileWriter,non_debt,-
apisix,336,summary,0,change(CLI): removed duplicated `include` option.,code_debt,duplicated_code
incubator-mxnet,5151,description,0,"This PR adds more comments for ndarray operators and refactors ndarray API docs.
A preview is available at http://ec2-54-152-159-175.compute-1.amazonaws.com///api/python/ndarray.html
The refactor consists
1. operators that should be deprecated are removed from the summary list, including
2. we are agreed to use lower cases for array operators, so several alias are added.
3. bug fix for `rint` and `fix`. 
4. updated mxnet.css
For example, we used several input names including
we can use either 1) `data`, or `data0`, `data1` for multi-input, or 2) `a`, `b` to stay the same with numpy
Also we used `axis`, `axes`, `dim` to refer to dimensions. Suggest to use `axis` for single dimension and `axes` for multi-dimensions. 
some additional changes include `ret_typ` -> `ret_type` or `rtype` for `topk`.
1. comments for neural network layers
2. improve `symbol.md`",non_debt,-
kafka,8701,review,429551798,Maybe we should just say segment deletion? We can also delete segments during recovery and such. Could that not trigger this path?,non_debt,-
apisix-dashboard,1571,comment,795057724,"# [Codecov](https://codecov.io/gh/apache/apisix-dashboard/pull/1571?src=pr&el=h1) Report
Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags#carryforward-flags-in-the-pull-request-comment) to find out more.
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/apisix-dashboard/pull/1571?src=pr&el=continue).",non_debt,-
gobblin,2459,review,219005113,Use {} for formatting.,non_debt,-
carbondata,2209,review,184917127,"I don't think it is required to have a separate constructor, if (dictionary == null && !isDirectDictionary) then it becomes nodictionary",non_debt,-
trafodion,1532,comment,383765758,"Yes, I will review tomorrow.",non_debt,-
flink,7186,comment,453921155,"Thanks @Zhijiang. I agree with you that a shuffle manager will be a better solution for this issue in Flink 1.8. If more users request this before they can upgrade to Flink 1.8, we could still merge this PR to Flink 1.5~1.7 branch.",non_debt,-
arrow,9785,review,604887936,changed,non_debt,-
reef,641,review,44857751,"I don't think we should be calling ToUniversalTime() here. Lets keep this exactly what we get from the client? I think we should not get into the timezone business here. If it is absolutely necessary then we can consider changing LastModifiedTime in FileStatus to DateTimeOffset.
Thoughts?",non_debt,-
spark,25007,review,306928380,"Actually an identifier of the shuffle itself, not the stage, right? If you reuse a shuffle, you get the same shuffle id, but different stage id.",non_debt,-
druid,4131,comment,292841747,"@leventov, in reply to:
This patch won't preserve extension compatibility anyway, since Query gained `withQueryMetrics` but BaseQuery doesn't provide a default implementation.
But also: is there a nice migration path from this change now, to something that would be a ""better"" design for 0.11.0? From your discussion with @himanshug, it sounds like there isn't, and in 0.11.0 we'd just want to remove these methods we're adding now and replace them with something else. I think if that's the case, it's fine to make the ""better"" change now and have the next release after 0.10.0 be 0.11.0.",non_debt,-
druid,2698,review,57078970,this is in a separate section called additional firehoses,non_debt,-
cxf,275,summary,0,[CXF-7374] Fix for jpa refreshToken writeLock,non_debt,-
incubator-mxnet,2329,review,65903056,I have changed it. :),non_debt,-
camel,508,summary,0,CAMEL-8640 on Camel 2.15.x,non_debt,-
airflow,4381,comment,450809046,@BasPH PTAL https://github.com/apache/incubator-airflow/pull/4421,non_debt,-
lucene-solr,823,comment,527765667,"Folks, post the discussion above, I am assuming this is ready to merge?",non_debt,-
beam,12040,review,445090535,I'll do that in a follow-up so it's easier to test.,non_debt,-
bookkeeper,1059,description,0,"Descriptions of the changes in this PR:
This is cherry-pick from yahoo repo of branch yahoo-4.3.
original commit is:
https://github.com/yahoo/bookkeeper/commit/42bdc083
Release addEntry-Bytebuf on readOnlyBookie to prevent memory-leak",design_debt,non-optimal_design
trafficserver,3322,review,176559777,Do me a favor and move both `MemSpan` and `MemArena` so the test source files are in alphabetically order.,architecture_debt,violation_of_modularity
lucene-solr,1155,comment,595607002,I missed the fact that `mergeFinished` is executed under IndexWriter lock. I will dig into this again. Please ignore my previous comments.,non_debt,-
airflow,2628,review,148506473,don't skip. Mock should be present,non_debt,-
incubator-mxnet,10900,review,188720460,I would suggest to simply use `mx.image.imread()` and then call `.asnumpy()` before plotting. There shouldn't be a need of a dependency on `cv2` that way.,code_debt,complex_code
geode,4161,comment,546551736,"I had to rebase the branch to fix merge conflicts, but not all feedback/requests for changes have yet been applied.",non_debt,-
beam,13118,review,509433018,"I've moved it into the test class where it is used. I would still like to leave the alteration of `DataflowRunner` out of this PR, since my other PR that just adds a `checkState` illustrates that the `DataflowRunner` batch view overrides result in a corrupted graph that sort of works by luck. I don't want to disturb that potentially sensitive situation.",non_debt,-
kafka,8000,review,374961789,Why these values need to be atomic?,non_debt,-
spark,29367,review,466633630,Good catch. I remember the locking here had a difficulty when I was doing development but @dongjoon-hyun added a test for it so I'll try and minimally scope the lock and make sure it passes.,non_debt,-
cloudstack,2793,comment,440772450,Packaging result: ‚úîcentos6 ‚úîcentos7 ‚úîdebian. JID-2451,non_debt,-
hadoop,1823,summary,0,HADOOP-16794 S3 Encryption keys not propagating correctly during copy operation,non_debt,-
airflow,14581,comment,790768377,"That column name is wrong now though -- I wonder if it is worth updating/renaming that to ""last_parsed_time"" or ""last_seen_time""?",non_debt,-
incubator-mxnet,7772,review,137280575,Thanks for the contribution. I don't think this will work on GPU though as this kernel is shared by CPU and GPU..,non_debt,-
beam,3113,comment,301301323,"Thanks @hepeknet ,
In case you want to contribute another AutoValue PR, HBaseIO does not have autovalue, you can create a JIRA/PR if you feel like doing this too.",non_debt,-
spark,29136,description,0,"This PR aims to remove Python 2 test case from K8s IT.
Since Apache Spark 3.1.0 dropped Python 2.7, 3.4 and 3.5 support officially via SPARK-32138, K8s IT fails.
No.
Pass Jenkins K8s IT.",non_debt,-
spark,28708,review,440410753,Got it~,non_debt,-
kafka,4691,description,0,A few small logging improvements which help debugging replication issues.,code_debt,low_quality_code
incubator-doris,1694,comment,524725582,99919302-1694 comment-524725582,non_debt,-
nifi,1036,review,81411506,"Not an incredibly big deal, but getSupportedPropertyDescriptors() is called very often (most often in validation during a UI Refresh) which causes lots of ArrayLists to be created.  I prefer the way DistributeLoad handles this, by creating one ArrayList and reusing it, while also using an AtomicBoolean to know when it should be recreated.",code_debt,low_quality_code
accumulo,1182,comment,499659659,"@ctubbsii , sorry I little confused again.  I guess I didn't reread this new ticket referenced.  I was working off of accumulo-proxy.  Do you want me to do all of this in accumulo-proxy#3?  OR if accumulo-proxy is what it the goal and you are updating it then I can just refork or pull after your update and pull things together.",non_debt,-
flink,979,review,36167357,These values should be configurable. Thus they should be definable in the `flink-conf.yaml` file.,non_debt,-
kafka,8662,comment,628329507,"Nooooooo `EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[true]` still had one failure on the Java 8 build üò≠ 
But it failed on a different line which seems more in line with real flakiness. FWIW I ran this locally 40 times without failures (technically 80 in total for both true/false variations) ... I think it's worth still merging this PR and we can continue investigating it from there.",non_debt,-
cloudstack,2464,comment,368872305,@rafaelweingartner I couldn't test the view with having existing remote vpns configured. But my assumption was that the api is working identically as other list apis returning the results of the current owner.,non_debt,-
beam,14300,comment,804418896,Run Java Postcommit,non_debt,-
spark,30521,review,532273012,"I see DataFrameReader and DataFrameWriter have been already mixed up. I won't insist on `DataStreamReader.table` as `DataFrameReader.table` has been living from 1.4.0, but I still don't agree we should have another trigger entry except `start()` for `DataStreamWriter`. That was a mistake and this is a great chance to fix before we release.",non_debt,-
incubator-weex,2951,summary,0,[iOS] Rename the backgroundColor property name.,non_debt,-
kafka,3547,review,128396998,"Perhaps it would be more palatable if, instead of exiting, we just log a warning when the wait time expires and not all queries have been matched?",non_debt,-
druid,7915,comment,507899421,"I think enough +1's between here and dev thread, going to merge",non_debt,-
spark,12825,comment,216076628,@jkbradley,non_debt,-
spark,1111,comment,46391465,Never mind. I had to close the pull request. I thought about it. The ccAccumulator is not accessible from the vprog which was my goal. I'm going to have to use a broadcast. I will have an update for tomorrow.,non_debt,-
cloudstack,4248,review,467866137,"50 line, two for loops and quadruple nested blocks. please dissect.",non_debt,-
cloudstack,1295,description,0,"Added quotes to prevent syntax errors in weird situations.
Error seen in mgt server:
Cause:
Somehow a nic was missing.
After fix the script can handle this:
The other states are also reported fine:
While at it, I also removed the INTERFACES variable/constant as it was only used once and hardcoded the second time. Now both are hardcoded and easier to read.
This is the same as PR #1249 except it is against `4.7`.",code_debt,low_quality_code
spark,12539,review,60696040,Why `transformDown`? Wouldn't `transformUp` allow us to resolve most outerReferences in one pass?,non_debt,-
spark,3099,review,20122266,not needed?,non_debt,-
carbondata,2502,comment,405482490,"SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/5882/",non_debt,-
couchdb,2843,description,0,Two simple backports to 3.0.x to make life friendlier for operators and devs. These won't be mentioned in the relnotes.,non_debt,-
orc,116,review,114468791,That sounds good. We also need suggestion from @omalley about the coding style.,non_debt,-
netbeans-website,30,summary,0,Update index.asciidoc,non_debt,-
beam,6511,description,0,"Prevent the duplication of encode/decode cycle in wasted call to groupByKey in spark streaming. It is a waste because after the groupByKey is a call to updateStateByKey which will ensure that the data is shuffled to the correct processing location.
------------------------
It will help us expedite review of your Pull Request if you tag someone (e.g. `@username`) to look at it.
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) </br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | --- | ---",non_debt,-
tajo,674,summary,0,TAJO-1736: Remove and improvement a unnecessary method(getMountPath()).,code_debt,dead_code
myfaces-tobago,352,summary,0,chore(deps): bump animal-sniffer-maven-plugin from 1.18 to 1.19,non_debt,-
incubator-heron,918,review,67564368,"""Check to ensure that the local version of the Heron topology compiles by deleting your local `~/.heronapi`, updating the project `pom.xml` file, and pulling from Maven Central""",non_debt,-
iceberg,1326,review,473297905,"Agreed. We always merge by squashing the entire PR into a commit, so we do get a linear history in master.",non_debt,-
gobblin,2867,comment,574426534,"# [Codecov](https://codecov.io/gh/apache/incubator-gobblin/pull/2867?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-gobblin/pull/2867?src=pr&el=continue).",non_debt,-
nifi-minifi-cpp,1011,review,596175075,See below.,non_debt,-
attic-apex-malhar,516,review,95512203,Would BucketTimeMapper be a more appropriate name?,non_debt,-
attic-apex-core,555,comment,315202571,@sgolovko Are the latest changes in?,non_debt,-
incubator-mxnet,17808,comment,605556855,"@vexilligera as discussed offline, lets try testing locally for WIN_GPU and WIN_GPU_MKLDNN build 10 times each (since 1 run takes 20-30mins) to come up with some basis... (ideally would have tried 100 times but given the resource & time constraints)",non_debt,-
spark,7012,comment,115346040,@prakashpc I actually fixed the packages thing in #7022 -- I'll try out the file separator fix today and get back to you.,non_debt,-
spark,7129,comment,122069744,@JoshRosen @rxin Hi folks - any chance of getting a review? Thanks!,non_debt,-
spark,9594,summary,0,[SPARK-11629] [ML] [PySpark] [Doc] Python example code for Multilayer Perceptron Classification,non_debt,-
groovy,517,description,0,"The analogy between '*' and breadthFirst() traversal seems wrong. The former only traverses one level, while the latter searches through the next levels as well. See http://stackoverflow.com/questions/42985716/groovy-xml-tree-traversal-breadthfirst-method-using-as-syntactic-sugar:
    def books = '''\
    def response = new XmlSlurper().parseText(books)
    def bk = response.'*'.find { node ->
       node.name() == 'book' && node['@id'].toInteger() == 2
    }
    assert bk.empty
    def books = '''\
    def response = new XmlSlurper().parseText(books)
    def bk = response.breadthFirst().find { node ->
       node.name() == 'book' && node['@id'].toInteger() == 2
    }
    assert bk.title == 'bar' // bk is no longer an empty list of children",non_debt,-
trafficserver,6038,comment,545153161,"As it turns out any of the tests using tcp_client.py or netcat assume the half open semantics.
The slice plugin also does shutdown read and shutdown write.  I think that is probably not what you want to do since it limits you to only one request from client connection.  I removed the calls to TSVConnShutdown here.",non_debt,-
flink,8480,review,285775675,"This is a private method and has its unit functionality. While being called by only one place, I don't see any harm to keep it this way.",non_debt,-
tvm,891,comment,364725680,"has updated as your comments, @tqchen please review again",non_debt,-
nifi,1918,review,122836798,I knew that code should have been simpler :) updated it to use the readValueAsTree,code_debt,complex_code
camel,3202,description,0,Ensures that the client configuration is set on the SQS Client Builder when it's present.,non_debt,-
kafka,1095,review,60963008,"This can fall through because the constructor supports version 0 and 1. The version is passed as the last parameter. 
This is related to the discussion here: https://github.com/apache/kafka/pull/1095#discussion_r60758124",non_debt,-
spark,42,review,10368525,"Makes sense. The fact that storage status list isn't updated is a bug; I forgot to add  super.onTaskEnd to JobProgressListener. However, this will no longer be relevant in the new proposal, which involves JobProgressListener extending the usual SparkListener as you suggest, and listening for the trimmed ExecutorStateChange events. (More details in discussion above)",non_debt,-
cloudstack,1926,comment,277102135,"@borisstoyanov unsupported parameters provided. Supported mgmt server os are: `centos6, centos7, ubuntu`. Supported hypervisors are: `kvm-centos6, kvm-centos7, kvm-ubuntu, xenserver-65sp1, xenserver-62sp1, vmware-60u2, vmware-55u3, vmware-51u1, vmware-50u1`",non_debt,-
kafka,2273,summary,0,KAFKA-4555: Using Hamcrest for expressive intent in tests,non_debt,-
incubator-weex,822,comment,345958963,"Ah, sure, that's ok, sorry for the misunderstanding. I think it is implicit, but it's ok to have it anyway.",non_debt,-
incubator-pinot,4387,review,300516190,Would be nice to add a comment on why TreeMap (ordering) is needed.,documentation_debt,low_quality_documentation
zookeeper,1328,summary,0,Extra horizontal lines,code_debt,low_quality_code
arrow,2350,comment,409238911,"@wesm Does this affect the OSX packages (conda, wheel)? Should We run packaging tasks to test it?",non_debt,-
airflow,6122,summary,0,[AIRFLOW-5499] Move GCP utils to core,non_debt,-
carbondata,1620,comment,349865134,retest this please,non_debt,-
helix,470,description,0,"https://github.com/apache/helix/issues/469
Building helix-front is failed because node_modules/@types/lodash/common/collection.d.ts(1783,24): error TS2304: Cannot find name 'Exclude'.
 types/lodash module uses `^4.14.71` which would get the latest release 4.14.138 and the version doesn't have ""Exclude"". Downgrading the module version to a fixed version 4.14.120 is the easiest solution for now.
NO new test added.
mvn test doesn‚Äôt run in helix-front as it doesn't have maven tests.
mvn build result:
[INFO] Reactor Summary for Apache Helix 0.9.2-SNAPSHOT:
[INFO]
[INFO] Apache Helix ....................................... SUCCESS [  1.137 s]
[INFO] Apache Helix :: Core ............................... SUCCESS [ 25.523 s]
[INFO] Apache Helix :: Admin Webapp ....................... SUCCESS [  3.209 s]
[INFO] Apache Helix :: Restful Interface .................. SUCCESS [  6.165 s]
[INFO] Apache Helix :: HelixAgent ......................... SUCCESS [  1.370 s]
[INFO] Apache Helix :: Recipes ............................ SUCCESS [  0.044 s]
[INFO] Apache Helix :: Recipes :: Rabbitmq Consumer Group . SUCCESS [  0.782 s]
[INFO] Apache Helix :: Recipes :: Rsync Replicated File Store SUCCESS [  0.915 s]
[INFO] Apache Helix :: Recipes :: distributed lock manager  SUCCESS [  0.660 s]
[INFO] Apache Helix :: Recipes :: distributed task execution SUCCESS [  0.602 s]
[INFO] Apache Helix :: Recipes :: service discovery ....... SUCCESS [  0.608 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  08:03 min
[INFO] Finished at: 2019-09-12T11:11:42-07:00
[INFO] ------------------------------------------------------------------------
Subject is separated from body by a blank line
Subject is limited to 50 characters (not including Jira issue reference)
Subject does not end with a period
Subject uses the imperative mood (""add"", not ""adding"")
Body wraps at 72 characters
Body explains ""what"" and ""why"", not ""how""
Documentation",non_debt,-
incubator-heron,1893,description,0,These tests all have to duplicate the same verbose boilerplate. Centralizing that in `SlaveTester`.,test_debt,expensive_tests
skywalking,6439,summary,0,"grammar error for ""The answer definitely YES""",non_debt,-
spark,23814,comment,465220900,I would also note that a workaround for the AWS EKS issue is that user can first manually log in with AWS and then pass the OAuth token explicitly to `spark-submit` i.e.,non_debt,-
spark,27150,review,365117734,"shall we apply it to all numeric types?
since we don't know the value of the string, using double seems safest to avoid overflow.",non_debt,-
spark,13428,comment,241079022,"ping @JoshRosen, I think this should be good to go now",non_debt,-
incubator-pinot,6333,review,541846547,"Yes, but only if the user clicks on the Metrics tab.  Do we want it to automatically select the Metrics table when the user selects forecast?",non_debt,-
flink,15265,summary,0,[FLINK-21836][table-api] Introduce ParseStrategyParser,non_debt,-
cloudstack,1523,comment,215447324,"@nlivens I don't care for more scripts, just for a test showing the problem so we know it doesn't regress. Whatever meets the goal easiest.",non_debt,-
geode-native,590,description,0," * ACE 6.5.8
 * SQLite 3.31.1
 * Xerces-C 3.2.3",non_debt,-
storm,107,description,0,"instead of using env var changed it to read from config
if user defines storm.log.dir log files goto that particular dir otherwise it will be under storm.home/logs",non_debt,-
incubator-mxnet,10183,comment,375782120,"That's going to be turned on, right?",non_debt,-
carbondata,2390,comment,398931696,"Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/5260/",non_debt,-
beam,4080,review,150063020,I believe setting the compile scope is redundant,code_debt,complex_code
phoenix,933,review,510479056,nit: 0L,code_debt,low_quality_code
incubator-brooklyn,877,comment,138606796,"@ahgittin Thanks for the suggestions.
It was in the `CreateUserPolicy` at the beginning. I've moved it to `SshMachineLocation` so it is backward compatible as much as it can. It guarantees that customers will make no changes at their end.
I will address your comments taking into account that customers will have to change theirs BPs, because of the package renaming",non_debt,-
spark,15027,comment,258087561,@frreiss Thanks for explanation!,non_debt,-
hive,1280,review,468525216,!inputColumn.isNull[j] can be merged with the above condition,non_debt,-
netbeans,414,comment,366287008,I have decided that #416 is a better fix than this one.,non_debt,-
calcite,2055,summary,0,[CALCITE-4104] Add automatically link to GitHub PR and 'pull-request-available' label to issues,non_debt,-
pulsar,8996,comment,763411650,/pulsarbot run-failure-checks,non_debt,-
hadoop,1572,comment,537701365,/retest,non_debt,-
systemds,204,review,73768209,Would you want to check if sc is a SparkContext instance to raise a more meaningful error?,code_debt,low_quality_code
beam,1746,comment,270984320,R: @robertwb,non_debt,-
spark,10667,comment,170158610,/cc @andrewor14 for review.,non_debt,-
attic-apex-core,337,review,63920391,"I see that setTokenRefreshKeytab method is used in testing..
Looks good to me üëç",non_debt,-
druid,5789,review,205683872,:+1:,non_debt,-
pulsar,1192,comment,363611803,"@jai1 this conflicted with my PR, please rebase",non_debt,-
airflow,2078,description,0,"Dear Airflow Maintainers,
[Master branch](https://travis-ci.org/apache/incubator-airflow/jobs/201242728): Ran 256 tests in 819.435s
[This branch](https://travis-ci.org/apache/incubator-airflow/jobs/201335186): Ran 360 tests in 912.790s
It also fixes a few subtle bugs not captured by the already enabled tests.",non_debt,-
geode,5875,review,552244877,Since these are just string comparisons there's probably no need to do any numeric conversions.,non_debt,-
fluo,755,summary,0,Fixed bug in suggested InfluxDB config,non_debt,-
spark,6587,review,31883893,"I thought that also before I made this change, but I don't think that the strong reason we should stop this change.
For most of cases in the code, we returns the same references by well-design the `rules` for a `TreeNode` object, and if the code still keep creating the identical objects(`.equals` returns `true`) in its rule for every iteration, even unnecessary, can this be considered as a bug of the user code? 
I think it's the responsibility for user code to decide whether `TreeNode` object substitutions should be taken (via creating new instance), as user code always knows when a object substitution needed, right? That's also give more freedom for user code to define the `.equals()` in a semantic way for `TreeNode` object.",non_debt,-
nifi-minifi-cpp,95,review,117498245,Good catch! Will push a commit removing that to my branch,non_debt,-
druid,5194,description,0,"fixed missing gz extension from wikiticker json quickstart example
Without this fix running the quickstart will fails with the exception
2017-12-24T19:02:25,097 ERROR [task-runner-0-priority-0] io.druid.indexing.overlord.ThreadPoolTaskRunner - Exception while running task[HadoopIndexTask{id=index_hadoop_wikiticker_2017-12-24T19:02:12.487Z, type=index_hadoop, dataSource=wikiticker}]
...
...
Caused by: java.lang.RuntimeException: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: .../quickstart/wikiticker-2015-09-12-sampled.json
...",non_debt,-
attic-apex-core,522,review,144998177,What is the difference between multiple calls to activate() and calling reactivate()? Why will existing operators break? Will not activate() be called on the same instance multiple times only for operators marked as REUSE_INSTANCE and there are no such operators?,non_debt,-
skywalking,4815,summary,0,Fix endpoint grouping bug,non_debt,-
hudi,1186,description,0,*Execute docker/setup_demo.sh in any directory*,non_debt,-
airflow,12882,summary,0,Enable pylint rule to check for missing commas,non_debt,-
carbondata,2776,comment,425162096,"Build Failed  with Spark 2.3.1, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/8872/",non_debt,-
druid,1936,review,45828055,can you add cases for lower/upper being null,non_debt,-
camel,1266,comment,260025043,"You are surely welcome to look for more javadoc issues to fix, there is plenty of Camel components ;)",non_debt,-
spark,24843,review,292663649,This is actually the only change I seemed to need to make to the Spark javascript.,non_debt,-
netbeans,734,review,211586709,Thanks for the hints!,non_debt,-
spark,19125,summary,0,[SPARK-21913][SQL][TEST] `withDatabase` should drop database with CASCADE,non_debt,-
flink,3771,review,113471045,you are right,non_debt,-
hadoop,2353,review,507018462,Thanks for the tip. I've added the frequently updated counters as counter ref.,non_debt,-
druid,8236,review,311358060,Added some description about it.,non_debt,-
camel-quarkus,115,comment,519659356,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/camel-quarkus-pr/69/",non_debt,-
spark,563,comment,41485626,"Yeah, I think we discussed moving the MLlib driver programs to examples.",non_debt,-
incubator-brooklyn,741,summary,0,Ports @aledsage's Troubleshooting blog post to docs,non_debt,-
openwhisk,681,comment,227222935,Now includes @rabbah's change per @lehoanganh's suggestion.,non_debt,-
spark,21711,review,202536673,ok,non_debt,-
spark,7233,comment,127815741,Closing this patch since we have #7928 open.,non_debt,-
spark,10534,review,49024620,"Whoops, thanks for catching that.",non_debt,-
kafka,2405,comment,273832031,retest this please,non_debt,-
carbondata,2083,review,177685409,"There is a possibility of losing the data from the fact if you set only segments for fact and read the streaming segments directly from fact streaming segments, in case of handoff streaming segments would be marked for delete and those won't be accessible during fact read. So set all the current fact segments also here along with aggregate segments. During fact read,compare the current segments and set segments and read all intersected segments",non_debt,-
spark,17941,comment,303256154,Retest this please.,non_debt,-
hadoop,2021,review,425329924,"As you said keeping the DEFAULT prefix to highlight the default behaviour it is assumed that there are non default behaviours associated with this feature, are there any?",non_debt,-
druid,205,review,5465475,"Make this
""druid.discovery.curator.compress""",non_debt,-
beam,9758,comment,598566648,@lukecwik thanks for the suggestions!,non_debt,-
trafodion,1134,summary,0,[TRAFODION-2657] Remove Automating Update Statistics for SQL Reference Manual,non_debt,-
hbase,2191,review,487111250,"Semantics of this have changed, but I'm not seeing conversation that indicates that it was intentional.
Before: we would stop all workers, then wait for them all to be stopped. Each worker could stop itself concurrently. Now, for each worker, we request a stop and then wait for it to be stopped, then move on to the next worker.
I don't _think_ this is a big deal, but wanted to call it out.",non_debt,-
apisix,3712,review,584581028,We don't use v3.3,non_debt,-
spark,26139,review,335825318,We have test suites for the overflow behavior. I don't think it makes sense to test overflow again in all the functions that take integers.,non_debt,-
flink,6560,description,0,"Created .bat script to start Flink task manager.
Uses scripts flink-console.bat and flink-daemon.bat - see PRs https://github.com/apache/flink/pull/6559 and https://github.com/apache/flink/pull/6552",non_debt,-
skywalking,4473,summary,0,#4463 FAQ for Maven compilation failure with error like python2 not found,non_debt,-
flink,8322,review,282443479,"After thinking deeply, I agree with you. Following the checkpoint's sequence is a better choice. Accept.",non_debt,-
cloudstack,1283,comment,167078298,"Yes, It is missing in all releases. It can be backported to 4.6.",non_debt,-
trafodion,436,comment,214548728,Test Failed.  https://jenkins.esgyn.com/job/Check-PR-master/674/,non_debt,-
beam,7597,review,251529718,"It would require the option parsing to be repeated, first to get the options w/o runner options (where unknown runner options are ignored), then send those to the job service, then repeat parsing with runner options present. I think the return of that isn't worth the extra work and I would like to leave it out of this PR.",non_debt,-
incubator-mxnet,13101,summary,0,[Doc] Fix repo paths in Ubuntu build doc,non_debt,-
superset,6703,summary,0,Trim query before parsing,non_debt,-
trafficserver,1154,comment,256509643,[approve ci],non_debt,-
thrift,1709,review,250756261,"I put SameMajorVersion only because it's often used, but if I understand correctly you'd prefer ExactVersion? The choice is currently AnyNewerVersion|SameMajorVersion|SameMinorVersion|ExactVersion, see https://cmake.org/cmake/help/v3.0/module/CMakePackageConfigHelpers.html",non_debt,-
beam,1885,review,99030478,Could you change `T` to something more clear like `tries_per_work_item`?,non_debt,-
spark,15285,review,83811366,private,non_debt,-
camel-quarkus,1258,review,429120467,Do we need this file? Presumably not if everything is commented out.,non_debt,-
apisix,3300,review,559895428,done,non_debt,-
carbondata,3769,description,0," - No
 - Yes. (please explain the change and update document)
 - No
 - Yes",documentation_debt,outdated_documentation
nifi,2722,comment,391012645,@markap14 understood and agree. I'm +1 on this.  Will also look at NIFI-5222 as well now,non_debt,-
airflow,1682,comment,234900191,33884891-1682 comment-234900191,non_debt,-
karaf,906,summary,0,[KARAF-6373] Upgrade to CXF 3.3.2 to build example on JDK 11,non_debt,-
spark,27330,comment,579611063,"Deprecating the APIs in the major/feature releases, like 2.4.0, 2.3.0, 2.2.0, is common; but deprecating the APIs in the maintenance releases, like 2.2.1, 2.2.2, 2.3.4 is rare.",non_debt,-
kafka,10165,review,579628613,"default should be 10 * 60 seconds
https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java#L152",non_debt,-
kafka,3535,review,133795011,Good catch!,non_debt,-
activemq-artemis,2427,review,232558150,"It may have them from legacy someone put it there, but if you were designing a collections class, you'd design it in a fashion so it focussed just on the logic it needs to have. And any interaction needed is supplied generically, e.g. for your case, you;d need generic method: remove(Predicate<SimpleString> predicate). This is a collection class in reality so i apply the same rules of engagement..
If anything that field, the flag and the method check hasInternalProperties really should move up to CoreMessage, as its only used there. Doing that would mean you can still get your benefit as then the check stays in CoreMessage. 
This would really clean up TypedProperties to just having fields it really should only care for, keeping it clean.",design_debt,non-optimal_design
spark,2576,comment,57905623,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/21293/Test PASSed.",non_debt,-
spark,19752,review,153081719,"Ok, I'll do. Then I'd suggest to do the same also in other places. I can check where an analogous pattern is used and create a PR if it is ok.",non_debt,-
accumulo,274,review,124411313,"Do we no longer care if `ugi.hasKerberosCredentials()` when the case is KERBEROS?
I guess I was kind of expecting this to supplement the verification that the KERBEROS case was logged in, instead of replace it. But, if that's checked elsewhere (at RPC time?), then I guess it's fine.",non_debt,-
spark,10615,description,0,"CSV is the most common data format in the ""small data"" world. It is often the first format people want to try when they see Spark on a single node. Having to rely on a 3rd party component for this leads to poor user experience for new users. This PR merges the popular spark-csv data source package (https://github.com/databricks/spark-csv) with SparkSQL.
This is a first PR to bring the functionality to spark 2.0 master. We will complete items outlines in the design document (see JIRA attachment) in follow up pull requests.
Spark-csv was developed and maintained by several members of the open source community:
@dtpeacock: Type inference
@mohitjaggi: Integration with uniVocity-parsers 
@JoshRosen: Build and style checking 
@aley: Support for comments
@pashields: Support for compression codecs
@HyukjinKwon: Several bug fixes
@rbolkey: Tests and improvements
@huangjs: Updating API
@vlyubin: Support for insert
@brkyvz: Test refactoring
@rxin: Documentation
@andy327: Null values
@yhuai: Documentation
@akirakw: Documentation
@dennishuo: Documentation
@petro-rudenko: Increasing max characters per-column
@saurfang: Documentation
@kmader: Tests
@cvengros: Documentation
@MarkRijckenberg: Documentation
@msperlich: Improving compression codec handling
@thoralf-gutierrez: Documentation
@lebigot: Documentation
@sryza: Python documentation
@xguo27: Documentation
@darabos: License text in build file
@jamesblau: Nullable quote character
@karma243: Java documentation
@gasparms: Improving double and float type cast
@MarcinKosinski: R documentation
@addisonj: nullValue parsing",non_debt,-
parquet-mr,227,summary,0,PARQUET-318: Remove unnecessary object mapper,code_debt,complex_code
spark,29586,comment,704321818,You cannot do so?,non_debt,-
geode,6096,review,589702147,AssertJ will print out the entire result and why the assertion failed if you change to:,non_debt,-
carbondata,3283,comment,501740527,retest this please,non_debt,-
camel,995,review,64336403,Make a setter that takes a single String where the values are separated by comma so you can pass in the value in the same string. This makes it easier if loading values from properties/config/tooling,non_debt,-
spark,22094,description,0,"With code changes in https://github.com/apache/spark/pull/21847 , Spark can write out to Avro file as per user provided output schema.
To make it more robust and user friendly, we should validate the Avro schema before tasks launched.
Also we should support output logical decimal type as BYTES (By default we output as FIXED)
Unit test",design_debt,non-optimal_design
cloudstack,3680,comment,593795196,"Test results LGTM, the test_nic is a known intermittent issue probably due to env issue. Further investigation on the env issue that causes test_nic to fail is requested @andrijapanicsb @DaanHoogland @PaulAngus",non_debt,-
airflow,7484,review,382926937,"http://pylint.pycqa.org/en/latest/technical_reference/c_extensions.html
We could add setproctitle to to the list, or disable this warning globally  c-extension-no-member?
Also perhaps ignored-modules=setproctitle may be needed",non_debt,-
spark,5056,review,26860497,BTW let me use the same fix that Sean mentioned. That will tighten the exclusion so that other methods are still checked by MiMA.,non_debt,-
arrow,6425,comment,630448066,"@BryanCutler you are correct, for some reason I thought LargeList was coupled with LargeVarChar/LargeBinary.",non_debt,-
druid,1259,review,35168022,can we describe these fields in a table?,non_debt,-
couchdb,340,comment,165982039,"As suggested by @iilyak I added a docker helper script (`dev/docker-cli`), making it easier to orchestrate the docker build. It does the following at the moment:
Let me know, if this makes sense!",non_debt,-
carbondata,4026,comment,735039945,"Build Failed  with Spark 2.4.5, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.5/3209/",non_debt,-
incubator-pagespeed-ngx,1332,review,91550845,@hillsp is also not keen on doing this silently,non_debt,-
spark,23057,comment,451204281,"Hi @mgaido91 , since we are going to have new releases for branch 2.3 and 2.4, do you know if this bug exists in 2.3/2.4 and shall we backport it? thanks!",non_debt,-
kafka,8360,comment,624821936,retest this please,non_debt,-
openwhisk,3202,comment,359521936,"Hey @tardieu, does this work with web actions, and is there a test to ensure that an action composition can't be used to infinitely invoke a single action?",non_debt,-
flink,2004,summary,0,[FLINK-3926][TypeSystem]Incorrect implementation of getFieldIndex in TupleTypeInfo,non_debt,-
beam,10268,comment,566967099,"@lukecwik thank you for the review. Before any follow-up changes are made, do you think you could share your thoughts in the discussion in the [JIRA ticket comments](https://issues.apache.org/jira/browse/BEAM-5495)?",non_debt,-
incubator-mxnet,7223,review,130206501,It's actually dim_t in nnvm. index_t is defined in mshadow. Could you please confirm?,non_debt,-
beam,2193,review,106146561,"Added after the for loop, do: node.client().admin().indices().prepareRefresh(ELASTIC_INDEX_NAME).get(); 
Removed comment of sleep(1000)
Ran it to test in a loop for 10 times, works okay",non_debt,-
airflow,2590,comment,330732194,ping @bolkedebruin,non_debt,-
beam,1858,comment,277418547,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7078/
--none--",non_debt,-
flink,10754,review,366364492,I would not shadow the local variable. One could rename the variable `currentLastState`.,non_debt,-
openwhisk,2201,review,114188140,end sentence after feed action with period. Start new sentence: The feed action is an ...,non_debt,-
ozone,312,summary,0,HDDS-2677. Acceptance test may fail despite success status,non_debt,-
nifi-minifi-cpp,107,review,120233311,56750161-107 review-120233311,non_debt,-
carbondata,2824,comment,430259749,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/819/",non_debt,-
spark,22135,comment,414926487,"thanks, merging to master!",non_debt,-
openwhisk,2544,review,135878561,function label .. does not reflect what the function does.. ie.. does not really print out the action code,non_debt,-
tvm,6052,review,454471816,:+1:,non_debt,-
kafka,2341,comment,273664243,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1010/
Test FAILed (JDK 7 and Scala 2.10).",non_debt,-
flink,1771,comment,217159043,Hi guys :) any update on this?,non_debt,-
nifi,1534,comment,282726872,"Couple quick comments...
- With the new layout, we can probably remove some of the width restrictions (or at least increase the values).",non_debt,-
incubator-doris,4345,summary,0,[Bug][MemTracker] Cleanup the mem tracker's constructor to avoid wrong usage,non_debt,-
zookeeper,593,description,0,160999-593 description-0,non_debt,-
flink,6026,review,188786194,"Do you need this extra function, I think just inlining it should be fine.",non_debt,-
druid,5136,comment,349046917,"I think this is the last thing that should squeak into 0.11.0, I'll do the release immediately after it's merged.",non_debt,-
incubator-mxnet,13687,comment,448835162,"Could you explain the difference between split(sliceChannel) and split_v2 , and maybe show some performance compare between these two OP?   Thanks",non_debt,-
nifi,2910,comment,431203861,"Thanks for reviewing and merging it, @ijokarumawak!",non_debt,-
zookeeper,1606,summary,0,ZOOKEEPER-3987: Reduce fork count for tests to 1,non_debt,-
carbondata,3741,summary,0,[CARBONDATA-3791] Frequently Asked Question doc changes,non_debt,-
cloudstack,1005,comment,152503018,@remibergsma can you spare a bubble for this one?,non_debt,-
beam,1632,summary,0,[BEAM-1165] Fix unexpected file creation when checking dependencies,non_debt,-
airflow,4291,review,300401843,What is this change for?,non_debt,-
spark,11048,review,52622161,"There is not one single `ALTER TABLE` command, there are many (stopped counting at 20). The parser gives us pretty good trees to match on. For instance: `ALTER TABLE table_name UNSET TBLPROPERTIES ('comment', 'test')` gives us:
Lets split this code by matching on the `TOK_ALTERTABLE_*` tokens. The result should be alot easier to understand.",non_debt,-
drill,1665,comment,468777946,"I tied to follow your instruction by doing:
- I created the same file
https://github.com/apache/drill/blob/master/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapDecoder.java
- I added at line 57 the code:
@Test
public void testMicrotQuery() throws Exception {
  runSQLVerifyCount(""select `timestamp_micro` from
dfs.`store/pcap/cap-test.pcap`"", 1);
}
where cap-test.pcap is the file I added in the pcap folder.
-Then I run the command
mvn -Dtest=TestPcapDecoder#testMicrotQuery test and I got the error:
Failed to execute goal
org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M2:test
Failed to execute goal
org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M2:test (default-test)
on project drill-protocol: No tests were executed!
First time running/writing unit test, so I will need help.
Thank you very much
Il giorno ven 1 mar 2019 alle ore 15:27 Charles S. Givre <
notifications@github.com> ha scritto:",non_debt,-
beam,13558,review,577357291,Should this be ||?,non_debt,-
superset,11019,review,494429866,"Should we be referencing concrete model classes in migrations? This migration will likely fail to run in the future should any of their structure change. Ideally, partial impl's, or copies should be defined directly in the migration in order to avoid such a situation. The other option is to roll all of these migrations up into a subset of what we currently have.",non_debt,-
geode,4344,comment,556555273,"StressNewTestOpenJDK11 fails because the new dunit tests use regex to verify existence of certain stack traces. However, the CI uses a different JVM which has different formatting which doesn't match the regex.",non_debt,-
flink,6070,description,0,"Both flink-json and flink-avro were added to the /opt directory in order to use them as jars for the SQL Client. However, they are not required anymore because we added dedicated SQL jars later that are distributed through Maven central (see FLINK-8831).
`mvn clean install -pl flink-dist -am` is working again
Remove all formats from /opt
This change is a trivial rework / code cleanup without any test coverage.",non_debt,-
spark,6190,review,31941337,I think adding there lines here is enough:,non_debt,-
incubator-pinot,4761,review,347688745,changed this class to be a wrapper class,non_debt,-
beam,92,review,58796896,"This particular test (and probably others in this file) could be made runner-agnostic. That would make them much better.
However, this is an underlying problem here. Ok to ignore for now.",code_debt,low_quality_code
spark,3583,comment,65415563,@dikejiang Do you mind creating a JIRA and adding the JIRA number to the PR title? Thanks!,non_debt,-
spark,10947,review,55476565,Below code are moved from RRDD with no change.,non_debt,-
flink,1386,review,46773689,"You can probably skip these checks, given that `TaskInfo` has these checks already...",non_debt,-
spark,22537,review,220050102,one more space,non_debt,-
airflow,4779,comment,471400185,@feng-tao But `chain` is helpful in some situation. do you think so.,non_debt,-
bookkeeper,266,review,128908746,What if we have multiple ledger directories and have different/conflicting status and last update times in all those files?,non_debt,-
geode,2043,review,194470796,‚úîÔ∏è,non_debt,-
trafficcontrol,168,summary,0,merge postgres branch,non_debt,-
airflow,4926,comment,476698790,Curious - why isn't Travis running the tests?,non_debt,-
spark,224,comment,38748839,"Hi Cheng Hao,  Thanks for implementing these functions.  I bet they will be much faster than using HiveUDFs as we do now!
Based on the number of comments on the test case refactoring, I think it would be easiest to try and commit just the LIKE and RLIKE with a few simple test cases, and then take on the rest in a separate PR.",code_debt,slow_algorithm
beam,12656,comment,685845145,Retest this please,non_debt,-
geode,3576,review,289920619,Thanks,non_debt,-
spark,11704,summary,0,"[SPARK-13880][SPARK-13881][SQL] Rename DataFrame.scala Dataset.scala, and remove LegacyFunctions",non_debt,-
spark,8508,review,38374063,Perhaps we should also extract a `HasThreshold` mixin for binary classifier thresholds,non_debt,-
iceberg,1096,review,438271764,"I didn't intend to ask you to add another module to the build. Sorry about not being clear.
In many cases, it's possible to work with both 2.11 and 2.12 with compiled binaries. We have two internal versions of Spark that we do this with, although Spark defines the catalog and table APIs in Java so we have less to worry about. Ideally, we would have one module that works with both and we would have some plan to test that. Otherwise, it's probably a good idea to move this code into Flink as soon as possible because it makes more sense to maintain just source compatibility there.
For now, I think we should consider options for testing this against both 2.11 and 2.12. Maybe we can add a build property to switch between 2.11 and 2.12 and just test both in CI. Until then, I think we should just build for 2.12. 2.11 hasn't been supported since 2017, so it makes sense for new development to target 2.12.",non_debt,-
spark,13758,comment,239747204,"@hvanhovell , @cloud-fan , @davies could you please review this?",non_debt,-
flink,15314,review,603761487,these fields should be private,non_debt,-
skywalking,1723,description,0,Add through oal script,non_debt,-
activemq-artemis,2427,review,233197168,this would be a contains method,non_debt,-
ambari,406,summary,0,[AMBARI-23025] NN Federation Wizard: implement step2,non_debt,-
arrow,98,review,97407368,"Easy to read but buggy.
int bug = (int) Long.MAX_VALUE % 64; 
bug will be -1",non_debt,-
carbondata,3542,comment,569505754,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1350/",non_debt,-
pulsar,4403,comment,504319874,run cpp tests,non_debt,-
trafficcontrol,3733,summary,0,CDN-in-a-Box now caches Carton dependencies by default,non_debt,-
samza,30,review,93352730,Remove newline.,non_debt,-
drill,1618,review,250595479,What user is expected to do when he hits such limitation?,non_debt,-
reef,1313,review,120169945,This is a new requirement for developers to setup. Can we do without it? E.g. set the REEF folder as a command line option?,non_debt,-
spark,28105,review,402905582,whole stage codegen,non_debt,-
spark,29767,review,501476018,nice catch @dongjoon-hyun !,non_debt,-
spark,29298,review,462797732,Got it. My bad.,non_debt,-
ignite,854,description,0,Cache query cursor do not create value copy.,non_debt,-
shardingsphere,1252,summary,0,fix test case,non_debt,-
spark,26330,review,341407750,Can this fall back to python 2 actually?,non_debt,-
couchdb,1658,summary,0,Enable cluster auto-assembly through a seedlist,non_debt,-
guacamole-client,200,description,0,"This pull request finishes up the code that @alt36 started for fixing issues with LDAP Referrals.
This closes #129.",non_debt,-
hbase,645,review,326464585,And same way below also.,non_debt,-
spark,316,comment,39515943, Merged build triggered.,non_debt,-
activemq-artemis,2241,comment,412575863,"IMO, the JMS pool from 5.x should not be migrated to Artemis.  It belongs in it's own project with it's own release cycle.  Also, it makes sense for it to *not* be in the ActiveMQ project to make clear that the pool is generic and isn't tied to any ActiveMQ broker.
The pool on messaginghub has JMS 2.0 support.",architecture_debt,violation_of_modularity
cloudstack,3055,description,0,"This add missing test data for one of the keys for a recently added
migration test.",non_debt,-
daffodil,282,summary,0,Specify the appropriate shell for installing GitHub Actions dependencies,non_debt,-
spark,10753,comment,171546245,Would return attribute b in table a.  resolved within the logic of `table.col`,non_debt,-
beam,697,comment,234148097,"Jenkins ran for 1:30:00, seeming to take extremely long on the integration tests. Unclear what this indicates.",non_debt,-
activemq-artemis,2786,summary,0,ARTEMIS-2440 Call timeout should retry the connection asynchronously,non_debt,-
flink,14120,description,0,"*Currently, `TypeInfoTestCoverageTest` which checks a test that extends `TypeInformationTestBase` for all type infos. But `TypeSerializer` doesn‚Äôt have the same thing that would verify that `TypeSerializer` has tests that extend `SerializerTestBase` and `TypeSerializerUpgradeTestBase`. Therefore this should add `TypeSerializerTestCoverageTest` to check whether to have tests based on `SerializerTestBase` and `TypeSerializerUpgradeTestBase` because all serializers should have tests based on both of them.*",non_debt,-
apisix-dashboard,651,review,516443012,got it~!,non_debt,-
cloudstack,2637,review,187600417,can you externalise this string (like the ones above),non_debt,-
spark,18037,comment,302808740,@mgummelt,non_debt,-
activemq-artemis,3448,review,574070715,Im really not. Im suggesting drop trying to be JMS at all if you want to remove JMS api stuff here.,non_debt,-
flink,7550,comment,509489301,This pr is ready for review.,non_debt,-
cloudstack,3165,comment,460543956,@blueorangutan test,non_debt,-
spark,29496,comment,679314749,"You can make another backporting PR against on branch-3.0 while keeping this PR independently.
After that PR is merging, we can revisit here.",non_debt,-
reef,1360,review,132061606,It matches the one defined in Yarn. So may be just use the same name.,non_debt,-
cloudstack,617,comment,124027175,"[cloudstack-pull-requests #808](https://builds.apache.org/job/cloudstack-pull-requests/808/) SUCCESS
This pull request looks good",non_debt,-
incubator-brooklyn,910,review,41485190,"No particularly strong feelings from me, but I'd lean towards including the no-arg constructor for two reasons:
1. It's consistent with the other enrichers (we could change them all, but having some of each pattern seems more confusing).
2. The contract for enrichers/policies/entities/locations to be instantiated through the `EnricherSpec` etc is that the class must have a no-arg constructor. We don't expect people to call this constructor directly.
The second point means we probably should include the constructor with a comment. We could maybe even change `InternalPolicyFactory` etc so that it can handle calling protected constructors, which would enforce that more.
Anyway, I'm happy to ignore it in this PR.",design_debt,non-optimal_design
beam,5253,comment,388164781,Tests passed. Checkstyle and archetypes failed.,non_debt,-
incubator-doris,2222,review,347693807,99919302-2222 review-347693807,non_debt,-
spark,9432,comment,154740151,"Oops my bad, also removed the unnecessary sbin/../ from the other tachyon paths",code_debt,complex_code
pulsar,7555,summary,0,Modify the log level of functions and move the java_instance_log4j2.xml file from the jar package to the conf directory,non_debt,-
incubator-mxnet,19602,comment,753847526,What are the next steps for this PR? Is this ready to be merged?,non_debt,-
arrow,5723,comment,546202890,"@tianchen92 from the logs:
""/home/travis/build/apache/arrow/docs/source/java/index.rst:19: WARNING: Title underline too short.""
I think you need to add an equals sign to that line",non_debt,-
kafka,2143,review,94173222,"Besides not removing this, it seems we should also probably have a unit test validating the new behavior for naming consumer groups in sinks?",test_debt,lack_of_tests
airflow,4409,comment,451617211,got this error even after `pip install cython`,non_debt,-
nifi,4645,summary,0,NIFI-7976: Infer ints in JSON when all values fit in integer type,non_debt,-
hawq,1251,comment,306961322,"I'm thinking from the perspective of an Apache developer. They shouldn't have to go to external corporate documentation sets or knowledge bases (https://discuss.pivotal.io/hc/en-us/articles/201081408-How-to-collect-core-files-for-analysis) to get useful information about a utility (packcore) included with the HAWQ distribution. As it stands, the utility (packcore) will be automatically bundled with all future HAWQ releases.",non_debt,-
spark,31224,summary,0,[SPARK-33819][CORE][FOLLOWUP][3.1] Restore the constructor of SingleFileEventLogFileReader to remove Mima exclusion,non_debt,-
accumulo,1759,description,0,In response to [comment](https://github.com/apache/accumulo/pull/1746/files#r514604929) from @keith-turner. I added a try block around my previous change on #1746. With the unreserveNamespace call in the finally block.,non_debt,-
spark,24155,comment,475524405,"@ueshin @srowen @HyukjinKwon Please, review the PR.",non_debt,-
flink,12120,review,424557600,"Either the methods in the `if` conditions should be renamed to self-documenting ones, or the inlined comments should be preserved. 
~By looking at this right now, I'm not even sure which condition was mapped to which one (has the order of conditions been preserved?)~ after couple of minutes I figured it out, that the order was preserved, but since it wasn't obvious for me, it proves the point that something is missing here.",non_debt,-
reef,183,comment,102267300,"[Reef-pull-request-ubuntu #486](https://builds.apache.org/job/Reef-pull-request-ubuntu/486/) SUCCESS
This pull request looks good",non_debt,-
spark,1100,comment,46265829,Merged build started.,non_debt,-
cloudstack,3965,comment,640322672,@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.,non_debt,-
spark,21018,review,183903195,"Without the changes in this PR, this test still can pass. : )",non_debt,-
incubator-pinot,3153,review,216405500,"I think we already have users with this header, so I will leave it as it is.",non_debt,-
hive,345,review,189552180,This also can be debug log.,non_debt,-
carbondata,3787,review,486246357,why is the filter removed?,non_debt,-
flink,11725,review,425731246,"why it is easier if not a real timestamp? I can change it back to a static number, but feeling this might be more realistic.
This change is originally motivated by debugging the Kafka Server. I thought some data is trimmed because of the timestamp is too old. But it is not related.",non_debt,-
thrift,716,summary,0,[THRIFT-3445] Generates Throwable#getMessage override,non_debt,-
kafka,5492,comment,416127274,@lindong28 Thanks for the comments. Please review again.,non_debt,-
phoenix,153,review,58469369,Similarly here. Call clearTableFromCache.,non_debt,-
incubator-dolphinscheduler,5104,comment,803699297,"@zhuangchong You can start a discussion in the dev email, and realize this improvement according to the idea you mentioned, not on this 1.3.6-prepare. The current pr is a cherry-pick from dev, and this improvement is the result of the previous discussion on March 17.",non_debt,-
apisix,4,review,281085850,"add `type` for auth,limit, rewrite,log...",non_debt,-
ignite,6420,review,273863106,Nanoseconds is too much. Milliseconds will be enough.,design_debt,non-optimal_design
spark,3976,review,23888565,"https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L451
@andrewor14  here is not outdated. this is same to master's code.",non_debt,-
beam,3852,review,139262810,Done.,non_debt,-
kafka,9318,review,495932581,"This check makes sure the code always stays in sync. However, if we can make great code reviews (like yours), this static check should be unnecessary :)
I will remove this painful check (to me also) in next commit.",design_debt,non-optimal_design
hawq,679,review,64878972,"Hawq register si a utility to register file(s) in hdfs into the table in hawq. It will move the file in the path(if path refers to a file) or files under the path(if path refers to a directory) into a new place corresponding to the table <tablename>,
and then update the meatadata in the table.
=>
""hawq register"" is a utility to register file(s) on HDFS into the table in HAWQ. It moves the file in the path (if path refers to a file) or files under the path (if path refers to a directory) into the table directory corresponding to the table <tablename>, and then updates the table meta data to include the files.",non_debt,-
beam,8826,review,293177180,"Seems like you are claiming only one document here but returning all documents ?
Instead claim each record (document position) before returning. This will guarantee that we do not claim (and return here) documents that belong to splits that have already been given to other workers through dynamic work rebalancing (RangeTracker.split() call).",non_debt,-
spark,26206,comment,545540968,"Yea, please describe the problem from an end-user's perspective.",non_debt,-
samza,1007,summary,0,SAMZA-2173: Fix NullPointerException in SetConfig MessageFormat when configurations are deleted.,non_debt,-
shardingsphere,8075,summary,0,Reuse TableMetaDataLoader in SchemaMetaDataLoader,non_debt,-
spark,7753,description,0,"This is a sub-task of [SPARK-9103](https://issues.apache.org/jira/browse/SPARK-9103), we'd like to expose the memory usage for spark running time, this is the first step to expose the netty buffer used both with on-heap and off-heap memory. Also the metrics are showed on WebUI. In this PR, a new web Tab name `Memory` is added. Which is used to show the memory usage of each executors (can be in more details in future). the screenshot is like the following:
This is for each stages memory info:
This is History View:
This is WIP because with unit tests left.",non_debt,-
druid,6215,review,212155874,"I went and double-checked this file. It was contributed in #5440 by @njhartwell, and it looks like he authored this file and intended to contribute it. So I think we are good to add this header.",non_debt,-
activemq-artemis,1867,comment,367475804,I will merge this.. but if there's any failures from the full testsuite.. I may have to revert this.,non_debt,-
carbondata,2814,comment,483209226,"Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2882/",non_debt,-
hadoop,1001,review,296306101,23418517-1001 review-296306101,non_debt,-
rocketmq,511,summary,0,Update CountDownLatch2.java,non_debt,-
incubator-pagespeed-ngx,170,description,0,4766638-170 description-0,non_debt,-
incubator-mxnet,5518,summary,0,Add pad param to DataBatch returned by BucketSentenceIter,non_debt,-
beam,2512,review,111508679,similar comment about javadoc link to specific function,non_debt,-
flink,4473,comment,327681764,"Hi @bowenli86, sorry about this. I had the commit ready to merge but was waiting for another test PR to be merged first. Merging this now!",non_debt,-
kafka,5630,comment,421338046,"@mutdmour  Above comment is not for you :) .  By commenting  ""retest this please"",  we can re-trigger the Jenkins builds/tests.",non_debt,-
tvm,1251,review,195164394,two lines between functions,non_debt,-
flink,2642,comment,254715930,"OK, I see your point now @StephanEwen.
@nssalian, I'm sorry for opening the jira; I didn't think it through from this perspective at the time.",non_debt,-
spark,15172,review,83513326,nit: space after `if`,code_debt,low_quality_code
spark,20640,review,169556816,"@kayousterhout BlacklistTracker has it's own logging that is concerned with blacklisted nodes, won't it be enough? on the other hand, if blacklisting is disabled, which is default, then we will lose this information.",non_debt,-
superset,2138,comment,278523515,"Yeah, it is.",non_debt,-
spark,6551,review,31416746," `numericPrecedence` doesn't contain `StringType`, you need to write a separate case like `case (t1 @ StringType(), t2 @ NumericType())`.",non_debt,-
geode,4765,review,387347011,"the `SC` prefix is clearly a convention, but what does it stand for?",non_debt,-
spark,13163,review,63943470,You could check that `parser.HELP` is in the final command.,non_debt,-
zeppelin,2323,comment,301308202,"Each cell in table supposed to print html. For example
This PR
In master branch",non_debt,-
trafficserver,6721,review,418591026,I added an `ink_release_assert(netvc);` to give clang a valid code path.,non_debt,-
fluo,541,description,0,‚Ä¶types implement serializable.  Renamed to FluoInputFormat to FluoRowInputFormat,non_debt,-
flink,5448,comment,396135754,@StephanEwen any opinion about this PR?,non_debt,-
flink,11794,comment,619004787,@godfreyhe I think that this is a valid tradeoff for now. In the future we may have an exception that says that `CollectionEnviroment not allowed to be used with TableEnvironment. Please use XXXX instead`. For `DataSet` we do not have this problem because the execution logic is hardcoded in the environment.,code_debt,low_quality_code
carbondata,3774,comment,636062526,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/3090/",non_debt,-
kafka,683,comment,167010840,"@benstopford @fpj This looks good to merge aside from the comment I left about the `use_authorizer` parameter to `KafkaService`. We only have one `Authorizer` implementation, so the parameter kind of makes sense in the scope of only Kafka's tests, but we should think of the `Service` classes as semi-public interfaces we want to be careful about changing -- they should be reusable by others to write system tests. If anyone wanted to test other authorizer implementations, I think the current `use_authorizer` parameters wouldn't be a great solution -- they'd have to add still another option (or make assumptions about the implementation by setting the `authorizer_class_name` field on `KafkaService` directly).",design_debt,non-optimal_design
trafficserver,3957,comment,404708724,"Now we can argue about the name! ""Guard"" is usually used for protective mechanisms, such as locks or guards against garbage collection. This will normally do the opposite and guarantee destruction. Something like ""Always"" or ""Afterwards"" or ""PostAction"" or ""OnExit"" or ""PostScope"" or ""ScopeAction"" or ""PostScript"". I kind of like ""Afterwards"" or ""PostScript"".",non_debt,-
ignite,8509,review,533248278,"I see your point, but if we introduce a method that not throws then we should return a tuple from it - T2(className, errorMsg).
Moreover, the second value of the tuple required in rare cases when we can't resolve the class name from type id.
AFAIU `getClassName` executed several times on every interaction between DotNet and Java so the creation of an extra object on each invocation will hurt the performance.
So I propose to keep changes as is. What do you think?",non_debt,-
druid,10039,summary,0,Escape not working,non_debt,-
ozone,1163,comment,657391415,Thanks @xiaoyuyao for the review.,non_debt,-
zookeeper,953,comment,503348419,"Yes, sorry, I mean defaults to `initLimit`.
With my comment I wanted to emphasise that we usually don't add every single new config option to sample config file (keeping the size at the bare minimum) and this new one doesn't seem to me any exceptional. It's more like an advanced, low level setting to me while the sample config is for newbies.",non_debt,-
nifi-minifi-cpp,489,review,259901864,"std::set::insert supports exactly what you need:
So you can avoid using find.",non_debt,-
incubator-mxnet,14743,comment,485249309,#14740 is merged now. Can you rebase? @yajiedesign,non_debt,-
airflow,13929,review,570530609,This exception is too broad,code_debt,low_quality_code
thrift,602,comment,138420774,Jira issue here https://issues.apache.org/jira/browse/THRIFT-3318,non_debt,-
commons-lang,37,comment,62560062,General comment about RandomUtils: shouldn't there be methods that take a Random instance? Sometimes you want control over what RNG you use (e.g. Random vs. SecureRandom).,non_debt,-
cloudstack,4178,comment,650724813,"This feature aims to allow user to specify how many CPU sockets the target VM would have by using the 'cores per socket' parameter.  
Assume user assign 16 vCPUs to a new VM,  without manually specify the 'cores per socket',  CloudStack would try to divide the 16 vCPUs by 6, 4 and 1 in order.  In this case,  CloudStack would assign 4 sockets (16vCPUs /4) to this VM and every scoket has 4 vCPUs.
In most cases,  it shall casue no issue, however if there's sort of CPU limits due to Guest OS or application, e.g. SQL Sever version,   it could turn into a big performance issue. 
In our real case,  user deployed an Windows 10 guest OS with 16 vCPUs,  and CloudStack assgins 4 virtual CPU sockets by default.    Due to Windows 10 only supports 2 CPU sockets (https://answers.microsoft.com/en-us/windows/forum/windows_10-win_upgrade/windows-10-versions-cpu-limits/905c24ad-ad54-4122-b730-b9e7519c823f?auth=1) ,  it turns out this guest OS is only able to utilize two sockets (8 vCPUs) even though we have assigned 16 vCPUs in toal.   If user has CPU intensive application running on this VM,  this could introduce bottelneck.",non_debt,-
samza,295,review,139497932,"I would expect the output should be translated from ""insert into"" (TableAlter relNode?) to sendTo() operator.",non_debt,-
gobblin,1800,description,0,"Problem statement:There's a `runtime.props` contained in hive table properties. The value of this props is all the metadata k-v pair that we obtained in runtime, including kafka topic for example. Keeping this value in final metadata is not necessary. This is a minor fix for removal.
@ibuenros Can you help review? Thanks.",non_debt,-
hadoop,787,review,282611587,The Ignore annotation says getFileInfo.. I would have thought you wanted the getListing row in the permission chart?,non_debt,-
spark,30138,review,511167105,"sure, updated, it's safer.",non_debt,-
hive,1470,comment,712424419,"Thanks for your help, Vihang!",non_debt,-
incubator-mxnet,16189,description,0,"- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here",non_debt,-
pulsar,1903,comment,394860390,it seems like [Python-test](https://builds.apache.org/job/pulsar_precommit_cpp/1707/) build is stuck. will try to do force push again.,non_debt,-
cloudstack,3251,comment,484031952,The feature of uploading a volume/template and with this PR iso requires that the env has ssl/tls certificates setup. The failure to upload a template/volume/iso when endpoint is incorrectly setup with a wrong certificate (default) is a known case and should not limit the acceptance of the PR. Instead in a separate PR we can attempt to allow users to upload a template/volume/iso when admin has not setup a certs by providing it a non-https endpoint of the ssvm.,non_debt,-
kafka,10310,description,0,"Initially we want to be strict about the loss of committed data for the `@metadata` topic. This patch ensures that truncation below the high watermark is not allowed. Note that `MockLog` already had the logic to do so, so the patch adds a similar check to `KafkaMetadataLog`.",non_debt,-
hbase,214,review,281063865,Sure. will move the test to a new class,non_debt,-
carbondata,2984,description,0,"ProblemÔºö
24274.0 (TID 664711) | org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.carbondata.datamap.bloom.BloomCoarseGrainDataMap.prune(BloomCoarseGrainDataMap.java:230)
        at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:379)
        at org.apache.carbondata.core.datamap.DistributableDataMapFormat$1.initialize(DistributableDataMapFormat.java:108)
        at org.apache.carbondata.spark.rdd.DataMapPruneRDD.internalCompute(SparkDataMapJob.scala:77)
        at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:82)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
SolutionÔºö
when filteredShard is empty, return arraylist<>(), and protect null exception of other scenes
        protection of null exception, existing test cases are fine",non_debt,-
arrow,2501,comment,473802642,I'm not sure if this is the correct issue but it looks like manylinux 2010 is now very close https://github.com/pypa/manylinux/issues/179?,non_debt,-
spark,1226,summary,0,[SPARK-2287] [SQL] Make ScalaReflection be able to handle Generic case classes.,non_debt,-
spark,1841,review,15979883,"not that big of a deal - how come you didn't use ""i""?",non_debt,-
spark,17161,review,104306047,"I think we should avoid having `tempfile()` as output path in example, as that might point users into the wrong direction - anything saved in tempfile will disappear as soon as the R session ends.",non_debt,-
kafka,2193,review,91697384,Why not return the number of fetches that were sent?,non_debt,-
spark,17403,comment,288903865,Merging to master,non_debt,-
airflow,4328,review,242232376,I think there's a `dag_bag.bag_dag()` method that we should use instead?,non_debt,-
cloudstack,4543,comment,748986444,some errors during succesfull build:,non_debt,-
spark,19888,review,154857148,"Yep. It exists in master, too.
I'll update like the example by @cloud-fan .",non_debt,-
kafka,2910,review,114688209,"Hmm, not sure about the comment on HW. With KIP-101, typically the truncation point is >= the local HW.",non_debt,-
arrow,2770,description,0,"Thank you to @romainfrancois and others who have pushed forward the R side of this project!
This PR is my attempt to address [ARROW-3336](https://issues.apache.org/jira/projects/ARROW/issues/ARROW-3366), providing a testing container for the R package.
This follows up on work done by @kszucs in #2572 in an R-specific way.
**NOTE:**  This PR is a WIP. `R CMD INSTALL` currently fails because it cannot find wherever I installed `arrow` to. But I felt that this is far enough along to put up for review.",non_debt,-
shardingsphere,5701,summary,0,Fix ClassCastException in ShardingSpherePreparedStatement and add spring namespace example for shadow,non_debt,-
druid,5418,review,179293278,Added class name,non_debt,-
hawq,1349,review,177941676,"The logic of case T_TableScanState in VExecEndNode is not the same as VExecInitNode, what's the difference ?",non_debt,-
pulsar,8988,review,549638842,"@sijie ok. I open issue #9081 for adding the validation when uploading a package and will do it in another PR.
Can we merge this PR first? Because this PR will allow us to use the package name to create functions.",non_debt,-
fluo,560,review,40940395,done in 4d0872b,non_debt,-
trafficcontrol,3534,review,336146327,done,non_debt,-
zookeeper,1085,description,0,160999-1085 description-0,non_debt,-
superset,7770,comment,505962109,"@villebro @mistercrunch Thanks for the backstory! Wow, I can see that there was a lot of work done to fix the previous issues. One question: Will there be a regression if the option value is not specifically an object {table: ..., schema: ...}? I did not use fully-qualified table names. I just pulled out table and schema into separate properties in the parent object and it seems fine. Am I missing something?",non_debt,-
spark,9257,summary,0,[SPARK-11073] [core] [yarn] Remove akka dependency in secret key generation.,non_debt,-
samza,1455,review,553059922,"Maybe reword it as - ""Number of failed attempts to establish heartbeat when the AM changes""",non_debt,-
spark,14536,summary,0,Merge pull request #1 from apache/master,non_debt,-
spark,9773,comment,157865029,I will add a test case.,test_debt,lack_of_tests
flink,1617,review,52468108,Is there a better way to check this condition?,non_debt,-
fineract,726,comment,596234596,"This is just a rebase of  #719 from @xurror and @percyashu and I'm just curious if this passes... I ran this locally x3 times, and it failed with https://issues.apache.org/jira/browse/FINERACT-855 every time - may be that is not just a flaky test, but really something that this PR breaks, for some (strange, yes) reason.
NB that other PRs passed today (e.g. #723 and #725) so if this still fails, perhaps the new RestAssured version is somehow changing some timing or something which causes IT test failures?!",test_debt,flaky_test
incubator-pinot,6640,review,586927835,Good idea. Updated.,non_debt,-
incubator-brooklyn,272,review,19522974,"If it's across thread boundaries or even in the same thread but far away then I think I agree with you.  But here the `throw` is just a few lines after the method call which originally threw and caught the exception.
What would be the right solution?  Some sort of `Exceptions.propagateFromExecution(exception)` which you invoke if not in the original thread context and which throws an `ExecutionException` (and does not interrupt the current thread)?
Since this is all same thread can we keep this code and in another PR address the places where we blithely do `Exceptions.propagate` across thread boundaries?",non_debt,-
spark,16099,review,165877094,"ok, but My pr is closed by community...",non_debt,-
spark,4588,review,25929407,Yes - I think it'd make sense to decouple this from the RPC itself.,non_debt,-
spark,25416,review,333402192,"@cloud-fan There are exists a problem the index of partition and the order of data are inconsistent.
I have a new implement but not works file as I can't assurance the order of output produced by child plans.",non_debt,-
hudi,1413,comment,599784095,@bschell HUDI-539 was for fixing this for spark sql.. did you test that as well? does it actually fix anything for spark queries,non_debt,-
spark,31438,description,0,"Since when `UnsafeRow.getString` return UTF8String, but `Literal.apply()` don't support UF8String.
In this patch support it.
Make Literal's constructor support UTF8String
When s is instance of `UTF8String`
user can use `Literal(s)`
Not need",non_debt,-
airflow,6075,review,411690502,@ashb I'm not sure why the static checks are failing on the docker lint. Any insights?,non_debt,-
skywalking,4958,summary,0,add brpc-java plugin,non_debt,-
incubator-heron,3132,summary,0,Add sum/min/max reducers,non_debt,-
beam,7209,review,243750314,Is there a way to explain this list? Or is it just a fairly arbitrary laundry list of reserved words that are somehow too aggressive?,non_debt,-
spark,14618,comment,240031951,"I think this cache is mainly used for file status, not `decoding the metadata fetched from the external Catalog`. Thus `MetadataCache` may not be a good name.
And this cache is useful whatever the external catalog is, why not put it in sql core module?",non_debt,-
beam,9840,comment,544768941,Run Python PreCommit,non_debt,-
hbase,722,review,340411134,Added in the latest commit now.,non_debt,-
hudi,689,review,289992610,Replace with FileStatus fileStatus : fileStatuses ? You don't need index...,non_debt,-
accumulo,321,comment,344053661,"We require that the user update the entry using the cache so that it can
reevaluate the weight. That could be a put or computation. The weight is
stored with the entry and not re-evaluated on eviction.
On Mon, Nov 13, 2017 at 12:45 PM Keith Turner <notifications@github.com>
wrote:",non_debt,-
guacamole-client,349,review,268450052,Renamed to `URIGuacamoleProperty`.,non_debt,-
flink,6815,review,224498635,We are storing a primitive value in a boxed value. Why not using `null` here?,non_debt,-
kafka,5975,review,239094999,"Maybe we could put this to `INFO` but change the message slightly to say ""This could be an issue for IQ"" or something along those lines.  Just a thought.",non_debt,-
geode-native,288,review,185342808,variable name could be better perhaps,code_debt,low_quality_code
beam,10699,comment,579993273,All fixed up and green.,non_debt,-
spark,14671,comment,240376957,@andreweduffy @rxin Maybe I can go for the simple benchmark quickly (maybe within this weekend) and open a PR to disable Parquet row-by-row filtering if it makes sense and this can be the reason to hold on this PR?,non_debt,-
incubator-pagespeed-ngx,31,comment,10198698,"While this looks good to me I've asked a team member here at google to look at it too.
The use of spinlocks makes me uneasy, but your justification seems reasonable.  Some profiling eventually to make sure we're not spending much time spinning would be good.",non_debt,-
druid,850,comment,63570527,@fjy / @xvrl  : any comments on this one?,non_debt,-
tvm,6351,review,495270303,"Unforunately, it seems like the compile engine can't find any schedules if I do this:",non_debt,-
flink,13641,review,544440450,I think you are right. Marking this conversation as resolved.,non_debt,-
spark,29801,comment,694865687,Merged to master.,non_debt,-
nifi-minifi-cpp,211,review,153915241,"Would like to see either a RAII object to ensure open/close happens, or at least a try {} catch {} with the closedir call in all exception paths to ensure things are closed if there are exceptions.",non_debt,-
superset,10043,review,439680610,Prefers `IndeterminateCheckboxProps`: https://github.com/apache-superset/superset-ui/pull/586,non_debt,-
spark,16270,review,92284372,"no problem, sorry I never get these right -- I actually thought package imports were supposed to be first and checked some other places to realize you are right.",non_debt,-
samza,569,comment,405318148,"@cameronlee314 This PR is not for open review. Sorry about not making that explicit. We have couple of ways of implementing side input stores and this was one of the illustration of the prototype.
We decided to drop this prototype and go with a separate class for side input storage manager. I will take a look at the comments and apply whatever is applicable to the new prototype.",non_debt,-
airflow,10380,comment,677442785,Rebased after merging #10368 - I think it is ready for review and very accurately describes what we have in CI now.,non_debt,-
druid,3226,review,70705800,Generally similar comments to LastAggregatorFactory,non_debt,-
spark,10152,review,46763785,"will add java doc to explain it.
for backward compatibility, I can add default value in this function instead of the newly introduced one.",documentation_debt,outdated_documentation
karaf,1267,description,0,832676-1267 description-0,non_debt,-
druid,8703,comment,545535774,I see the docker compose starts every service individually. Would it be better to not use docker compose and instead use a single docker running the `bin/start-quickstart` script?,non_debt,-
airflow,2664,comment,334822478,LGTM! +1,non_debt,-
flink,2795,comment,260432592,Thanks for the PR @uce! Could you explain how the user would retrieve the user class loader to load classes during runtime? Is the user class loader exposed to the user?,non_debt,-
activemq-artemis,3073,review,418238684,"Instead of passing the `configuration` map and then potentially extracting the keystore and truststore path & provider from it why not simply pass through the `keystorePath`, `keystoreProvider`, `truststorePath`, & `truststoreProvider` which were passed in to `getSSLContext` (i.e. the caller)?
Keep in mind that the individual configuration values that are passed to the caller aren't simply those pulled directly from the map. There is series of checks to ensure the client isn't overriding things with system properties, etc.",non_debt,-
tvm,6488,review,489127250,"Python does not support constructor overloading, so I have to create a new function for this.
I think both `auto_scheduler.create_task` and `auto_scheduler.task.create` are okay.",non_debt,-
reef,885,comment,207136499,@markusweimer Do you see any test failures that should block this PR?,non_debt,-
spark,22071,review,209714362,"Got it, my reasoning is that it could be harder for someone looking at the code to figure out why this is not allowed, since we don't really mention about the rest server which is really the one requiring security to be turned off. Another reason it will be beneficial to have the check in the MesosRestServer is that the MesosClusterDispatcher framework could technically be decoupled from the MesosRestServer and allow another way to receive requests, so to increase flexibility and avoid someone forgetting about why we put this here, my suggestion is to move the check closer to where it's being required will help maintain this a bit better.",design_debt,non-optimal_design
trafficserver,1893,summary,0,coverity 1021707: Uninitialized scalar field,non_debt,-
flink,8006,review,266768142,Scala example:?,non_debt,-
spark,21158,comment,385583092,"We are *not*. That's the whole reason why I'm adding the SQL-specific option to extend the behavior of the core options.
If I just wanted to revert the original behavior change, which added more unwanted default redactions to the core options, I'd just have changed the default value of the core option back to what it was.",non_debt,-
arrow,7466,comment,645187978,"Since this PR added a test that creates mutable execution context, it needs to be rebased and updated after https://github.com/apache/arrow/pull/7464 get merged.",non_debt,-
lucene-solr,1802,comment,683374849,"I ran a quick benchmark of just the javadoc task of `master` vs `LUCENE-9215` branch:
`./gradlew clean compileJava && time ./gradlew javadoc`
master: BUILD SUCCESSFUL in 3m 21s
LUCENE-9215: BUILD SUCCESSFUL in 3m 22s
So this check pass is effectively free and doesn't slow down javadoc processing at all (the hard part is already done), and I think it is a lot easier dealing with the errors directly from `gradlew javadoc`: things like filenames and line numbers really help. Plus we remove the previous python script which was recursively parsing HTML, and that thing was never instant.",code_debt,low_quality_code
incubator-mxnet,13993,review,253270291,update comment?,non_debt,-
incubator-mxnet,9420,description,0,"https://github.com/apache/incubator-mxnet/issues/9419
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable",non_debt,-
ignite,3385,description,0,31006158-3385 description-0,non_debt,-
madlib,355,review,266705159,"We should add a usage function in `madlib_keras.py_in`, and a version of `madlib_keras_fit()` here with no parameters that calls it.  Same for `madlib_keras_predict()` and `madlib_keras_evaluate()`.",non_debt,-
cassandra,762,comment,705298992,"Well that was a surprise @yifan-c The new custom junit executor was wrong. Adding that new test method you suggested I noticed I couldn't get any failures. The tests were green despite I knew they were broken. I found out the new executor was swallowing all errors. So I removed that and overrode the `@After` method, which on retrospect I don't know how I didn't think of it before.
Now everything works as expected, it's cleaner and has the extra test method.
CI [j11](https://app.circleci.com/pipelines/github/bereng/cassandra/144/workflows/8e0704dc-c5c2-4f34-8455-8625a715922c)
CI [j8](https://app.circleci.com/pipelines/github/bereng/cassandra/144/workflows/17c720fd-af9e-424b-b510-fa386266e9e1)",non_debt,-
spark,17070,comment,284604494,Jenkins retest this please,non_debt,-
zookeeper,1322,review,416971618,"@symat , as @anmolnar and @lvfangmin mentioned - the intention of this counter was to help in monitoring and I would suggest me keeping it in the code unless you still have some concerns on this ?",non_debt,-
kafka,7871,review,361525143,As above: use `assertThrows` and verify error message,non_debt,-
spark,7822,description,0,"Use print(x) not print x for Python 3 in eval examples
CC @sethah @mengxr -- just wanted to close this out before 1.5",non_debt,-
iceberg,2210,comment,786779145,"We have some strange classpath issues with ORC in our test classpath, to make things worse, when I attempt to debug the issue the debug console does not have the same issue and can call the missing method without issue.",non_debt,-
flink,8305,comment,487605589,"@flinkbot attention @zentol 
@flinkbot approve description",non_debt,-
spark,18767,comment,318823827,"I‚Äôm not convinced this would be useful, could you close this please?",non_debt,-
flink,6710,comment,425442916,"@fhueske hi, Stephan mentioned you in the email reply to me, let me find you help, I don't know if you have time to discuss this jira, but I still look forward to your reply.",non_debt,-
carbondata,3502,comment,569230321,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1313/",non_debt,-
ignite,7881,review,451388955,Removed.,non_debt,-
thrift,1878,comment,540006778,"This can unexpectedly break callers, please add it to [CHANGES.md](https://github.com/apache/thrift/blob/master/CHANGES.md#breaking-changes).",non_debt,-
arrow,7875,description,0,This provides some basic methods for interacting with a Flight RPC server via `pyarrow` and `reticulate`.,non_debt,-
beam,5195,description,0,"Instants are comparable. Earlier/Later are synonyms with Less/Greater.
------------------------",non_debt,-
flink,11403,review,394369655,"1. `protected` I don't think it's needed, as this is looks like valid public api of this class. Note,  `StreamOperatorStateHandler` is a private field of abstract classes.
2. ` KeyedStateBackend<?>` wouldn't work as some `PublicEvolving` apis need this to be casted. I think global type parameter wouldn't work, as this field/class is being used also in non keyed context.",non_debt,-
skywalking,4972,review,446476018,45721011-4972 review-446476018,non_debt,-
zeppelin,3380,review,292970940,"Just run an interpreter, just need this `zeppelin-interpreter-api-${Z_VERSION}.jar`?
Do need `zeppelin-${Z_VERSION}/lib` directory?",non_debt,-
flink,9820,review,334454700,"Yes, same as above. üëå",non_debt,-
beam,12067,comment,652715930,Run Python2_PVR_Flink PreCommit,non_debt,-
ambari,1091,description,0,"Renamed all metric tables that were changed to have UUID column
METRIC_RECORD -> METRIC_RECORD_V2
METRIC_AGGREGATE -> METRIC_AGGREGATE_V2
and so on...
Manual (Install ams with hdfs) + UTs",non_debt,-
echarts,11299,summary,0,feat: provide `echarts.graphic.registerShape` and `echarts.graphic.getShapeClass`,non_debt,-
zookeeper,156,comment,604991915,"@eolivelli yep, I know. I meant to wait for gcr repo to update to another version.",non_debt,-
activemq-artemis,2976,review,377905936,"No, it isn't for every message sent. It's only used when a handler is set (either on the session or passed-in when sent).",non_debt,-
flink,6170,review,195824665,indentation,code_debt,low_quality_code
kafka,5103,comment,395284654,I closed KAFKA-6538 and created https://issues.apache.org/jira/browse/KAFKA-7015 to track the `RecordCollectorImpl` issue -- this makes it easier to assign fixes to different versions.,non_debt,-
zeppelin,194,comment,129714746,"I think CI test failure is not related to this PR.
I'm merging it if there is no more discussions.",non_debt,-
geode,4135,review,333147849,"It seems like the `startFunctionExecution()` and `getTime()` should be hoisted outside the try/catch so that `startFunctionExecution()` is guaranteed to be called before `endFunctionExecutionWithException()`. I also don't like how we're invoking `getTime()` twice in this method because it makes the recorded elapsed times for successful and failed executions inconsistent.
Again, I'm not sure if this would cause any problems for users who rely on the current stats implementation.",non_debt,-
spark,5173,review,28445096,"Right now, vendoring is easier for us to minimize the dependencies. We'd like contributing back these changes to upstream later.
@rgbkrk Have you tried Dill ?  https://github.com/uqfoundation/dill",non_debt,-
incubator-mxnet,15909,summary,0,[numpy] random.rand,non_debt,-
spark,7674,comment,125068482,retest this please.,non_debt,-
spark,8982,review,41439621,"use ===, not ==",non_debt,-
flink,7974,summary,0,[FLINK-11902][rest] Do not wrap all exceptions in RestHandlerException,non_debt,-
carbondata,1404,comment,335447980,"Build Success with Spark 1.6, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/286/",non_debt,-
superset,12841,description,0,Adjust the weights on the subtitles to be less than the titles,non_debt,-
spark,6390,review,30955789,"Currently there is no test with legitimate kryoBufferProperty value expressed in 'm'
This addition fills the gap.",non_debt,-
storm,272,comment,60306459,"@viktortnk Thanks for your work on this. I made some minor changes and submitted a pull request to your repo.
I confirmed that the issue is reproducible in 0.9.3-rc1, and that it is fixed with (y)our changes.
I'm +1 with the suggested changes.",non_debt,-
flink,10836,comment,579829849,@aljoscha sorry for the ping. did you happen to get a chance taking a look at the PR?,non_debt,-
ambari,1646,review,199117523,This is unnecessary.,code_debt,complex_code
trafodion,1777,review,268524042,You can use some other function to find out whether the number has integer part rather than comparsion between the absolute value of the number and 1.0 .,non_debt,-
ignite,2941,summary,0,"ignite-6774 Java doc is broken: ""LUDecomposition.java:40: warning - T‚Ä¶",documentation_debt,low_quality_documentation
camel-quarkus,1217,comment,627760539,saas tests cancelled because of oom,non_debt,-
spark,10634,review,50659285,"For Sql aggregation, the `spillSize` here is 0 because the data are stored in a map instead of this sorter. So `incMemoryBytesSpilled(spillSize)` actually increase 0. We need update the `MemoryBytesSpilled` after freeing the memory in the map.",non_debt,-
incubator-pinot,5706,review,454770490,"Recommend using exception to pass the failure message instead of passing in a logger. The problem of using logger is that the caller cannot control the logging level. For user input, in most cases we don't want to log error which usually stands for severe problems.",non_debt,-
hbase,2769,review,601766918,You are right.,non_debt,-
tvm,2838,comment,473731604,cc @yidawang @JennyChou @junrushao1994 @jroesch @jermainewang,non_debt,-
spark,29104,review,459576703,"nit: Should we add a comment here that lookupKey contains only a single column ? It will make understanding ""allNull"" easier.",documentation_debt,low_quality_documentation
spark,16607,comment,273867639,"@srowen @jkbradley updated with comments. I used the spark version to sniff the version as suggested by @jkbradley, although I'm happy to continue the conversation about the best way to handle versioning. I'm assuming this will make it into spark 2.2, so that's what I used as the cutoff.",non_debt,-
zeppelin,2217,review,109497200,"HI @andersonjonathan, you have to use `dev:helium` instead of for 0.8.0p-SNAPSHOT. It was `visdev` in 0.7.x, but changed.",non_debt,-
kafka,8319,review,395346770,"This is the primary fix. Instead of relying (hoping) on TaskManager to put the changelog reader into restoring_active, we just idempotently make sure it's in that state any time we're in partitions_assigned.",non_debt,-
ozone,28,review,335792254,Rolled it back,non_debt,-
airflow,8047,description,0,"Add Jiajie Zhong (@zhongjiajie) to committers list
---
Make sure to mark the boxes below before creating PR: [x]
---
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.",non_debt,-
airflow,1489,summary,0,AIRFLOW-105 Implement SqoopHook,non_debt,-
carbondata,3999,review,516441544,I have modified code according to your suggestion,non_debt,-
spark,11882,summary,0,[SPARK-14015][SQL] Support TimestampType in vectorized parquet reader,non_debt,-
incubator-mxnet,14592,comment,478989101,Great work! This is awesome.,non_debt,-
kafka,8532,comment,618292163,Streams system tests above are green.,non_debt,-
lucene-solr,1184,review,369106946,How come the code didn't need/use `stale` before?,non_debt,-
spark,8832,comment,142156938,retest this please,non_debt,-
kafka,8680,review,429076742,No. But we want to test the behavior about what happens during a deletion (ex: operational error).,non_debt,-
beam,509,comment,227510376,False: it is the new name that must remain. I've put it back.,non_debt,-
tvm,911,comment,366882005,Lint issues are fixed,non_debt,-
arrow,9624,comment,792870507,"Thanks, guys! I appreciate you (especially @alamb and @andygrove) for taking the time to help me navigate the process. I look forward to continuing to participate in this community!",non_debt,-
beam,9363,comment,522767781,Run Python ReleaseCandidate,non_debt,-
airflow,5847,comment,523869438,"It is, but the less load we can place on the reviewers the better. If it's possible without lots of effort anyway.
Everything in `tests/utils` is badly named btw -- they are utils _for_ tests, not tests themselves.",code_debt,low_quality_code
arrow,2887,review,232495879,Why are you converting the representation of `times` to nanoseconds? I thought POSIXct was represented internally as seconds,non_debt,-
cloudstack,3991,comment,604311192,Packaging result: ‚úîcentos7 ‚úîdebian. JID-1090,non_debt,-
beam,2753,comment,299529347,@aaltay PTAL as this is already done in Java now.,non_debt,-
incubator-weex,370,summary,0,+ [ios] update prerender logic,non_debt,-
kafka,4213,review,151000351,"But, yes, IMO these are the right settings to make and the correct way to do it.",non_debt,-
cloudstack,1542,comment,240173519,"@nvazquez @serg38 how about we remove vmware.nested.virtualization.perVM and instead if the user vm detail exists and is true, it overrides the global setting?",non_debt,-
trafficserver,6609,review,419565115,indentation here is not multiple of four,code_debt,low_quality_code
tvm,1845,summary,0,[Relay][Op] Add operators full and full_like,non_debt,-
beam,9358,review,324347562,"If it still does not work, you could 
1. re-clone a new repo of Beam
2. add UnionMergeRule
3. change `BeamCostModel.FACTORY` to `null` at https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/CalciteQueryPlanner.java#L116
Then quickly test",non_debt,-
beam,7237,comment,447180932,"I am just starting to review, but I want to get some principles out beforehand:
1. The shaded path should absolutely never be used. It is derived from the module name just to make it unique. The reason we shade is as a way to isolate ""implementation detail"" dependencies. If this ends up on an API surface that is a bug. We had some tests for this, but they have rotted.
2. We work pretty hard to avoid Guava on the API surface, since the risk of diamond dependency conflicts is very high.
3. It is OK for an IO to have its own esoteric dependencies - including Guava - if the thing it is connecting to requires it. So if it is _Cassandra_ that requires Gauva on the API surface, then it can be included in the deps.
4. For those situations where Beam wants to use Guava internally, we are (slowly) moving to depend on `beam-vendored-guava-20_0`",design_debt,non-optimal_design
hbase,1476,comment,612198215,"Intentional.
CHANGES.txt has changes up to 0.94.0 if you are interested.",non_debt,-
druid,3148,review,73070206,"maybe it's a good idea to have a separate page talking about the metrics storage format, and move this note and the explanation there",non_debt,-
incubator-pinot,5910,review,479596102,19961085-5910 review-479596102,non_debt,-
incubator-mxnet,10536,review,189708470,what specifically are you checking here?,non_debt,-
spark,17008,description,0,"In [SPARK-19669](https://github.com/apache/spark/commit/0733a54a4517b82291efed9ac7f7407d9044593c) change the sessionState access privileges from private to public, this lead to the compile failed in TestSQLContext
this pr is a hotfix for this.
N/A",non_debt,-
arrow,4473,comment,505133078,I'm working on it https://github.com/apache/arrow/pull/4649,non_debt,-
activemq,422,summary,0,[AMQ-7351]¬†Update to Apache pom parent 21,non_debt,-
flink,10753,description,0,"-->
To complete finish the translation work of this page.
This page is about Flink's Table API & SQL. Mainly showing the usage of Table API & SQL, how Flink optimize SQL queries and the difference between Blink and Flink planner. Now it has been translate into Chinese.
This change is a trivial rework / code cleanup without any test coverage.",code_debt,low_quality_code
incubator-mxnet,14173,comment,476056958,"Thanks @ptrendx @eric-haibin-lin this is no FP16 GEMM from Intel MKL/MKL-DNN till now so I think we have to fall back to FP32 GEMM or simulated FP16 GEMM (maybe from `mshadow` or other BLAS library).
The current design is good for APEX.  I am thinking about a general solution for mixed precision training, such as INT8, FP16, BF16, FP32. The different HW supports for the different data type, like INT8 in CPU and small GPU chip and BF16 on CPU and FP16 on GPU. So we need more flexibility for data type change or other library integration.
We are going to support BF16 and the ideal situation is the user can switch FP16 and BF16 based on the devices transparently.
We are looking into the changes and will give back more details later.",non_debt,-
openwhisk,1757,review,97678195,"This is unrelated to the fix applied in the invoker http call right?
But if it works now let's enable the test I agree.",non_debt,-
lucene-solr,428,review,207725540,"I worked on test program demonstrating the JDK issue and I see now that this has already been reported:  https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8177021    (we should put a comment about this in the code).  It's nice to see it has been fixed in JDK 9 (and my test program revealed that too).
During my exploration of this, I discovered that a lowercase 'z' in the pattern can parse various formats, and so can ""VV"".  I replaced the 5-'Z' patterns in the test file with a single lower 'z' and the tests passed.  FYI this part of the javadocs is helpful: https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatterBuilder.html#appendPattern-java.lang.String-  (builder has more detail than the DateTimeFormatter).",non_debt,-
kafka,2632,description,0,"Fixes https://issues.apache.org/jira/browse/KAFKA-3182 by:
* Turning off Nagle on the sending sockets to force the socket to physically acknowledge after the first write in `sendRequest`
* (Still had to do this on Linux though not on Mac) Adding a `200ms` delay between write attempts (my guess would be [Slow Start](https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start) is to blame for this, but with Nagle off 200ms should more than suffice here imo)",non_debt,-
parquet-mr,411,comment,327307548,"Nice @costimuraru. No problem with the ""copyright"". Just as long as this gets merged I'm happy (one less reason to keep our internal parquet-mr fork!). cheers!",non_debt,-
camel,1938,comment,387427803,"@koscejev , it is up to users what they want to convey in headers of camel exchange. Here, the restlet component uses various headers (if they are set before getting applied in the code). i think you can do what you want to do by using anoym processor to set as camel header. I am not sure what you mean about camel being protocol agnostic vs setting header. setting of a header value is users preference and setting special  headers like HTTP_QUERY is also user's preference as it is optional and if you set explicity, i think you should know what you need to do with path param. (it should be URL encoded in your case).
This is just IMHO. @davsclaus , @zregvart @WillemJiang @stsiano  may also have a comment on it.
@koscejev i may be getting you/your purpose wrong, i guess it would be great if you can elaborate on the issue and your test case more .",non_debt,-
trafficserver,1753,comment,298794712,Linux build *successful*! https://ci.trafficserver.apache.org/job/linux-github/1932/,non_debt,-
airflow,2587,review,137921283,`pre_process` param is missing here.,non_debt,-
ozone,633,comment,594843813,@elek @avijayanhwx Please review,non_debt,-
beam,271,review,61813319,"`TypedWrite<Void, V>`",non_debt,-
kafka,8003,review,375450650,Nice catch!  Done.,non_debt,-
incubator-doris,1642,review,313838992,99919302-1642 review-313838992,non_debt,-
apisix,3481,description,0,Signed-off-by: spacewander <spacewanderlzx@gmail.com>,non_debt,-
netbeans,172,comment,338477074,"Hi Matthias,
Thanks for your comments. Should be ready now.
Regarding JDK9 & extending the Maven binaries I fully agree. I think it's worth keeping this as close as the original Oracle donation as possible. After all this module has been tested in the field and seems to be used by other modules. Later on, once the donation is complete, we can add more features... and bugs ;-)
Thanks,
Antonio",non_debt,-
spark,18000,review,116950919,"Yes, but the problem is, it (almost) always evaluates it with NULL when the columns have dots in the names because column paths become nested (`a.b` not `` `a.b` ``) in the Parquet predicate filter up to my knowledge.
You are right for `IsNull`. I pointed out this in https://github.com/apache/spark/pull/17680#discussion_r112285883 as it looks they (almost) always evaluate it to `true` in Parquet-side but it is filtered in Spark-side. So, for input/output, it is not an issue in this case but I believe we should disable this for this case too.
I think this example explains the case",non_debt,-
accumulo,221,description,0,"* Removed log4j config being done in Java but some remains
* Logging is now configured using standard log4j JVM property
  'log4.configuration' in accumulo-env.sh
* Tarball ships with less log4j config files (3 rather than 6)
  which are all log4j properties files.
* Log4j XML can still be used by editing accumulo-env.sh
* Moved AsyncSocketAppend to start module due to classpath issues
* Removed auditLog.xml and added audit log configuration to
  log4j-service & log4j-monitor properties files
* Accumulo conf/ directory no longer has an examples/ directory.
  Configuration files ship in conf/ and are used by default.
* Accumulo monitor by default will bind to 0.0.0.0 but will
  advertise hostname looked up in Java for log forwarding
* Shortened names of logging system properties
* Removed MonitorLoggingIT as it is now difficult to setup log
  forwarding using MiniAccumuloCluster.",code_debt,low_quality_code
hadoop,650,comment,523933848,":broken_heart: **-1 overall**
This message was automatically generated.",non_debt,-
pulsar,4309,summary,0,Fixed record latency before recycling in broker-side producer handler,non_debt,-
spark,21403,comment,400979913,"yes @cloud-fan , you're 100% right, we want to treat `(...)` differently when it is in front of IN.
Here you are the previous example in Postgres:
In Oracle/MySQL you cannot create structs using `(...)` but you have to define a custom data type for structs, so this situation is prevented to happen.",non_debt,-
pulsar,5829,description,0,"Fixes #5827
Fixes #5828
UDP protocol is not working for netty connector
Added a specific handler for UDP and use Channel instead of SocketChannel in NettyChannelInitializer
Successfully tested",non_debt,-
spark,9765,comment,165553194,retest this please?,non_debt,-
tvm,3642,comment,531385242,ping @cbalint13 can you update the PR?,non_debt,-
attic-apex-core,361,summary,0,APEXCORE-496 make operator name available to StatsListener.,non_debt,-
druid,1265,review,27927153,"Aren't 1, 2 and 4 the same as the 1, Shorts.BYTES and Ints.BYTES from the part above?
This code is really confusing me...",code_debt,low_quality_code
kafka,9998,review,569278561,Nit: seems a bit redundant. Can we not assign the size in the line below?,code_debt,complex_code
bigtop,577,description,0,"The Smoke tests deployment by provisioner would be broken due to [BIGTOP-2742](https://issues.apache.org/jira/browse/BIGTOP-2742), but actually zeppelin server has been started.",non_debt,-
spark,3564,comment,71155467,"Jenkins, retest this please.",non_debt,-
iceberg,1893,review,546487254,"I tested it ,if the sql is `select * from mytable where data = null` ,it do not supports filter push down in flink,and we do not can get any data.
if the sql is `select * from mytable where data is null`, it is normal ,and It will enter the `IS_NULL` branch of switch",non_debt,-
trafficserver,1703,summary,0,Protect against LRUHash x = x,non_debt,-
flink,5863,review,190145819,"Well, we can have a completely normal exit code from the `run` execution, but the `-p` option completely ignored if we change the CLI to simply not recognize the option.
This is an extreme case, though.",non_debt,-
storm,1108,review,53563947,Done.,non_debt,-
madlib,243,description,0,"JIRA: MADLIB-1206
This commit adds support for mini-batch based gradient descent for MLP.
If the input table contains a 2D matrix for independent variable,
minibatch is automatically used as the solver. Two minibatch specific
optimizers are also introduced: batch_size and n_epochs.
- batch_size is defaulted to min(200, buffer_size), where buffer_size is
  equal to the number of original input rows packed into a single row in
  the matrix.
- n_epochs is the number of times all the batches in a buffer are
  iterated over (default 1).
Other changes include:
- dependent variable in the minibatch solver is also a matrix now. It
  was initially a vector.
- Randomize the order of processing a batch within an epoch.
- MLP minibatch currently doesn't support weights param, an error is
  thrown now.
- Delete an unused type named mlp_step_result.
- Add unit tests for newly added functions in python file.
Co-authored-by: Rahul Iyer <riyer@apache.org>
Co-authored-by: Nikhil Kak <nkak@pivotal.io>
Closes #243",code_debt,low_quality_code
arrow,8325,review,499664483,"Both seem fine, fortunately.",non_debt,-
superset,12147,comment,748477405,"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/12147?src=pr&el=h1) Report
Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags#carryforward-flags-in-the-pull-request-comment) to find out more.
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/12147?src=pr&el=continue).",non_debt,-
kafka,658,comment,163534193,Isn't this scenario covered by the `requestUpdate()` in `Sender.run()`  which checks whether there are unknown leaders when there is data to send?,non_debt,-
drill,480,comment,210564893,@hnfgns can you please review?,non_debt,-
kafka,3765,review,144588126,"Btw, there are some really long lines in this PR. Our convention is that lines should not be longer than the GitHub review window.",code_debt,low_quality_code
zeppelin,3432,review,317863384,It's better to use `get` to load note and verify it.,non_debt,-
spark,22642,comment,427431611,@dongjoon-hyun Ah.. got it.. thanks a lot.,non_debt,-
zeppelin,1187,comment,232949145,"@zmhassan Hi zmhassan. As I mentioned in a JIRA issue, this is in progress by @zjffdu. Could you concede? @zjffdu already contacted me for a few days ago for this issue and I asked for him to work it after I merge ZEPPELIN-1012. I hope he already worked and is waiting for merging ZEPPELIN-1012. Sorry for late reply, and duplicated works.",non_debt,-
hadoop,1220,comment,518669750,+1 looks like a trivial change. Thanks.,non_debt,-
flink,1591,description,0,20587599-1591 description-0,non_debt,-
incubator-mxnet,16977,review,361060594,"Fixed, thanks",non_debt,-
flink,2060,comment,225739626,"Sorry for delay, got busy. 
Thank you @StephanEwen for your review.updated.Please have a look.thanks cc @greghogan",non_debt,-
arrow,8542,review,521379723,I would expect those additional values to also be part of `y`.,non_debt,-
camel-quarkus,280,description,0,193065376-280 description-0,non_debt,-
incubator-pinot,304,review,71789211,Trying having a parent class called JobSpec. AnomalyJobSpec and MonitorJobSpec should inherit from the parent class,non_debt,-
spark,15580,review,84442086,now we never use the `context` and `extension` parameters?,non_debt,-
lucene-solr,133,summary,0,LUCENE-9069: Prevent memory leaks in PerFieldAnalyzerWrapper,design_debt,non-optimal_design
openwhisk,4756,review,371010485,"Agree that ordering shouldn't matter in general - without inspecting those tests, I wonder if the tests you're referring to are actually looking at precedence order (which properties should win when there are conflicts between two maps).",non_debt,-
spark,24851,review,307285458,"No, relationships have their own identifier to allow for multiple edges between the same pair of nodes.",non_debt,-
shardingsphere,1573,description,0,"For #1482, ShardingTransactional annotation support switch transaction type for proxy.
Changes proposed in this pull request:
- Send switch transaction type SQL to sharding-proxy.
- Support DataSourceTransactionManager while using MyBatis.
- Support JpaTransactionManager while using Hibernate.
- Unit test",non_debt,-
spark,16664,review,99518656,Nit: -> `.map(Utils.classForName)`,non_debt,-
lucene-solr,900,description,0,50229487-900 description-0,non_debt,-
spark,26397,review,343143873,This is basically the same approach used elsewhere right? if so that's fine.,non_debt,-
pulsar,3912,review,269441199,The current version in master branch is actually 2.4.0-SNAPSHOT. Also we would have to make sure this is updated at each release,non_debt,-
spark,16630,review,100701441,use `~==`,non_debt,-
gobblin,88,comment,93015026,Updated.,non_debt,-
lucene-solr,1303,review,387662567,"This is fine with me but FWIW I wouldn't even bother defining it.  It has no value set aside like this; I doubt any user code would want to refer to it.  If we want to document what the default cost is, we should say so in cost()'s javadoc.  I know some devs like to make static constants for everything but IMO it's sometimes wasted ceremony.",code_debt,complex_code
spark,2350,review,17861794,"Its supposed to support copying it from other locations other then just file://, like another HDFS location.  The local variable is named badly (localPath).  Its actually reading what is specified by the user for the user app jar, spark jar, etc.
I guess it depends on the definition of remote here.  In general when running on yarn I wouldn't consider the hdfs installation on that yarn cluster as remote.   But its all in the perception.  I'm fine with the name as long as we are clear in description of what it does.",non_debt,-
tajo,643,comment,124004304,Thanks! It was my mistake. I've fixed.,non_debt,-
kafka,712,comment,169440932,@ewencp I adjusted the build to only create/publish the test jars if there are test sources.,non_debt,-
systemds,555,comment,311491581,@dusenberrymw can you please review this PR ?,non_debt,-
arrow,4542,review,293670228,it might be slightly faster to take the minimum of length and only compare that in the loop check?,code_debt,slow_algorithm
trafficserver,3010,description,0,"Susan: I need some input here if this is as expected, it seems odd that it could (before) possibly insert an empty (nullptr) entry, right ?",non_debt,-
drill,159,review,39804781,"please add a test where both _SYSTEM_ and _SESSION_ options are changed, and confirm the reset is working as expected.",test_debt,lack_of_tests
spark,23230,comment,444455224,"Oh, the original one was 3.0. Although this doc change can go to branch-2.4 alone as well, let me revert it in branch-2.4 for management simplicity.",non_debt,-
drill,1683,review,264045846,Should this be removed?,non_debt,-
tvm,6536,summary,0,[tvmc] linting error on onnx command line driver frontend,non_debt,-
guacamole-client,449,summary,0,Modify the start time of the history session to match the GUAC_DATE and GUAC_TIME in the parameter tokens,non_debt,-
incubator-doris,1931,review,332392011,field number should be ordered,non_debt,-
drill,157,review,39443333,you forgot to remove watch.start(),non_debt,-
kafka,5379,review,204563539,"Personally, I would get rid of this and use `extensionValue` and `extensionNames`. Otherwise, as @rondagostino said below, we should remove `extensionValue`.",code_debt,low_quality_code
cloudstack,1412,comment,183277819,"LGTM, simple change",non_debt,-
storm,132,comment,45357623,+1 looks good.,non_debt,-
carbondata,3330,comment,512331407,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/3838/",non_debt,-
iceberg,1587,review,520836248,You are correct `NessieNotFoundException`  refers to the `ref`. If the table were deleted it would be a conflict exception. This is similar to comparing the error modes of git if you committed to a non-existent ref compared to a merge conflict in the case of changing files in the repo,non_debt,-
tvm,7518,review,602446660,remove?,non_debt,-
ozone,1601,comment,744378432,"@smengcl Thanks the patch. @adoroszlai @avijayanhwx @GlenGeng Thanks for review, I have merged the patch.",non_debt,-
tomee,310,description,0,7748336-310 description-0,non_debt,-
incubator-mxnet,8340,comment,337756930,Agreed. I'm basically asking to keep both `fill` and `set_to`.,non_debt,-
beam,372,review,64155576,"""For running tests on {@link FlinkPipelineRunner}""",non_debt,-
griffin,520,review,314097712,make sense to me.,non_debt,-
beam,9677,comment,569907506,"@reuvenlax, done",non_debt,-
tvm,1371,review,199972342,You should enable or remove this line,non_debt,-
spark,9399,review,43590281,Let's add a comment?,documentation_debt,outdated_documentation
cloudstack,2734,comment,404056575,"@rhtyd done (force pushed again, to trigger build)",non_debt,-
brooklyn-server,967,review,194659942,done,non_debt,-
hadoop,200,review,107028894,Love it!,non_debt,-
ignite,5568,description,0,‚Ä¶eft grid,non_debt,-
superset,12022,comment,749194174,"Super nit-picky, but can we add borders to images? https://stackoverflow.com/questions/37349314/is-it-possible-to-add-border-to-image-in-github-markdown
Looks a bit rough here:
https://github.com/garden-of-delete/incubator-superset/tree/master/RELEASING/release-notes-0-38/",non_debt,-
nifi,2588,review,194380548,done,non_debt,-
cloudstack,4182,comment,654655086,"On hold - tests are failing, needs investigation and explore best case solution",non_debt,-
hive,935,description,0,206444-935 description-0,non_debt,-
netbeans,572,summary,0,"java.source.base: Fix for Java 11, update args, swap import",non_debt,-
flink,9363,comment,519560926,@godfreyhe  comments addressed.,non_debt,-
beam,6416,review,223210021,function needs to return something.,non_debt,-
nutch,484,comment,555061506,"Well, the previous non-REST test implemented a client which did not send anything to the server but just returned a successful response or (if `clusterSaturated` was set to true) a temporary failure.
But I'm ok to remove the Test class if it's too much work to rewrite it for the REST client.
I've tested the PR but the initial rounds failed for about 50% of the pages/documents:
I got it fixed by using XContentBuilder to pass document as JSON to ES client, you'll find the necessary changes in [this branch](https://github.com/sebastian-nagel/nutch/tree/NUTCH-2739). Also:
- updated the description how to upgrade the dependencies in the plugin.xml and added few exclusions of dependencies already provided by Nutch core.
- changed the default properties in index-writers.xml.template so that the indexer-elastic plugin works out-of-the-box with default settings
So far, I didn't run any tests at scale. Should be to make sure we are able to index millions of documents with the given settings.",test_debt,lack_of_tests
beam,548,summary,0,pom.xml: upgrade to dataflow version v1b3-rev30-1.22.0,non_debt,-
spark,15076,comment,247138433,"@hvanhovell  Since the optimization rule change the result (or sematics), I'd like to fix it, not to introduce more complexcity in physical layer. The saved few cycles may not worth, because people usually not use that in practice.",non_debt,-
spark,853,comment,43980689, Merged build triggered.,non_debt,-
kafka,2987,comment,299715337,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3615/
Test PASSed (JDK 8 and Scala 2.11).",non_debt,-
apisix,2089,summary,0,feature: multiple certificates support for single domain,non_debt,-
spark,8267,comment,132333980,Could you also modify the title to include java tests and python API?,non_debt,-
spark,16138,comment,274600744,@felixcheung yup! Thanks!,non_debt,-
ozone,1885,review,604637536,Done,non_debt,-
geode,3112,description,0,This fixes a potential cause of GEODE-6297 by preventing locator/server status from writing to status file if the status contains only whitespace.,non_debt,-
bigtop,188,review,108095450,@chinmaykolhatkar I've committed your change. I would like to make sure that we fixed the problem. What's the easiest way to run a test for this change?,non_debt,-
spark,20793,comment,372094829,"Ah, results are different since the number of operations are different. It may be an issue like #20630.
I am curious why test are failure when seed is changed. Of course, I understand the sequence of rand must be reproducable with certain seed value in a package or implementation.",non_debt,-
ignite,229,description,0,31006158-229 description-0,non_debt,-
incubator-mxnet,4911,comment,278487688,"Could you remove 
So it doesn't have any effect by default. I'll verify if it confuses users later",non_debt,-
drill,283,review,48051385,oops. will fix. how does everything else look?,non_debt,-
accumulo,1048,comment,475372461,It already logs the original error.  I am not sure how much value another log message adds...,non_debt,-
hive,1716,review,532486047,"Good idea, thanks!",non_debt,-
echarts,1813,description,0,Translation into English.,non_debt,-
cloudstack,4430,comment,718225326,"Environment: kvm-centos7 (x2), Advanced Networking with Mgmt server 7
Total time taken: 33180 seconds
Marvin logs: https://github.com/blueorangutan/acs-prs/releases/download/trillian/pr4430-t3096-kvm-centos7.zip
Intermittent failure detected: /marvin/tests/smoke/test_kubernetes_clusters.py
Smoke tests completed. 83 look OK, 0 have error(s)
Only failed tests results shown below:
Test | Result | Time (s) | Test File
--- | --- | --- | ---",non_debt,-
tajo,1052,review,154543606,I removed that too.,non_debt,-
drill,520,review,106658488,"This ` <exclude>javax/**</exclude>` exludes validation-api as well. Therefore it should be deleted. 
To avoid including unnecessary libraries I decided to add:
`               <exclude>javax/activation/**</exclude>`
`               <exclude>javax/annotation-api/**</exclude>`
`               <exclude>javax/inject/**</exclude>`
`               <exclude>javax/servlet-api/**</exclude>`
`               <exclude>javax/json/**</exclude>`
`               <exclude>javax/ws/**</exclude>`",code_debt,low_quality_code
spark,5434,comment,92291279,"Also, could you make the stats at the top of the page ""Waiting batches"" and ""Processed batches"" links to the corresponding sections? And the names should be consistent, so please rename them here, keep them as ""Active Batches"" and ""Completed Batches"".",code_debt,low_quality_code
lucene-solr,1394,review,401749799,I don't think it works since the logic there is to use the global field number if the field already exists in another segment. In the logic above we try to re-assign the global field number locally since it clashes with a local one.,non_debt,-
spark,20710,comment,370609796,"There isn't a currently a distinction between streaming and batch in the places where this interface is called (except in the experimental continuous processing streaming mode). The streaming engine executes a sequence of WriteToDataSourceV2Exec plans, in the same way that a sequence of unrelated batch queries would be executed. The only thing distinguishing streaming queries is that they have a custom DataSourceWriter implementation, which forwards each individual epoch to the StreamWriter.",non_debt,-
arrow,2039,review,187847713,"minor, but I'm not a huge fan of the variable name `d`",non_debt,-
storm,44,comment,38187106,"Sorry, the above is not a real issue. the latest code ""storm-buildtools/maven-shade-clojure-transformer"" have solved this problem.",non_debt,-
flink,4074,review,121618430,comment can be removed,non_debt,-
carbondata,2628,comment,420569857,"Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/420/",non_debt,-
beam,1706,review,95843283,"This worries me.
I'm ok with having the tests run with `Parameterized`, but if we do so we should still be able to determine which case we're in and set the expected exception per-test within the test based on the value of the parameters, rather than inspecting the name of the test. For example, `testConflict_runnableOnServiceAnnotation_expectIllegalStateException` doesn't demonstrate by reading the test code directly that it expects an exception.",code_debt,low_quality_code
carbondata,511,summary,0,[CARBONDATA-584]added validation for table is not empty,non_debt,-
bigtop,609,comment,592314776,"Thank you, @iwasakims , @sekikn",non_debt,-
beam,4185,review,153627791,Can you add a comment with an example of an actual value here?,documentation_debt,low_quality_documentation
spark,31252,review,560703040,"The only way `fieldScale` can make it into the dialect is by the field metadata.
It was always added prior to the previous commit (which I agree with on a fundamental level)
https://github.com/skestle/spark/commit/0b647fe69cf201b4dcbc0f4dfc0eb504a523571d#diff-c3859e97335ead4b131263565c987d877bea0af3adbd6c5bf2d3716768d2e083",non_debt,-
skywalking,1318,summary,0,Fix #1316: Exclude component-libaries.xml file when package collector jar,non_debt,-
hbase,2099,comment,660626678,20089857-2099 comment-660626678,non_debt,-
hudi,1810,review,453190335,done,non_debt,-
shardingsphere,2416,description,0,Ref #2275..,non_debt,-
carbondata,513,review,95326107,ok,non_debt,-
lucene-solr,1208,review,370881326,"From what I could find, deletedAt and deleteDelay is only set using System.nanoTime/1000000, so there should not be any mismatch from comparing times that used nanoTime and times that used currentTimeMillis",non_debt,-
ignite,8490,review,591299549,let's add `private` to all fields,non_debt,-
incubator-brooklyn,1013,review,44434065,`newConfigKeyWithPrefixRemoved` ?,non_debt,-
spark,12914,comment,217291600,"MiMa failure is clearly unrelated, jenkins retest this please.",non_debt,-
carbondata,2406,comment,399857961,"Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/6515/",non_debt,-
carbondata,2379,comment,398649225,retest it please,non_debt,-
shardingsphere,7704,review,501049860,@tristaZero I will optimize it too.,non_debt,-
pulsar,8367,comment,716102786,/pulsar-bot rerun-failure-checks,non_debt,-
trafficserver,4764,comment,461578856,[approve ci autest],non_debt,-
airflow,3992,review,224698969,Maybe up this a bit? A proper Java stacktrace can be bigger than 100 lines.,non_debt,-
drill,1561,review,239298502,"I have manually checked that all files are the same which obtained in the result of `mvn clean install -DskipTests -Papache-release`.
Looks like there are no differences, except those mentioned in Laurent's comment in the PR #449",non_debt,-
incubator-dolphinscheduler,3069,description,0,"*fix single yarn get application status failed*
deploy dolphin scheduler in single yarn hadoop cluster, 
exec a yarn job will lead npe when get the yarn app status 
*dolphinscheduler/common/utils/HadoopUtilsTest.java*",non_debt,-
spark,8153,comment,131457310,retest this please,non_debt,-
dubbo,2800,summary,0,[Dubbo-2798]fix apporiate NotWritablePropertyException,non_debt,-
kafka,2689,review,106283199,"If we match the old ProducerRecord behavior and don't include the actual record data in the toString (which I think we should change), then your proposal ends up essentially matching what I said.",non_debt,-
spark,14156,summary,0,[SPARK-16499][ML][MLLib] improve ApplyInPlace function in ANN code,non_debt,-
beam,7905,comment,465772598,Run Seed Job,non_debt,-
camel-quarkus,570,review,360695073,"I think we really need to consider option 1, eventually we may need to clean up a bit the camel's bom but I think that as long term we'll end up supporting most of the component camel supports and the chance to hit this issue again is in my opinion high",design_debt,non-optimal_design
arrow,4258,comment,490781968,"+1 lgtm
@liyafan82 , thanks for the contribution. I suggest you add a similar flag for isSet() to be optionally skipped in a separate jira to improve performance",code_debt,slow_algorithm
hadoop,1899,review,408627634,"So, the throws comments from all the javadoc in the tests, right ?",non_debt,-
superset,798,summary,0,Altering theme for more subtle alerts / labels / buttons,non_debt,-
superset,8219,review,344937948,"@willbarrett it would be a large refactor of existing functionality and all permissions model was based on using model triggers. It is definitely possible, however we should get buy-in from project maintainers & probably do it outside of this PR.",non_debt,-
systemds,765,review,189691788,remove since we added a fix in the code.,non_debt,-
spark,29596,comment,683887778,Looks good to me.,non_debt,-
beam,11564,review,419142387,Similar to the previous placeholder issue.  This is what appears when I tested the course in both IntelliJ and GoLand:,non_debt,-
spark,15970,comment,309166657,cc @hvanhovell,non_debt,-
cloudstack,3758,comment,576731060,@DaanHoogland a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests,non_debt,-
accumulo,1637,comment,656218117,@keith-turner Looks good.,non_debt,-
tinkerpop,141,review,44267313,"Typo.""the the"" => ""then the""",documentation_debt,low_quality_documentation
trafficserver,970,description,0,"Unless we copy the plugin information when creating the proxy
transaction, the %<pitag> logging field always emits ""*"".",non_debt,-
spark,14623,comment,239541850,"Yep, I changed the title.",non_debt,-
geode-native,433,description,0,Update compare strings to use 64 bit sequence Id.,non_debt,-
superset,10224,review,448684392,Added a new test for line chart to check clearing validator errors.,non_debt,-
spark,7950,comment,128094701,"@davies Using a build from the latest master, I still got the above error.
`net.razorvine.pickle.PickleException: invalid pickle data for datetime; expected 1 or 7 args, got 2`",non_debt,-
nifi,4369,review,452304677,Typo: configureSasToken(),documentation_debt,low_quality_documentation
iceberg,1143,comment,651924427,"Thanks for posting the test case that reproduces the error, @hzfanxinxin! I'll take a look and work on fixing it.",non_debt,-
hawq,178,comment,163559468,nice fix.,non_debt,-
beam,14057,summary,0,[BEAM-10961] enable strict dependency checking for sdks/java/io/kafka,non_debt,-
spark,1290,comment,67089972,"It looks like the error is:
Does it compile and run locally?",non_debt,-
tinkerpop,1309,comment,677678867,"Thanks for this - a few minor comments/nits:
1. Could you please rename tests that start with ""Test*"" to our more standard ""should*""
2. Have a look at where you might add more `final` declarations to match the code style.
3. I like the idea of integration tests with the `SimpleWebSocketServer` - very smart
4. I'm surprised you found as many places as you did where `Cluster.close()` wasn't called.
5. I don't see where the semantics of any of the `GremlinDriverIntegrateTest` tests changed so it seems you accomplished this fix without breaking behavioral changes in the driver...that's nice.
6. Maybe I missed it but was there a test in Gremlin Server to validate any of this change - perhaps it was already better tested by way of your `SimpleWebSocketServer` tests?
7. I assume you will polish up the commit history a bit on merge and squash things down to a few (or one) commits?",code_debt,low_quality_code
spark,30257,comment,722126716,Can one of the admins verify this patch?,non_debt,-
druid,2119,review,48119969,"i don't think asserts are checked in druid test runs, if you are really uncerstain about timeout and interval then you would use Preconditions. However, given that this method is private , I wouldn't worry about the checks.",non_debt,-
beam,235,review,61004234,"This guy is on the way out - it should be replaced by CountingSource, so I'm not going to go to town here.",non_debt,-
spark,10152,comment,182591988,@srowen Thanks! I will make a quick pass.,non_debt,-
hbase,921,review,357310388,include the table name,non_debt,-
nifi,4460,description,0,"Thank you for submitting a contribution to Apache NiFi.
_Enables X functionality; fixes bug NIFI-YYYY._
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
spark,20217,review,160950460,I think the following examples are self descriptive. I can emphasize it in the description too.,non_debt,-
incubator-dolphinscheduler,1633,summary,0,    fix sonarcloud bug,non_debt,-
druid,2883,comment,277517522,"LGTM, with the changes for decorate. @fjy Can you also review ?",non_debt,-
spark,22952,review,237314459,"NOTE 2: The source path should not be used from multiple **sources or** queries when enabling this option, because source files will be moved or deleted which behavior may impact the other **sources and** queries.",non_debt,-
trafficserver,6241,review,537619263,"Sorry, I did this wrong, should have tested it better.  I'll fix it.",non_debt,-
flink,12908,review,460758814,"The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of ""topic-pattern"" and ""topic"" can be specified for sources.",non_debt,-
accumulo,1450,review,355582479,"I made some suggestions about passing a KeyExtent to InitVolChooserEnvImpl.  If that works out, I don't think there would be any methods throwing UnsupportedOperationException",non_debt,-
spark,17219,comment,288655605,Jenkins test this please,non_debt,-
beam,14395,summary,0,[BEAM-11271] getDataset instead of getTable for query location infer‚Ä¶,non_debt,-
tvm,4747,review,368744581,Thanks. Will fix.,non_debt,-
flink,13885,comment,734116774,"Hi @AHeise ,
Do you think this plan is OK? If ok, I will develop the code of the current PR when I have time.",non_debt,-
flink,9203,review,306633770,"@godfreyhe , is it only enabled in CBO? I mean, users shouldn't care about what optimizer is used in Flink.",non_debt,-
spark,13778,comment,227367173,"@viirya Do we need to fix this in Spark 2.0? UDTs are private APIs and the only intended use case is Vector/Matrix UDTs for MLlib, which doesn't put vectors or matrices inside an array inside a pipeline. In Spark 2.1, we probably need a formal discussion on merging UDT into Encoder, which could completely change its implementation.",non_debt,-
fineract,502,description,0,Fix : Searching for groups while creating center is displaying groups which are already member of other center.,non_debt,-
cloudstack,3042,comment,441590415,@anuragaw please put a Fixes \#xyz ID in the description.,non_debt,-
arrow,2973,summary,0,ARROW-3798: [GLib] Add support for column type CSV read option,non_debt,-
couchdb,341,comment,137702151,For what its worth I +1. Not sure why travis is failing though.,non_debt,-
spark,20737,description,0,"Remove .md5 files from release artifacts
N/A",non_debt,-
flink,12991,summary,0,[FLINK-18710] Make ResourceProfileInfo serializable,non_debt,-
cloudstack,1813,comment,280281181,@blueorangutan test centos7 xenserver-65sp1,non_debt,-
incubator-heron,712,review,63973193,Can u break down the executorCommand(...) by invoking this method executorCommandArgs(...)? You can just copy-and-parse the snippet of code in executorCommand(...) and put it here.,non_debt,-
nifi-minifi-cpp,770,review,428501369,"Will do that in a separate PR, right?",non_debt,-
tvm,4113,summary,0,[Relay] WIP - *Do not merge* - Minor guard for Conv2D for Float32.,non_debt,-
trafficcontrol,626,description,0,67198520-626 description-0,non_debt,-
airflow,6870,review,372647002,"Back to your question,  we need to set the job state as FAILED as well. This is handled in BaseJob.run by throwing an exception.",non_debt,-
beam,9725,comment,542845583,"Filed https://issues.apache.org/jira/browse/BEAM-8414 to reenable missing checks, I don't think we need another issue.",non_debt,-
storm,970,description,0,https://issues.apache.org/jira/browse/STORM-1412,non_debt,-
beam,6822,summary,0,Add random int to table name in big_query_query_to_table_it_test.py.,non_debt,-
spark,16088,review,90581819,"then can we set it to false? `DROP PARTITION ... PURGE` is not supported in hive 0.13, setting it to false can make the partition related functions still work in older hive versions.",non_debt,-
kafka,4823,review,179269582,The same config values here are repeated in the following 3 tests.  Maybe extract to a method `getConfig()`?,non_debt,-
gobblin,2788,review,351428413,typo: Gobblin config path,documentation_debt,low_quality_documentation
incubator-pinot,4047,review,288787662,"Instead of these 3 comment lines, just point to the design document",documentation_debt,low_quality_documentation
trafficserver,2874,description,0,356066-2874 description-0,non_debt,-
spark,31477,review,585673885,"This seems like a ""progress bar"" instead of a metrics. Most likely people don't care about it because it's usually `num output rows of left` * `num output rows of right`.
Have we measured the perf overhead?",non_debt,-
arrow,6202,review,371000560,"Sure, new machine...",non_debt,-
cloudstack,222,comment,98250942,@koushik-das You sure this is needed for this PR to work? it seems to me this can be future work. Nothing will be broken more then it is now.,non_debt,-
skywalking,4211,comment,572910646,helijia join,non_debt,-
incubator-mxnet,12200,review,210763553,"I understand that checking on both versions is necessary and I'm supportive of it, but I've seen some machines images that came with no python at all and people may just randomly install only one of the python versions. To ensure an out-of-box experience for our users we should either clearly specify installation of both python versions as a prerequisite or we do it for them within the script.",non_debt,-
openwhisk,4790,review,585116849,You can use `akka.http.scaladsl.model.headers.BasicHttpCredentials` instead of creating this class.,non_debt,-
spark,1199,comment,47058601,Merged build finished.,non_debt,-
cloudstack,4215,comment,806032946,Packaging result: :heavy_check_mark: centos7 :heavy_check_mark: centos8 :heavy_check_mark: debian. SL-JID 241,non_debt,-
hive,1095,review,443062664,Rebased with master and only kept relevant changes.,non_debt,-
druid,2015,comment,162950067,I am :+1:,non_debt,-
flink,4146,review,124569928,remove extra line,non_debt,-
parquet-mr,289,summary,0,PARQUET-352: Add object model property to file footers.,non_debt,-
daffodil,332,review,389894439,"Does this logic need to be in the runtime unparsers? We can only know if a choice is hidden if it is directly referenced by a hidden group ref. But if a choice is a child of a hidden group ref then this can't know if it's hidden or not.
Do we need to just always determine a defaultable branch and pass that into the choice combiantor, and then only use that if we determine we are hidden at runtime?",non_debt,-
beam,4136,review,153940130,50904245-4136 review-153940130,non_debt,-
kafka,5451,review,207679076,"Hmm.. if the node itself satisfies `isKeyChangingOperation`, then we will return it directly right? Maybe we should fix it in findParentNodeMatching (see below)?",non_debt,-
spark,3198,summary,0,SPARK-3461. Support external groupByKey using repartitionAndSortWithinPa...,non_debt,-
skywalking,3369,comment,526433732,"Sorry, I may not have made it clear, but what I meant was to stop manually adding headers to ProducerRecord in the kafka-v1 plugin",non_debt,-
spark,29437,comment,674582324,cc @cloud-fan @viirya,non_debt,-
flink,1067,comment,135612599,"Thanks for the remind, @zentol and @StephanEwen , I should be too hurry to open this PR. I tried to fix the exception in bloom filter in this PR and verify other potential issues in hash table behind negative count number separately, obviously, there is no need to do in that way. So let's wait for Greg's response now.",non_debt,-
spark,228,comment,39641216,"Hi, @pwendell , Thank you for your comments, here is my reply
First, ""whether accumulator value should be subject to change in the case of failure""...I think no, though during a long period, Spark runs in this way (this patch is actually resolving a very old TODO in DAGScheduler.scala)...I think accumulator is usually used to take the task which is more complicate than rdd.count/rdd.filter.count (Maybe I'm wrong), e.g. counting the sum of distance from the points to a potential center in K-means (see mllib), I think in this case, the health status of the cluster should be transparent to the user, i.e. the final result of K-means should be irrelevant to whether executor is lost, etc....
Second, Good point, I can understand what the use scenario is, but do you mind providing more details like how to implement this in Spark? I guess this can be solved by providing a approximateValue API in Accumulator or SparkContext....
Third, actually, this patch ensures that the value of the accumulator in a stage will only be available when this stage becomes independent (means that no job needs it any more, https://github.com/apache/spark/pull/228/files#diff-6a9ff7fb74fd490a50462d45db2d5e11L400), if a job finishes, and the other job still needs the certain stage, the accumulator value calculated in that stage will not be counted...",design_debt,non-optimal_design
kafka,3278,review,121205276,Does this really need to be `info`?,non_debt,-
pulsar,7200,summary,0,[broker] Prevent redirection of lookup requests from looping,non_debt,-
incubator-doris,3677,review,438517645,Fix done,non_debt,-
airflow,12096,review,517611582,"It's a more efficient way to get the last item from a generator - no need to iterate over every item from the generator just to get to the last one.
It might make a difference in case of lots of events.",code_debt,slow_algorithm
airflow,7170,review,368826713,"user is doing simple click from UI, nothing selected so why should they see nasty error?",non_debt,-
iceberg,1669,comment,718305151,"In the case of a large amount of data, in order to ensure the performance of normal writing,
1. Should we use asynchronous Rewrite? (use AsyncWaitOperator)
2. Should you add a switch to control whether to  enable rewrite",code_debt,slow_algorithm
spark,4214,review,25109930,can you add a quick comment of what `true` means here:,documentation_debt,low_quality_documentation
trafodion,696,comment,246499551,Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/1121/,non_debt,-
tinkerpop,119,comment,150828153,"This looks good to me. Simple getters added.
VOTE +1.
@pietermartin --- you are really introspecting deep into the traversal. Note that we have yet to publish the virtual machine step library and thus, things like `LoopTraversal` may not be in the specification. Thus, we can't guarantee you stability of the API in this area. Just a heads up.",non_debt,-
cassandra,447,description,0,206424-447 description-0,non_debt,-
airflow,7075,comment,571462616,"yes.there is no concept of async driver status poll for other modes , read https://spark.apache.org/docs/latest/running-on-yarn.html ! in other modes the submit to launch is synchronous . i think u can cancel this @albertusk95",non_debt,-
nifi,2616,description,0,"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
flink,13595,comment,719467007,"Thanks for the updates @wsry ! 
I think most of my previous comments were addressed except for https://github.com/apache/flink/pull/13595#discussion_r514988635. And as @gaoyunhaii also mentioned above, it is better to supplement some tests for covering empty subpartition case.",test_debt,low_coverage
beam,13296,comment,725072875,Run Dataflow PostRelease,non_debt,-
spark,15539,review,83991606,"I have similar concerns. I think that the proposed, simple solution may suffice for a single-user or short-lived spark session. On the other hand, an unbounded cache here will probable lead to trouble for some users in a multi-user or long-running Spark app such as the thriftserver.
Right now I'm thinking the `InMemoryCache` is okay as-is because this feature is behind a configuration flag, but I think we'll need a resource-sensitive solution in order to support this cache in the thriftserver.",non_debt,-
spark,6210,comment,102585249, Merged build triggered.,non_debt,-
camel-website,18,comment,465485591,Thanks!,non_debt,-
cloudstack,1479,summary,0,Cloudstack-9285 exception log addition,non_debt,-
spark,28370,comment,628792196,Could you add the code to avoide the infinite retry loop on error & also checking thread interrupted incase something else swallows the thread interruption exception in the future?,code_debt,low_quality_code
beam,7710,comment,459934045,Run Python PostCommit,non_debt,-
tvm,3491,comment,508968067,Thanks @yidawang,non_debt,-
samza,606,review,215474507,Missed that there is one test class extending the StreamProcessor and override this method. Changing to VisibleForTesting as suggested.,non_debt,-
cloudstack,4815,comment,802544883,Packaging result: :heavy_multiplication_x: centos7 :heavy_multiplication_x: centos8 :heavy_multiplication_x: debian. SL-JID 174,non_debt,-
beam,1464,review,91220228,this is a very good suggestion. It doesn't have to be distinct. But I will try to make it more unique.,non_debt,-
incubator-doris,5219,review,559096882,better to add the value of max journal id and image id in response msg for easy debugging.,code_debt,low_quality_code
kafka,6236,summary,0,KAFKA-7799: Use httpcomponents-client in RestServerTest.,non_debt,-
brooklyn-server,672,review,115984092,Why not make this a `VersionedName`?,non_debt,-
hudi,417,review,202487963,"okay, let me check.",non_debt,-
incubator-mxnet,5419,review,106336023,why remove this?,non_debt,-
reef,531,description,0,"Add Geon-Woo Kim to committers
JIRA:
[REEF-784](https://issues.apache.org/jira/browse/REEF-784)",non_debt,-
superset,8814,description,0,"Choose one
A function in `superset/utils/core.py` had a circular import fix applied to it (required in utils/core due to it being depended on by the model layer) but it was only referenced in one place, in another `utils` file. Moving it to that file removes the need for the circular import fix.
@dpgaspar @craig-rueda",non_debt,-
spark,24762,review,289522137,I might just write `if (buffer == null) null else buffer.buf`,non_debt,-
tvm,4251,summary,0,Fix typo in err msg,documentation_debt,low_quality_documentation
openwhisk,2877,comment,344737281,"@rabbah, sure thing.",non_debt,-
guacamole-client,280,review,188524055,"Was just about to retest these changes but out of curiosity:
What is the effect of including a semicolon within the attribute of an AngularJS directive like this? Does Angular (intentionally) support multiple statements within expressions?",non_debt,-
incubator-pinot,1238,description,0,"In realtime consuming segment, we first add value to dictionary then update inverted index.
If we query a column with inverted index, it is possible that the inverted index has not been generated.
The fix is filter out all null inverted index.
This will not affect the accuracy of the results because we just ignore a single under-indexing record.",non_debt,-
kafka,5221,comment,399157359,"here are the results on my workstation:
the update path is somewhat slower (because the update must start by 1st copying the latest snapshot and then applying the new data to the copy), but starvation is gone.
also note that metadata codepaths in general do an awful lot of copying - seems to me that converting the entire class to java (or at least making it use 100% java collections) would avoid multiple copies going to/from KafkaApi (all those toJava() calls).",code_debt,slow_algorithm
arrow,7233,description,0,51905353-7233 description-0,non_debt,-
spark,18223,comment,306912908,I took out 17645 per https://github.com/apache/spark/pull/17645#issuecomment-306907150,non_debt,-
incubator-mxnet,14350,description,0,"This PR adds Text Sentiment Classification examples using Gluon fit() API. This PR depends on its parent PR([14346](https://github.com/apache/incubator-mxnet/pull/14346)).
JIRA epic can be found here: <https://issues.apache.org/jira/projects/MXNET/issues/MXNET-1335>
The examples has been modified from D2L Gluon [book](http://d2l.ai/chapter_natural-language-processing/index.html). This examples shows that the fit() API helps in reducing the training script by approx. 25 lines, and also the target users doesn't need to write their own training loop.
Note: We have covered some of the models, more to be added soon.  
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here",non_debt,-
iceberg,1213,review,460611333,"Well, sounds great.",non_debt,-
hive,1756,review,550923905,Fixed,non_debt,-
incubator-mxnet,13889,summary,0,fix the GPU flavor problem for Scala,non_debt,-
spark,15363,comment,285187051,"@wzhfy I've looked at the new CBO join reordering. The star schema detection can be used as follows: 
Assume a four-way join: A, B, C, D.
Star schema join detection is called before CBO. It returns **{A, B, D}** as a star join. This info can be used by the the dynamic programming as follows:
* level 0: p({A}), p({B}), p({C}), p({D})
 * level 1: p({A, B}), ~~p({A, C})~~, p({A, D}), ~~p({B, C})~~, p({B, D}), ~~p({C, D})~~
 * level 2: ~~p({A, B, C})~~, p({A, B, D}), ~~p({A, C, D})~~, ~~p({B, C, D})~~
 * level 3: {{A, B, D}, C}
At level 2 we only generate a plan for {A, B, D}. The winner plan should be **{{A, B, D}, C}**",non_debt,-
arrow,2536,review,216379398,"We probably want to make a CMake list of libraries to bundle, then iterate over that with `foreach`",non_debt,-
hbase,2726,description,0,‚Ä¶haning from meta replica to non-meta-replica at the server side.,non_debt,-
trafficserver,1864,comment,300671004,AU check *successful*! https://ci.trafficserver.apache.org/job/autest-github/521/,non_debt,-
cloudstack,1716,summary,0,CLOUDSTACK-9555 when a template is deleted and then copied over again‚Ä¶,non_debt,-
apisix-dashboard,1128,description,0,"- Why submit this pull request?
- Related issues",non_debt,-
beam,7133,comment,442169281,"Not one, two, or three flakes, but four!",non_debt,-
flink,9558,review,319847769,"Is this while loop still needed after changing the verification approach or `HistoryServer` will not monitor job files created after its start?
Also a side note, would it be a more incapsulated approach to poll `MultipleJobsDetails` similar way in this loop until the expected, non-failed state reached instead of instrumenting `HistoryServer` internals for testing?",non_debt,-
tajo,1041,review,75109669,You should add tajo-storage-kafk module to tajo-storage and add module copy-script in prepare-package,non_debt,-
pulsar,460,comment,307470264,"Fixed test #464 made [build green](https://travis-ci.org/yahoo/pulsar/builds/241298883) so, I think now we can merge this one or can trigger the build after rebasing.",non_debt,-
incubator-mxnet,6144,summary,0,Fix for invalid numpy float indexing,non_debt,-
kafka,6012,comment,456244123,"updated this with unit test showing saving records with bloom filters off, closing RocksDB then open again with bloom filters enabled, no errors and can retrieve previous records successfully.",non_debt,-
infrastructure-puppet,844,summary,0,Adding JDK10 to Windows Jenkins build nodes.,non_debt,-
kafka,1672,comment,245139145,"oops, missed the update. sorry for delay. LGTM and merging.",non_debt,-
hadoop,1041,comment,507326715,"What's the difference you see between LimitedPrivate(""management tools"") and Public? When things are LimitedPrivate(""HBase"") there's a specific community that we know to coordinate with. With ""management tools"" it's arbitrarily open, and I don't know what the compatibility guarantees we need to hold to are. I know with Hadoop 3's compatibility we technically violated compatibility guidelines with the original metrics framework being removed before it had been deprecated for a full release, because metrics were also seen as a similar ""administrative"" or ""auxiliary"" interface and it didn't need to really be ""public"". If we're doing that again, I think we can still set expectations more clearly.",non_debt,-
spark,18014,description,0,"This PR enhances `ColumnVector` to keep `UnsafeArrayData` for array to use `ColumnVector` for table cache (e.g. CACHE table, DataFrame.cache). Other complex types such as Map and struct will be addressed by another PR if it is OK.
Current `ColumnVector` accepts only primitive-type Java array as an input for array. It is good to keep data from Parquet.
This PR changed or added the following APIs:
`ColumnVector ColumnVector.allocate(int capacity, DataType type, MemoryMode mode, boolean useUnsafeArrayData)`
* When the last is true, the `ColumnVector` can keep `UnsafeArrayData`. If it is false, the `ColumnVector` cannot keep `UnsafeArrayData`. 
`int ColumnVector.putArray(int rowId, ArrayData array)`
* When this `ColumnVector` was generated with `useUnsafeArrayData=true`, this method stores `UnsafeArrayData` into `ColumnVector`. Otherwise, throw an exception.
`ArrayData ColumnVector.getArray(int rowId)`
* When this `ColumnVector` was generated with `useUnsafeArrayData=true`, this method returns  `UnsafeArrayData`.
Update existing testsuite",non_debt,-
spark,26980,review,360782760,Hmm...This may can be a problem...,non_debt,-
camel,2908,comment,491535472,"It's not clear for me: cost of what was saved?
I can only see new cost of creating bunch of new, unrelated `Random`s every time `process` or `attempt` are called.
There is no performance evidence or analysis provided in [CAMEL-13499](https://issues.apache.org/jira/browse/CAMEL-13499) neither.
I'd like to learn more about this issue.
From jira:
And if it isn't? Such control, IMO shall be introduced when needed, as it might never come.",non_debt,-
druid,1076,comment,72532858,@cheddar I believe the latest revision addresses your comments,non_debt,-
tvm,4657,comment,572378913,sounds good @FrozenGene  i agree with all your points,non_debt,-
spark,7298,comment,119747662,Thanks - merging this.,non_debt,-
nifi,1279,comment,263946052,@mcgilman good catch. Going to push another commit to address this.,non_debt,-
spark,30214,summary,0,[SPARK-29574][K8S][2.4] Add SPARK_DIST_CLASSPATH to the executor class path,non_debt,-
spark,29566,comment,682280358,"@maropu - cool, thanks.",non_debt,-
airflow,8974,review,436378446,Yeah. I removed it. We have fully backwards-compatible behaviour now.,non_debt,-
airflow,8962,review,441010609,33884891-8962 review-441010609,non_debt,-
spark,26656,review,350726522,We still need the tests for HashAggregateExec here?,non_debt,-
spark,11152,review,52630690,"maybe mention both date formats under `minDate`, and then just say this can take the same formats as `minDate`?  It should be pretty obvious that both versions work for either option, but just to be a little more clear.",non_debt,-
flink,10399,review,353576967,Ditto.,non_debt,-
kafka,616,comment,167126036,@ewencp Rebased and updated based on comments,non_debt,-
spark,7509,comment,122723571,"Oh, code conflict, actually we'd better let this PR merge asap. as some of my PR will depends on this.",non_debt,-
spark,22421,review,217792214,?,non_debt,-
trafficcontrol,4959,review,470844261,I hope someone can figure out why I couldn't get that to work. It was *very* confusing.,code_debt,low_quality_code
hudi,1727,review,438953612,pull this into a helper like `HoodieSparkEngineContext.getSparkContext(engineCtx)` ?,non_debt,-
kylin,1237,comment,635086959,"# [Codecov](https://codecov.io/gh/apache/kylin/pull/1237?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/kylin/pull/1237?src=pr&el=continue).",non_debt,-
spark,1222,comment,58242932,"Jenkins, retest this please.",non_debt,-
lucene-solr,1157,comment,573680544,"[patch.txt](https://github.com/apache/lucene-solr/files/4053932/patch.txt)
I reworked a few things, Mike. Now... rat hangs for me (?). I wonder if we could bypass ant and use rat classes directly - the conversion between gradle and ant filesets is a bit clumsy (it's fine as a first draft though!).
Inclusion/ exclusion patterns have to be reviewed and consolidated with ant: some projects override these, gradle should exclude project-local build/**, etc.",non_debt,-
accumulo,25,comment,91049205,How do these changes preclude you from later adding in your proposed split generation function?,non_debt,-
superset,4620,comment,373494350,"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/4620?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/4620?src=pr&el=continue).",non_debt,-
skywalking,191,review,115411237,You miss `segment.trim()` and avoid empty string.,non_debt,-
kafka,151,review,38279239,Should we mock the other new method disconnect() too?,non_debt,-
spark,14914,review,77128029,"Writing it as `data += (i, j + startCol, v)` would yield compilation errors:
thus here it's written as `data.+=((i, j + startCol, v))`",non_debt,-
openwhisk,3696,comment,391734844,"@csantanapr Yes, I changed that line.",non_debt,-
carbondata,4045,comment,803626913,"Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12602/job/ApacheCarbonPRBuilder2.3/5068/",non_debt,-
kafka,1673,comment,235947414,@ijuma,non_debt,-
ignite,4519,description,0,31006158-4519 description-0,non_debt,-
flink,15313,review,603262596,Extract that.,non_debt,-
spark,17459,review,118818160,"Nit: can the last two args simply be m, n for clarity?",code_debt,low_quality_code
hive,335,review,187593991,"Instead of looking for parent in this filter why don't apply the filter on first listStatus done on loadPath?
Also, should skip the files even if the name matches the prefix.",non_debt,-
flink,6698,review,223291788,nit: how about `localSubpartitions.forEach(ResultSubpartition::flush);`?,non_debt,-
spark,19222,review,179397907,"As the implementation of `subBlock` returns something like `new OffHeapMemoryBlock(this.offset + offset, size)`, doesn't it be relative offset to original offset? `new absolute offset` looks incorrect.",non_debt,-
spark,11349,comment,189473112,Merging to 1.6,non_debt,-
netbeans,1021,comment,441019332,"Thank you for looking at it, Svatopluk!",non_debt,-
hbase,809,review,344314816,"nit: Always better to use parameterized logging for performance [1]. Here info is the default, so probably doesn't matter as much. 
[1] https://logging.apache.org/log4j/2.x/performance.html",code_debt,slow_algorithm
kafka,2476,review,103751509,@ijuma Have we explicitly stated in the code or documentation that this class is internal and should not be used by users? Becket is not aware of this as well. It seems that user can already construct Scala AdminClient directly and that is how I expect this API to be used.,non_debt,-
spark,17248,summary,0,[SPARK-19909][SS] Batches will fail in case that temporary checkpoint dir is on local file system while metadata dir is on HDFS,non_debt,-
spark,5361,summary,0,"[SPARK-6661] Python type errors should print type, not object",non_debt,-
daffodil,19,comment,352417409,+1,non_debt,-
spark,4788,description,0,"Join on output threads to make sure any lingering output from process reaches stdout, stderr before exiting
CC @andrewor14 since I believe he created this section of code",non_debt,-
camel-k,2125,summary,0,Create new test folder with kamel cli specific tests,non_debt,-
spark,1313,comment,50300190,"I see, in that case, maybe you can remember a Boolean the first time you saw your tasks to see whether some of them had hostnames specified but not executors. Those are the ones we're worried about.",non_debt,-
activemq-artemis,1231,comment,297753130,36057260-1231 comment-297753130,non_debt,-
thrift,1586,comment,450271533,Based on the comments it looks like more work is needed and this needs to be rebased.  Please complete this.,non_debt,-
spark,21473,comment,394487970,"thanks, merging to master!",non_debt,-
flink,11626,review,404654699,Seems to be correct given `mvn dependency:tree -Phive-3.1.1`.,non_debt,-
spark,2851,comment,70644262,ping,non_debt,-
spark,26738,comment,560665325,@dongjoon-hyun can you take a look?,non_debt,-
tvm,5154,review,399416785,a/and/is ?,non_debt,-
incubator-pinot,659,review,82649073,why should we order it according to schema order.,non_debt,-
spark,16998,comment,281042949,@hvanhovell Yes. #16785 only does a limited improvement. Both #16785 and this are non-parallel approach.,non_debt,-
incubator-mxnet,17465,comment,579535105,@anirudh2290 can you please review,non_debt,-
incubator-mxnet,14409,review,265662688,This diff should not be displayed. Can you try rebasing?,non_debt,-
kafka,5428,review,208741696,Should we check the root cause?,non_debt,-
incubator-mxnet,19896,comment,799899842,@mozga-intel let me know once the comments are addressed and I can take another look then.,non_debt,-
cloudstack,720,comment,132769493,"[cloudstack-pull-analysis #288](https://builds.apache.org/job/cloudstack-pull-analysis/288/) SUCCESS
This pull request looks good",non_debt,-
flink,11671,review,407302559,"hi @aljoscha, do you have any other concern about this ?",non_debt,-
tinkerpop,889,comment,408684427,"@jorgebay I've modified the Sasl Authenticator so that a mechanism can now be passed in along with any mechanism options. I've changed the Sasl Plain Authenticator to mimic the Java handler code as discussed, however I had to do some fudging as I couldn't get the flow to work right. 
From my understanding of the Java code the Driver/Handler/GremlinSaslAuthenticationHandler class receives a 407 from the Gremlin Server and then sends a message containing { saslMechanism: 'PLAIN', sasl: (A base64 encoded null byte string) }, the server then returns an initial response and the client then sends the actual sasl authentication message... I tried that and I was getting a message that ""Authentication ID must not be null"". Having looked at the Gremlin Server code and the Handler/SaslAuthenticationHandler class and the Auth/SimpleAuthentication class it doesn't appear that saslMechanism token is actually parsed and the the server expects the actual sasl message right away. So, currently, in the JS SaslAuthenticator class I've passed the full Sasl message with the initial SaslMechanism argument. 
I hope that makes sense and if it needs changing I'm happy to do so.",non_debt,-
hudi,2048,review,488345849,"Yes,  we do delete and insert. Definitely, would be helpful to have someone with more experience review this.",non_debt,-
beam,6343,description,0,"MustFollow can be used to force processing to wait for another stage to complete, when a direct PCollection dependency does not exist.
The unit test only checks pass-through correctness.  I have tested the actual sequencing on DirectRunner and FlumeRunner, and believe the mechanism should be portable to other runners.
This could also be added as a call on ParDo, but providing a standalone PTransform seemed cleaner.
The series of operations adds more to the execution graph than I'd like; a dotted-line dependency would be better.  Not sure if this is easy to fix though.
@robertwb 
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) </br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | --- | --- | --- | ---",non_debt,-
arrow,8503,review,510061337,"I wonder if picking a fixed channel size might be a good idea. Specifically, given there is s one  That way the producers can't get too far ahead. I think a small fixed number (`number_of_cores`? 10?) might be reasonable to begin with
Given that the actual work of `MergeStream` is pretty light (simply passing the record batches on) this may not be a problem in practice, but it probably depends on what order the tasks are run.",non_debt,-
spark,28016,comment,604128330,Retest this please.,non_debt,-
incubator-mxnet,5558,description,0,"This PR fixes the problem of SliceChannel operator only accepting data type float32. It should support any data types supported in MXNet.
https://github.com/dmlc/mxnet/issues/5504
Test script:",non_debt,-
airflow,11589,review,516156770,Will it work for operators that inherit from `DummyOperator`?,non_debt,-
reef,858,description,0,"Apache Mesos 0.27.1 resolves the problems in 0.27.0.
Now, REEF integration test passes on Apache Mesos 0.27.
JIRA:
  [REEF-1213](https://issues.apache.org/jira/browse/REEF-1213)
Pull request:
  This closes #",non_debt,-
incubator-brooklyn,525,comment,75658504,"[incubator-brooklyn-pull-requests #827](https://builds.apache.org/job/incubator-brooklyn-pull-requests/827/) SUCCESS
This pull request looks good",non_debt,-
spark,27437,comment,581170246,"Hi, @zero323 . Thank you always for your contribution.
Since you are active contributor, I want to give two recommendation.
1. You had better use `[MINOR]` if you don't have a JIRA issue. This is a convention and irrelevant to `JIRA Priority`.
2. For documentation PR, `[DOCS]` is recommended.
In short, you had better revise your title to `[MINOR][SPARKR][DOCS] Remove ...` . I didn't change this PR because I want you to learn for both this and your future PRs.",non_debt,-
incubator-mxnet,8732,comment,347630910,Can resource_request also be added to ndarray.cc? Others may use ElementwiseSum as a black box. Also see https://github.com/apache/incubator-mxnet/blob/master/src/ndarray/ndarray.cc#L667 which request temp resource,non_debt,-
kafka,4579,description,0,"https://github.com/apache/kafka/pull/4356 added `batch.size` config property to `FileStreamSourceConnector` but the property was added as required without a default in config definition (`ConfigDef`). This results in validation error during connector startup. 
Unit tests were added for both `FileStreamSourceConnector` and `FileStreamSinkConnector` to avoid such issues in the future.",non_debt,-
kafka,6234,comment,462503916,"ping. it'd be good to get this in since it'll help with debugging tests that flake pretty consistently on jenkins (though we may also need some follow up to turn up logging output during tests, I think a bunch of modules don't even have logging turned on).",test_debt,flaky_test
flink,2221,comment,233556343,I will merge this later today if there are no objections...,non_debt,-
netbeans,1617,comment,549980784,"I think this is safe for a point release. In the worst case the exception is exchanged for wrong syntax highlighting, but the user should then still be able to work with his/her file.",non_debt,-
spark,21356,review,189349209,Not used anymore.,code_debt,dead_code
flink,3380,comment,286807853,"I think you are right about the argument that we do not expect users to ""handle"" the exceptions in any way, so they may as well be unchecked exceptions.
Should we then throw `FlinkRuntimeException` or define a specific `StateException extends FlinkRuntimeException` that we throw there?",non_debt,-
skywalking,5103,description,0,45721011-5103 description-0,non_debt,-
ambari,860,comment,378632098,retest this please,non_debt,-
cloudstack,1017,comment,152850429,@bprakg is this a mistake? can you close it?,non_debt,-
apisix-dashboard,218,review,431547403,",",non_debt,-
hudi,830,review,312762266,"this also is something we should note down to remove down the line.. for now, makes sense..",non_debt,-
flink,3520,description,0,"This PR is based on #3166 , and added following changes:
1. Refactor `RexProgramExpressionExtractor` and `RexProgramExpressionExtractor` to `RexProgramExtractor` and `RexProgramRewriter`. `RexProgramExtractor` is responsible for retract either projection expressions or filter expression.
2. Make sure we don't fail during extracting and converting filter RexNodes to expressions. The expressions which successfully translated and unconverted RexNodes will both be returned.
3. Add some tests for `RexProgramExtractor`.
4. Provide unified `PushFilterIntoTableSourceScanRuleBase` to support filter push down in both batch and stream mode.
5. Add some logical tests for filter push down in different situations, like fully push down and partial push down.
6. Slight change of testing class `TestFilterableTableSource` to make it less specialized.
Another thing to notice is that UDF expression can not be translated from RexCall to Expression yet, since the name is not consistent with the name in FunctionCatalog due to the change of #3330. We can revisit this issue later since it's not very practicable for a TableSource can understand and execute UDF registered in Flink.",non_debt,-
spark,16452,comment,270048956,"@mpjlu This is the behavior I get:
So, it throws an exception when nothing is set, as intended it seems.",non_debt,-
spark,3121,comment,62032343,"@JoshRosen I don't think the situation is quite a dire as you suggest (every line of test code?).  We can add logic to `QueryTest` and the other base test classes that creates a `SQLContext` per suite with whatever `SparkContext` you want.  We can then turn the data objects into traits that are mixed into the test cases they need.  As long some SQLContext is in scope, and the required tables are added to that context during the constructor I don't anticipate any major problems.
Hive is going to be another story.  The whole reason for this singleton context pattern is that we have problems initializing more than one HiveContext in a single JVM.  If you try to do that all DDL operations fail with a mysterious `Database default does not exist` error.  We have never been able to figure out what sort of global state Hive relies on (though admittedly it has not been a very high priority since a global context with a robust `.reset()` has worked pretty well so far).",non_debt,-
reef,851,comment,187919912,@shulmanb I've done a coding style-level review.,non_debt,-
spark,31522,review,580996896,"Instead of adding a new method, we can probably just add one more parameter here. Then we don't need to worry about the execution order of `processCommitDuration` and `processStats`.",non_debt,-
beam,11053,comment,595046910,Run Python PreCommit,non_debt,-
openwhisk,3592,comment,384964071,"# [Codecov](https://codecov.io/gh/apache/incubator-openwhisk/pull/3592?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-openwhisk/pull/3592?src=pr&el=continue).",non_debt,-
airflow,4967,comment,477355709,@XD-DENG @zhongjiajie - Please let me know if you have more comments/recommendations and should I resolve your comments.  Thanks,non_debt,-
spark,8600,comment,239015776,How can we combine two columns with different values?,non_debt,-
carbondata,2878,summary,0,[CARBONDATA-3107] Optimize error/exception coding for better debugging,code_debt,low_quality_code
flink,15074,description,0,"Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project flink-test-utils-junit: ExecutionException
flink 1.7 not  find 
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.table.api.java.StreamTableEnvironment;",non_debt,-
trafodion,631,comment,236953400,It would be good to add some summary info about JIRA in the same line as the [TRAFODION-1986]. Otherwise it just shows an empty subject.,non_debt,-
cloudstack,3984,comment,603825002,Packaging result: ‚úñcentos6 ‚úîcentos7 ‚úîdebian. JID-1086,non_debt,-
flink,9082,review,340395215,throw `IllegalArgumentException` or `UnsupportedOperatorException` makes more sense to me.,code_debt,low_quality_code
spark,18465,comment,312939982,"Sounds good to.check. I will be back after investigating. BTW, I guess the original state does not handle that case too.",non_debt,-
tvm,629,comment,344140131,CUDA has __popc and __popcll http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__INT.html#group__CUDA__MATH__INTRINSIC__INT_1g43c9c7d2b9ebf202ff1ef5769989be46,non_debt,-
kafka,3812,review,138736632,"All nodes in the cluster have access to the global connector configuration and can respond to REST apis, but the `Worker` class only tracks the connectors being executed on that particular node. To find the type of a connector which is not executing on that node, we probably have to pull the classname out of the config using `ConnectorConfig.CONNECTOR_CLASS_CONFIG`.",non_debt,-
spark,10047,review,46215657,Mention `SPARK-12058` in the message.,non_debt,-
incubator-mxnet,18525,comment,653954287,Ping @eric-haibin-lin regarding the tracking issue,non_debt,-
spark,23714,description,0,"The file source has a schema validation feature, which validates 2 schemas:
1. the user-specified schema when reading.
2. the schema of input data when writing.
If a file source doesn't support the schema, we can fail the query earlier.
This PR is to implement the same feature  in the `FileDataSourceV2` framework. Comparing to `FileFormat`, `FileDataSourceV2` has multiple layers. The API is added in two places:
1. Read path: the table schema is determined in `TableProvider.getTable`. The actual read schema can be a subset of the table schema.  This PR proposes to validate the actual read schema in  `FileScan`.
2.  Write path: validate the actual output schema in `FileWriteBuilder`.
Unit test",non_debt,-
spark,5011,comment,78962544,ok to test,non_debt,-
pulsar,4815,description,0,A message doesn't get automatically ACKed if there is the output topic is not set,non_debt,-
calcite,2189,description,0,https://issues.apache.org/jira/projects/CALCITE/issues/CALCITE-4251?filter=recentlyviewed,non_debt,-
tvm,5186,review,403298913,ditto,non_debt,-
camel,1406,review,96633419,"cab you precise what is expected as ""data type""? is it a classname? is a value taken from somewhere elese?",non_debt,-
cloudstack,2190,comment,319162685,@blueorangutan package,non_debt,-
trafodion,813,review,86404383,This buffer is too short.,non_debt,-
drill,1892,review,393135561,Fixed.,non_debt,-
arrow,5207,comment,528614954,"@bkietz why remove those DCHECKs? I think it's a good idea to have them, they caught a legitimate issue here (which could be memory corruption, but...)",non_debt,-
druid,7647,review,283550456,Is `lenghth` a typo here?,documentation_debt,low_quality_documentation
storm,1726,comment,256511956,+1 Thanks for adding the documentation @revans2 .,non_debt,-
carbondata,4074,comment,767821108,retest this please,non_debt,-
nifi-minifi-cpp,979,review,576201597,should we log the exception here?,non_debt,-
beam,6101,comment,412692922,Retest this please,non_debt,-
tvm,5689,comment,635675471,@comaniac @mbrookhart good to go?,non_debt,-
flink,4279,comment,314653963,"Hi @wuchong  Thanks for the update.
+1 to merge.",non_debt,-
spark,26651,comment,558927022,thanks for merging,non_debt,-
incubator-mxnet,10550,summary,0,[MXNET-320] Support elemwise_add/sub between dense and csr tensors,non_debt,-
druid,36,review,2270352,Are we guaranteed that this string of 3 get().remove() calls will never NPE after the get() and before the remove()?,non_debt,-
kafka,7984,review,372683286,As above. Simplify to:,code_debt,complex_code
zeppelin,3100,comment,408333428,I am afraid currently we can not do this. Because some unit test in zeppelin-zengine & zeppelin-server depends on spark interpreter (ZeppelinSparkClusterTest).,non_debt,-
arrow,6156,comment,667456601,"@tianchen92 would you mind starting a thread on the ML, it seems that @jacques-n might not have bandwidth.",non_debt,-
spark,9031,comment,147046856,"@adrian555, yes, attach() is useful in R. What I mean is that supporting attach() for DataFrame by allowing direct use of column name is so useful? User can simply use <df>$<colname>?",non_debt,-
cassandra,716,review,476606951,I think this mean we now have 2 tests named `TupleTypeTest`.,non_debt,-
airflow,12530,review,528867848,"looks like it's calling `super().get_serialized_fields()`, so we should be good for ExternalTaskSensor",non_debt,-
storm,3381,summary,0,exclude org.glassfish.web:javax.servlet.jsp from storm-autocreds hbase-server dependency,non_debt,-
druid,10267,review,600967002,"I guessed it was because the metadata changed. By that, I mean the `org.apache.druid.segment.Metadata` object stored in the segment, which contains the TimestampSpec.
It adds up, I think, since -1 character is the difference between the old default `timestamp` + `auto` (13 chars) and the new default `__time` + `millis` (12 chars).",non_debt,-
pulsar,4311,review,285428211,is this relevant to this PR?,non_debt,-
qpid-dispatch,1027,comment,777126228,This is the first part - creating a generic Q2 unblock handler.  Follow up commits will use this to enable Q2 backpressure on HTTP/1.x messages.,non_debt,-
spark,3778,review,22297307,to remove this if,non_debt,-
druid,2806,review,59071300,the default should be made a constant somewhere instead of being defined in multiple places,code_debt,low_quality_code
spark,19495,review,144992379,nit: extra line,code_debt,low_quality_code
hbase,2810,comment,751321209,"`executeProcedures()` is taken care of internally using `preExecuteProcedures()` I think, but feel free to include `clearSlowLogsResponses()`.
Thanks @lujiefsi",non_debt,-
activemq-artemis,3104,comment,621179877,"I am not sure this is not a WARN at all... it happens all the time...
-1000
this is supposed to happen that way... the TransactionManager will commit the Transaction from its own session in a lot of instances.",non_debt,-
trafodion,1401,comment,358168176,"jenkins, retest",non_debt,-
camel-quarkus,1159,summary,0,deps: update kotlin to v1.3.72,non_debt,-
airflow,10947,review,497376687,I just pushed commit without it.,non_debt,-
incubator-pinot,5081,summary,0,[TE][subscription] Fix duplicate anomaly report issue and clean up subscription pipelines,non_debt,-
trafodion,405,description,0,"NATableDB is caching a pointer to a HiveClient_JNI object
client disconnects.  Fixing this by keeping the HiveClient_JNI around
across sessions.",non_debt,-
airflow,1435,comment,215288030,"Much thanks for the in-depth review!
Is the scenario you are worrying about (two workers running the same task instance) already possible? For example if a worker's communication with the DB gets interrupted, then the scheduler assigns the task instance to a new worker, and then the communication between the initial worker and the DB resumes.
This makes sense. I misspoke in the PR description though, SLAs should still be sent, the difference would be the SLA email would now omit task instances in the dagrun that didn't succeed for reasons other than depends_on_past not being met (e.g. a task that couldn't run because it's pool was full won't get reported in the email). I think I'm going to just include all tasks that don't have a successful status in the SLA miss email, even those stuck on depends_on_past to align with your criteria (if the task caused core_data to not be delivered by 9AM the task caused the DAG to miss it's SLA regardless of it's depends_on_past_dependency), plus is stops treating depends_on_past differently from the other dependencies like the pool being full. LMK what you think.
Agreed about the efficiency, was going to look into caching if this causes perf issues.
The newfound power of the force flag could be used instead of ignore_depends_on_past, but making ""force"" the default for every backfill could potentially be a bit dangerous as users could e.g. unintentionally force run over a large range of already successful tasks in a backfill or violate a pool constraint. If you have any ideas let me know.
Agreed about not passing in a bunch of different flags. There is actually a TODO above that part of the code in the PR to use a context parameter instead (it will be addressed in this PR).
For the flag upstream_failed I would prefer to leave the fix for another PR since it was an existing hack and the cope of this PR is already a bit dangerously large.",code_debt,low_quality_code
hive,1261,description,0,"Fixed Binary data type in beeline rows to encode to Base64
https://issues.apache.org/jira/browse/HIVE-23856",non_debt,-
accumulo,368,review,165091158,You may be able to check expected parameters with easymock.,non_debt,-
systemds,963,comment,640634259,This is a temporary PR -- opened to verify if tests are passing.,non_debt,-
beam,12241,review,454528596,I would add a comment above to state the assumption. But it's minor though.,documentation_debt,low_quality_documentation
helix,653,review,362586939,I still think we should pass null if HelixParticipantProperty is not there. Having an object but no value does not really benefit us.,non_debt,-
kafka,1608,description,0,"Since current additivity setting of `kafkaAppender` is `true` (default value), This causes duplicated logs in the `kafkaAppender` and the `root` appender.  
It would be better default log4j configuration for **production env** if the additivity value of `kafkaAppender` is `false`",non_debt,-
accumulo,371,description,0,"Added a check to the configuration sanity checker to ensure that, if a crypto module other than NullCryptoModule was selected, a
SecretKeyEncryptionStrategy other than NullSecretKeyEncryptionStrategy must also be selected (and vice versa).",non_debt,-
geode-native,615,review,443683084,"I tend to always work in an IDE that supports project view (Visual Studio, Xcode), wherein I already know I'm working with a test project. Hence the Test suffix is a bit redundant. Also, we already have 4 tests in the new framework that don't use the Test suffix.",test_debt,expensive_tests
tvm,1973,review,243703973,"no switch is necessary, you can use pf->CallPacked",non_debt,-
airflow,4348,summary,0,[AIRFLOW-XXX] Add section to Updating.md regarding timezones,non_debt,-
drill,348,comment,177013054,+1 tested the change by swapping the 1.5 mongo storage plugin jar with the 1.4 one and it worked.,non_debt,-
arrow,2366,review,208745187,Moved it back. We should address de-unittest-ing the tests in a different PR if we want to do that,non_debt,-
incubator-pinot,484,comment,244438160,"Travis already came back clean, skipping Travis for just the file name change (non-functional) to address comments.",non_debt,-
flink,2807,summary,0,[FLINK-4631] Prevent some possible NPEs.,non_debt,-
thrift,139,summary,0,Implement Thrift.Protocol.prototype.skip() for JavaScript library,non_debt,-
ambari,2707,description,0,"Improve `KerberosDescriptorResourceProvider`:
 * Return HTTP 409 if trying to create duplicate `kerberos_descriptor` instead of HTTP 500 with ugly stack trace
 * Clarify message for incomplete request
 * Clean up the unit test
 * Minor clean-up in `KerberosDescriptorResourceProvider`
https://issues.apache.org/jira/browse/AMBARI-25025
Added test case in unit test.
Tested manually:",code_debt,low_quality_code
hudi,360,comment,377576044,"@n3nash : Thanks. Added both Unused and Redundant Imports in checkstyle and corresponding code-style. If there are any other rules missing, we can add them in future PRs.",code_debt,low_quality_code
flink,1553,comment,191752181,"Sorry, I forgot a `groupBy()` in my example.
It should be",non_debt,-
zookeeper,32,comment,110500112,"Fyi, https://issues.apache.org/jira/browse/ZOOKEEPER-2211",non_debt,-
tvm,1716,summary,0,[NNVM][KERAS] Fix keras model converter and improve tutorial,documentation_debt,low_quality_documentation
superset,6030,summary,0,Upgrade flask-appbuilder to latest.,non_debt,-
trafficserver,1717,comment,296241153,FreeBSD11 build *successful*! https://ci.trafficserver.apache.org/job/freebsd-github/1966/,non_debt,-
guacamole-client,375,review,261795640,"If this is the actual translation, it can stay, but this looks like an untranslated string?",non_debt,-
flink,10748,review,362788785,"That would violate our definition of begin well-defined; from the javadocs:
`no 2 options exist for the same key with different descriptions/default values`
The organization into separate classes that we have is only for developer convenience and readability only; it does not (and must not) have any semantics attached to it. Hence the containing class doesn't matter.
The reason for this is simplicity; we don't have to worry about
* options clashing in their default value, which is difficult to document in a good way and can lead to subtle issues when de-duplicating options
* options clashing in their description, which is also difficult to document and usually leads to stale documentation at one place
* options clashing in their type, which may result in a component being unusable when used in conjunction with another
* users not being able to configure distinct values per option",non_debt,-
hadoop,1752,comment,566876917,"Thanks @bgaborg for testing and providing update.
I can draft a release notes later, but to answer the question of what should we change to enable this:
I'll address Steve's other comments in a new commit, and post testing results with different config settings in auth-keys.xml including default SSE config (AWS owned CMK), AWS managed CMK and customer managed AWS key.",non_debt,-
airflow,3773,comment,414979183,"K. Just getting a sense of urgency. We might have a 1.10.1 anyway, and if we do I'll pull this in to it.",non_debt,-
arrow,1744,comment,373059009,The Travis-CI failure is due to a regression in a Cython 0.28: https://github.com/cython/cython/issues/2148,non_debt,-
spark,10208,comment,164531611,yeah if it doesn't mean adding a dependency on HttpClient into core -- it may happen to come in transitively already,non_debt,-
trafficserver,276,description,0,"Fix [TS-3752](https://issues.apache.org/jira/browse/TS-3752).
Approach: Collect all Header Block Fragments before decode with HPACK.",non_debt,-
incubator-doris,3513,review,422451684,"maybe use doc_value_mode is more suitableÔºü
in future, we can use different parser for _source or doc_value mode",non_debt,-
druid,2683,comment,198480289,Are these documented?,documentation_debt,outdated_documentation
ozone,702,comment,608511555,"The CI result shows green CI now, :).",non_debt,-
spark,23260,comment,446345304,"My understanding is that this allows pointing the Spark UI directly at the history server (old JHS or new ATS) instead of hardcoding the NM URL and relying on the NM redirecting you, since the NM may not exist later on.
That does open up some questions though. The code being modified is in the AM, which means that the user needs to opt into this when he submits the app, when perhaps if there was a way to hook this up on the Spark history server side only, that may be more useful.
I think someone tried that in the past but the SHS change was very YARN-specific, which made it kinda sub-optimal.",code_debt,low_quality_code
trafficserver,1946,review,117639097,"Interesting. We have an internal lib/ts/ink_base64.cc, I wonder if it'd be useful to have generic support for all the various Base\<nn\> flavors in the core, and expose those as TS APIs? Not saying this has to be done here, as part of this PR, but maybe it'd be generally useful to have long-term? Similar to how we have",non_debt,-
hawq,1290,comment,331820126,"LGTM, +1",non_debt,-
flink,13393,review,495556369,Ditto.,non_debt,-
spark,25585,comment,525012166,"alrighty!  things looking good.  the builds i broke (spark-master-test-sbt-*) are good, and i spot-checked one (https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2/324/) and the spark unit tests just started successfully:",non_debt,-
flink,10827,summary,0,[FLINK-15548] [table-runtime-blink] Make KeyedCoProcessOperatorWithWatermarkDelay extends KeyedCoProcessOperator instead of LegacyKeyedCoProcessOperator,non_debt,-
iceberg,1874,description,0,"This PR adds a stored procedure to expire snapshots.
The main author of this change is @liukun4515. I took commits from #1819, rebased, and added more changes to match the current state of procedures.
Resolves #1597.",non_debt,-
groovy,1266,summary,0,GroovyShell parse script use given context,non_debt,-
openwhisk,5061,description,0,"Manage New scheduler's invoker healthy management
Design document: https://cwiki.apache.org/confluence/display/OPENWHISK/InvokerHealthyManager",non_debt,-
ambari,3151,comment,562590943,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/5605/
Test PASSed.",non_debt,-
trafficcontrol,1572,comment,345855649,"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/incubator-trafficcontrol-PR-trafficops-test/639/",non_debt,-
camel,1898,description,0,‚Ä¶ided by the camel-zipkin component,non_debt,-
flink,13348,review,485303631,"Tumble.over(lit(1).hour.on(orders.rowtime).alias(""hourly_window"")) -> Tumble.over(lit(1).hour).on(orders.rowtime).alias(""hourly_window"")",non_debt,-
ambari,1839,summary,0,Ambari-24331. Hide not restartable components from Service Auto Start,non_debt,-
incubator-pinot,3969,description,0,19961085-3969 description-0,non_debt,-
kafka,4672,comment,371973195,Merged to trunk and 1.1.,non_debt,-
beam,12232,review,455392148,I see. Thanks for clarification.,non_debt,-
shardingsphere,6062,review,440642573,refresh,non_debt,-
spark,5030,comment,96768974,Can one of the admins verify this patch?,non_debt,-
kafka,8103,review,414240204,Is this test useful? We don't set the header value in the test.,non_debt,-
nifi,1628,description,0,"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?",non_debt,-
beam,3287,comment,306643413,"But we shouldn't be updating DSL_SQL branch -- that requires force push, and may lose data.",non_debt,-
flink,10086,review,353592042,ditto,non_debt,-
druid,4139,comment,404134850,@drcrallen @jihoonson Travis is good enough. Now close this PR. Thanks for your comments. :+1:,non_debt,-
hudi,782,comment,515634785,@vinothchandar it worked without changes to /etc/hosts,non_debt,-
hadoop,316,description,0,This is PR for https://issues.apache.org/jira/browse/HADOOP-15124,non_debt,-
camel,2457,review,208259951,"this may be a breaking change. @davsclaus , @ffang  could you have a look?",non_debt,-
flink,15085,review,589279908,@rmetzger / @zentol can you please advise on which `TravisGroup` annotation I should apply?,non_debt,-
iceberg,1318,review,468870977,Fixed. Thanks!,non_debt,-
skywalking,4093,review,360375251,Get it.,non_debt,-
spark,26337,comment,551156015,let's go option 1 then,non_debt,-
spark,3141,comment,62226551,Can one of the admins verify this patch?,non_debt,-
beam,7737,comment,461523125,"@chamikaramj, I think we're ready for a review",non_debt,-
pulsar,7552,comment,661158943,/pulsarbot run-failure-checks,non_debt,-
nifi,2517,comment,375875600,"@MikeThomsen ok, sorry, I am too impatient :D",non_debt,-
openwhisk,1600,summary,0,Generalization of the WIP API gateway controller route for supporting system packages as API handlers,non_debt,-
spark,7569,comment,125282944,"OK, closing.",non_debt,-
accumulo,337,review,157819248,should also check have the following checks,non_debt,-
storm,2236,summary,0,STORM-2652: fix error in open method of JmsSpout,non_debt,-
spark,13546,comment,224956535,Agree! This has an external change. Just let me know if we can do it. Thanks!,non_debt,-
flink,7466,review,247049482,"Hi, I think it's correct now, here is the code view form my patch.",non_debt,-
kafka,586,description,0,@guozhangwang,non_debt,-
spark,1751,comment,51282152,Trying that right now,non_debt,-
spark,10106,comment,168844006,oh jira already closed. excellent :),non_debt,-
systemds,553,comment,313253989,Ping @mboehm7,non_debt,-
